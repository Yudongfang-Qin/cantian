

SQL> exp users=abc, pfa, huawei, cantiandb
  2 file="cantiandb.sql"
  3 log = "cantiandb.log"
  4 query="where '1' = true"
  5 CONTENT=METADATA_ONLY
  6 COMPRESS=1;
Parsing export options ... 
Verify options ...
  verify schema ...

CT-00781, The user ABC does not exist.
Logical export failed.

SQL> 
SQL> exp tables=abc, pfa, huawei, cantiandb
  2 query="where '1' = true"
  3 CONTENT=METADATA_ONLY
  4 COMPRESS=1;
Parsing export options ... 
Verify options ...
  verify tables ...

CT-00601, Sql syntax error: can not export SYS schema
Logical export failed.

SQL> 
SQL> exp tables=abc, pfa, huawei, cantiandb,
  2 query="where '1' = true"
  3 CONTENT=METADATA_ONLY
  4 COMPRESS=1;
Parsing export options ... 

CT-00601, [2:6]Sql syntax error: invalid option for EXPORT
Logical export failed.

SQL> 
SQL> export users="   xxx"
  2 query="where '1' = true"
  3 CONTENT=METADATA_ONLY
  4 COMPRESS=1;
Parsing export options ... 
Verify options ...
  verify schema ...

CT-00781, The user    xxx does not exist.
Logical export failed.

SQL> 
SQL> export users="   xxx", SYS
  2 query="where '1' = true"
  3 CONTENT=METADATA_ONLY
  4 COMPRESS=1;
Parsing export options ... 

CT-00601, [1:24]Sql syntax error: can not export SYS schema
Logical export failed.

SQL> 
SQL> export users="SYS"
  2 query="where '1' = true"
  3 CONTENT=METADATA_ONLY
  4 COMPRESS=1;
Parsing export options ... 

CT-00601, [1:14]Sql syntax error: can not export SYS schema
Logical export failed.

SQL> 
SQL> export users="sys"
  2 query="where '1' = true"
  3 CONTENT=METADATA_ONLY
  4 COMPRESS=1;
Parsing export options ... 

CT-00601, [1:14]Sql syntax error: can not export SYS schema
Logical export failed.

SQL> 
SQL> export query="where '1' = true"
  2 CONTENT=METADATA_ONLY
  3 COMPRESS=1;
Parsing export options ... 
Verify options ...
  default to export current schema: SYS

CT-00601, Sql syntax error: can not export SYS schema
Logical export failed.

SQL> 
SQL> export users=abc, pfa, huawei, cantiandb query="where '1' = true"
  2 CONTENT=METADATA_ONLY
  3 COMPRESS=1;
Parsing export options ... 
Verify options ...
  verify schema ...

CT-00781, The user ABC does not exist.
Logical export failed.

SQL> 
SQL> exp log = "cantiandb.log"
  2  users=abc, pfa, huawei, sys query="where '1' = true"
  3 CONTENT=METADATA_ONLY
  4 COMPRESS=1;
Parsing export options ... 

CT-00601, [2:25]Sql syntax error: can not export SYS schema
Logical export failed.

SQL> 
SQL> exp log = "cantiandb.log"
  2  tables=bg:nh query="where '1' = true"
  3 CONTENT=METADATA_ONLY
  4 COMPRESS=1;
Parsing export options ... 

CT-00601, [2:10]Sql syntax error: invalid option for EXPORT
Logical export failed.

SQL> 
SQL> -- DTS2019020205132
SQL> drop USER if exists exp_invalid_file;

Succeed.

SQL> create user exp_invalid_file IDENTIFIED by 'exp_invalid_file123';

Succeed.

SQL> grant dba to exp_invalid_file;

Succeed.

SQL> conn exp_invalid_file/exp_invalid_file123@127.0.0.1:1611

connected.

SQL> drop table if exists DTS2019020205132_T1;

Succeed.

SQL> create table DTS2019020205132_T1 (
  2     `column1` int,
  3     `column2` int
  4 );

Succeed.

SQL> insert into DTS2019020205132_T1 values (1,2),(3,4),(5,6);

3 rows affected.

SQL> commit;

Succeed.

SQL> exp TABLES=DTS2019020205132_T1 CONTENT=METADATA_ONLY file="/root/exp_invalid_file.dmp";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...

CT-00002, Failed to open the file /root/exp_invalid_file.dmp, the error code was 13
Logical export failed.

SQL> imp TABLES=DTS2019020205132_T1 file="/root/exp_invalid_file.dmp";
Parsing import options ... 
Verify options ...
  verify tables ...
Starting import ...
Preparing to import ...

CT-00002, Failed to open the file /root/exp_invalid_file.dmp, the error code was 13
Logical import failed.

SQL> dump table DTS2019020205132_T1 into file '/root/exp_invalid_file.dmp';
CT-00003, Failed to create the file /root/exp_invalid_file.dmp, the error code was 13
SQL> 
SQL> -- DTS2018092111805
SQL> create user fenglang identified by Cantian_234;

Succeed.

SQL> grant dba to fenglang;

Succeed.

SQL> conn fenglang/Cantian_234@127.0.0.1:1611

connected.

SQL> drop table if exists FTV_global_003;

Succeed.

SQL> create global temporary table FTV_global_003(id int,name varchar2(10));

Succeed.

SQL> insert into FTV_global_003 values(1,'m'),(2,'m_s'),(3,'m_.&*%');

3 rows affected.

SQL> drop table if exists FTV_global_004;

Succeed.

SQL> create global temporary table FTV_global_004(id int,time date ,name clob);

Succeed.

SQL> insert into FTV_global_004 values(1,to_date('2018','yyyy'),'mm'),(2,('2017','yyyy'),'m_s')(3,('2015','yyyy'),'m_.&*%');

CT-00601, [1:76]Sql syntax error: the word "," is not correct
SQL> exp tables=FTV_global_003,FTV_global_004 file="temporary.dmp";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = FTV_GLOBAL_003, FTV_GLOBAL_004
-- FILE TYPE = TXT
-- DUMP FILE = temporary.dmp
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table FENGLANG.FTV_GLOBAL_003 ...
  exporting DDL of FENGLANG.FTV_GLOBAL_003 ...
  skipping to export the data of temporary table
  exporting indexes on FENGLANG.FTV_GLOBAL_003 ...
  exporting constraints on FENGLANG.FTV_GLOBAL_003 ...

exporting table FENGLANG.FTV_GLOBAL_004 ...
  exporting DDL of FENGLANG.FTV_GLOBAL_004 ...
  skipping to export the data of temporary table
  exporting indexes on FENGLANG.FTV_GLOBAL_004 ...
  exporting constraints on FENGLANG.FTV_GLOBAL_004 ...

Logical export succeeded.

SQL> 
SQL> conn sys/Huawei@123@127.0.0.1:1611

connected.

SQL> 
SQL> create user exp_user1 IDENTIFIED by 'exp_user123';

Succeed.

SQL> grant dba to exp_user1;

Succeed.

SQL> grant select on SYS_TABLES to exp_user1;

Succeed.

SQL> grant create session to exp_user1;

Succeed.

SQL> grant create table to exp_user1;

Succeed.

SQL> 
SQL> create user exp_user2 IDENTIFIED by 'exp_user123';

Succeed.

SQL> grant create session to exp_user2;

Succeed.

SQL> grant create table to exp_user2;

Succeed.

SQL> 
SQL> drop table if exists exp_user1.HOTELAVAILABILITY;

Succeed.

SQL> CREATE TABLE exp_user1.HOTELAVAILABILITY
  2      (HOTEL_ID INT NOT NULL, BOOKING_DATE DATE NOT NULL,
  3 --	ROOMS_TAKEN INT DEFAULT 0, PRIMARY KEY (HOTEL_ID, BOOKING_DATE));
SQL> 	ROOMS_TAKEN INT DEFAULT 0);

Succeed.

SQL> 
SQL> drop table if exists exp_user1.admin_emp;

Succeed.

SQL> CREATE TABLE exp_user1.admin_emp (
  2     empno      NUMBER(5) PRIMARY KEY,
  3 --	empno      NUMBER(5) PRIMARY KEY,
  4 	ename      VARCHAR2(15) NOT NULL,
  5 	ssn        NUMBER(9),
  6 	job        VARCHAR2(10),
  7 	mgr        NUMBER(5),
  8 	hiredate   DATE DEFAULT (sysdate),
  9 	birthday   timestamp(5) default current_timestamp(2),
 10 	photo      BLOB,
 11 	sal        NUMBER(7,2),
 12 	hrly_rate  NUMBER(7,2),
 13 	comm       NUMBER(7,2),
 14 	"abc"      NUMBER(7,2),
 15 	deptno     NUMBER(3) NOT NULL,
 16 	working_age  interval year(4) to month default '0-0',
 17 	working_age2 interval day(7) to second(4) default numtodsinterval(0, 'second')
 18 );

Succeed.

SQL> 
SQL> create or replace view exp_user1.test_v1 as select * from SYS_TABLES;

Succeed.

SQL> 
SQL> --BEGIN: test serveroutput
SQL> CREATE OR REPLACE FUNCTION exp_user1.func_test return varchar2
  2 IS
  3  cunt int := 0;
  4  Begin
  5  select count(*) into cunt from dual;
  6  dbe_output.print_line(cunt);
  7  IF SQL % FOUND
  8   then
  9  return cunt;
 10  end if;
 11  End func_test;
 12 /

Succeed.

SQL> 
SQL> CREATE OR REPLACE PROCEDURE exp_user1.proc_test(param1 out varchar2)
  2 IS
  3     tmp varchar2(20) :='12345678';
  4 begin
  5     dbe_output.print_line('OUT PUT RESULT:'||param1);
  6 end proc_test;
  7 /

Succeed.

SQL> 
SQL> DROP TABLE IF EXISTS exp_user2.T_PROC_1;

Succeed.

SQL> CREATE TABLE exp_user2.T_PROC_1 (F_INT1 INT);

Succeed.

SQL> 
SQL> -- use sys to export user
SQL> exp users = exp_user1, exp_user2 file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_USER1, EXP_USER2
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema EXP_USER1 ...
Exporting sequence of schema EXP_USER1 ...
Exporting profile of schema EXP_USER1 ...
Exporting type of schema EXP_USER1 ...
Exporting tables of schema EXP_USER1 ...
Reading table objects of EXP_USER1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
HOTELAVAILABILITY                                                1         
ADMIN_EMP                                                        2         

Exporting tables (scripts or data) of EXP_USER1
exporting table EXP_USER1.HOTELAVAILABILITY ...
  exporting DDL of EXP_USER1.HOTELAVAILABILITY ...
  exporting data of EXP_USER1.HOTELAVAILABILITY ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.HOTELAVAILABILITY ...
  exporting constraints on EXP_USER1.HOTELAVAILABILITY ...

exporting table EXP_USER1.ADMIN_EMP ...
  exporting DDL of EXP_USER1.ADMIN_EMP ...
  exporting data of EXP_USER1.ADMIN_EMP ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.ADMIN_EMP ...
  exporting constraints on EXP_USER1.ADMIN_EMP ...

Exporting procedures/functions/triggers of schema EXP_USER1 ...
  exporting FUNCTION EXP_USER1.FUNC_TEST
  exporting PROCEDURE EXP_USER1.PROC_TEST
Exporting views of schema EXP_USER1 ...
  exporting view EXP_USER1.TEST_V1
Exporting synonyms of schema EXP_USER1 ...
Exporting package of schema EXP_USER1 ...
End of export schema EXP_USER1 ...

Exporting schema EXP_USER2 ...
Exporting sequence of schema EXP_USER2 ...
Exporting profile of schema EXP_USER2 ...
Exporting type of schema EXP_USER2 ...
Exporting tables of schema EXP_USER2 ...
Reading table objects of EXP_USER2

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
T_PROC_1                                                         1         

Exporting tables (scripts or data) of EXP_USER2
exporting table EXP_USER2.T_PROC_1 ...
  exporting DDL of EXP_USER2.T_PROC_1 ...
  exporting data of EXP_USER2.T_PROC_1 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER2.T_PROC_1 ...
  exporting constraints on EXP_USER2.T_PROC_1 ...

Exporting procedures/functions/triggers of schema EXP_USER2 ...
Exporting views of schema EXP_USER2 ...
Exporting synonyms of schema EXP_USER2 ...
Exporting package of schema EXP_USER2 ...
End of export schema EXP_USER2 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_USER1, EXP_USER2
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = EXP_USER1;
DROP TABLE IF EXISTS "HOTELAVAILABILITY" CASCADE CONSTRAINTS;
CREATE TABLE "HOTELAVAILABILITY"
(
  "HOTEL_ID" BINARY_INTEGER NOT NULL,
  "BOOKING_DATE" DATE NOT NULL,
  "ROOMS_TAKEN" BINARY_INTEGER DEFAULT 0
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;

DROP TABLE IF EXISTS "ADMIN_EMP" CASCADE CONSTRAINTS;
CREATE TABLE "ADMIN_EMP"
(
  "EMPNO" NUMBER(5) NOT NULL,
  "ENAME" VARCHAR(15 BYTE) NOT NULL,
  "SSN" NUMBER(9),
  "JOB" VARCHAR(10 BYTE),
  "MGR" NUMBER(5),
  "HIREDATE" DATE DEFAULT (sysdate),
  "BIRTHDAY" TIMESTAMP(5) DEFAULT current_timestamp(2),
  "PHOTO" BLOB,
  "SAL" NUMBER(7, 2),
  "HRLY_RATE" NUMBER(7, 2),
  "COMM" NUMBER(7, 2),
  "abc" NUMBER(7, 2),
  "DEPTNO" NUMBER(3) NOT NULL,
  "WORKING_AGE" INTERVAL YEAR(4) TO MONTH DEFAULT '0-0',
  "WORKING_AGE2" INTERVAL DAY(7) TO SECOND(4) DEFAULT numtodsinterval(0, 'second')
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "ADMIN_EMP" ADD PRIMARY KEY("EMPNO");

CREATE OR REPLACE FUNCTION "FUNC_TEST"
return varchar2
IS
 cunt int := 0;
 Begin
 select count(*) into cunt from dual;
 dbe_output.print_line(cunt);
 IF SQL % FOUND
  then
 return cunt;
 end if;
 End func_test;
/


CREATE OR REPLACE PROCEDURE "PROC_TEST"
(param1 out varchar2)
IS
    tmp varchar2(20) :='12345678';
begin
    dbe_output.print_line('OUT PUT RESULT:'||param1);
end proc_test;
/


CREATE OR REPLACE FORCE VIEW "TEST_V1"
(
  "USER#",
  "ID",
  "NAME",
  "SPACE#",
  "ORG_SCN",
  "CHG_SCN",
  "TYPE",
  "COLS",
  "INDEXES",
  "PARTITIONED",
  "ENTRY",
  "INITRANS",
  "PCTFREE",
  "CR_MODE",
  "RECYCLED",
  "APPENDONLY",
  "NUM_ROWS",
  "BLOCKS",
  "EMPTY_BLOCKS",
  "AVG_ROW_LEN",
  "SAMPLESIZE",
  "ANALYZETIME",
  "SERIAL_START",
  "OPTIONS",
  "OBJ#",
  "VERSION",
  "FLAG"
) AS
select "USER#","ID","NAME","SPACE#","ORG_SCN","CHG_SCN","TYPE","COLS","INDEXES","PARTITIONED","ENTRY","INITRANS","PCTFREE","CR_MODE","RECYCLED","APPENDONLY","NUM_ROWS","BLOCKS","EMPTY_BLOCKS","AVG_ROW_LEN","SAMPLESIZE","ANALYZETIME","SERIAL_START","OPTIONS","OBJ#","VERSION","FLAG" from SYS_TABLES
/


ALTER SESSION SET CURRENT_SCHEMA = EXP_USER2;
DROP TABLE IF EXISTS "T_PROC_1" CASCADE CONSTRAINTS;
CREATE TABLE "T_PROC_1"
(
  "F_INT1" BINARY_INTEGER
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;

Logical export succeeded.

SQL> exp users = exp_user1 file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_USER1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema EXP_USER1 ...
Exporting sequence of schema EXP_USER1 ...
Exporting profile of schema EXP_USER1 ...
Exporting type of schema EXP_USER1 ...
Exporting tables of schema EXP_USER1 ...
Reading table objects of EXP_USER1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
HOTELAVAILABILITY                                                1         
ADMIN_EMP                                                        2         

Exporting tables (scripts or data) of EXP_USER1
exporting table EXP_USER1.HOTELAVAILABILITY ...
  exporting DDL of EXP_USER1.HOTELAVAILABILITY ...
  exporting data of EXP_USER1.HOTELAVAILABILITY ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.HOTELAVAILABILITY ...
  exporting constraints on EXP_USER1.HOTELAVAILABILITY ...

exporting table EXP_USER1.ADMIN_EMP ...
  exporting DDL of EXP_USER1.ADMIN_EMP ...
  exporting data of EXP_USER1.ADMIN_EMP ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.ADMIN_EMP ...
  exporting constraints on EXP_USER1.ADMIN_EMP ...

Exporting procedures/functions/triggers of schema EXP_USER1 ...
  exporting FUNCTION EXP_USER1.FUNC_TEST
  exporting PROCEDURE EXP_USER1.PROC_TEST
Exporting views of schema EXP_USER1 ...
  exporting view EXP_USER1.TEST_V1
Exporting synonyms of schema EXP_USER1 ...
Exporting package of schema EXP_USER1 ...
End of export schema EXP_USER1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_USER1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = EXP_USER1;
DROP TABLE IF EXISTS "HOTELAVAILABILITY" CASCADE CONSTRAINTS;
CREATE TABLE "HOTELAVAILABILITY"
(
  "HOTEL_ID" BINARY_INTEGER NOT NULL,
  "BOOKING_DATE" DATE NOT NULL,
  "ROOMS_TAKEN" BINARY_INTEGER DEFAULT 0
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;

DROP TABLE IF EXISTS "ADMIN_EMP" CASCADE CONSTRAINTS;
CREATE TABLE "ADMIN_EMP"
(
  "EMPNO" NUMBER(5) NOT NULL,
  "ENAME" VARCHAR(15 BYTE) NOT NULL,
  "SSN" NUMBER(9),
  "JOB" VARCHAR(10 BYTE),
  "MGR" NUMBER(5),
  "HIREDATE" DATE DEFAULT (sysdate),
  "BIRTHDAY" TIMESTAMP(5) DEFAULT current_timestamp(2),
  "PHOTO" BLOB,
  "SAL" NUMBER(7, 2),
  "HRLY_RATE" NUMBER(7, 2),
  "COMM" NUMBER(7, 2),
  "abc" NUMBER(7, 2),
  "DEPTNO" NUMBER(3) NOT NULL,
  "WORKING_AGE" INTERVAL YEAR(4) TO MONTH DEFAULT '0-0',
  "WORKING_AGE2" INTERVAL DAY(7) TO SECOND(4) DEFAULT numtodsinterval(0, 'second')
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "ADMIN_EMP" ADD PRIMARY KEY("EMPNO");

CREATE OR REPLACE FUNCTION "FUNC_TEST"
return varchar2
IS
 cunt int := 0;
 Begin
 select count(*) into cunt from dual;
 dbe_output.print_line(cunt);
 IF SQL % FOUND
  then
 return cunt;
 end if;
 End func_test;
/


CREATE OR REPLACE PROCEDURE "PROC_TEST"
(param1 out varchar2)
IS
    tmp varchar2(20) :='12345678';
begin
    dbe_output.print_line('OUT PUT RESULT:'||param1);
end proc_test;
/


CREATE OR REPLACE FORCE VIEW "TEST_V1"
(
  "USER#",
  "ID",
  "NAME",
  "SPACE#",
  "ORG_SCN",
  "CHG_SCN",
  "TYPE",
  "COLS",
  "INDEXES",
  "PARTITIONED",
  "ENTRY",
  "INITRANS",
  "PCTFREE",
  "CR_MODE",
  "RECYCLED",
  "APPENDONLY",
  "NUM_ROWS",
  "BLOCKS",
  "EMPTY_BLOCKS",
  "AVG_ROW_LEN",
  "SAMPLESIZE",
  "ANALYZETIME",
  "SERIAL_START",
  "OPTIONS",
  "OBJ#",
  "VERSION",
  "FLAG"
) AS
select "USER#","ID","NAME","SPACE#","ORG_SCN","CHG_SCN","TYPE","COLS","INDEXES","PARTITIONED","ENTRY","INITRANS","PCTFREE","CR_MODE","RECYCLED","APPENDONLY","NUM_ROWS","BLOCKS","EMPTY_BLOCKS","AVG_ROW_LEN","SAMPLESIZE","ANALYZETIME","SERIAL_START","OPTIONS","OBJ#","VERSION","FLAG" from SYS_TABLES
/

Logical export succeeded.

SQL> 
SQL> conn exp_user1/exp_user123@127.0.0.1:1611

connected.

SQL> exp users = exp_user1 file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_USER1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema EXP_USER1 ...
Exporting sequence of schema EXP_USER1 ...
Exporting profile of schema EXP_USER1 ...
Exporting type of schema EXP_USER1 ...
Exporting tables of schema EXP_USER1 ...
Reading table objects of EXP_USER1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
HOTELAVAILABILITY                                                1         
ADMIN_EMP                                                        2         

Exporting tables (scripts or data) of EXP_USER1
exporting table EXP_USER1.HOTELAVAILABILITY ...
  exporting DDL of EXP_USER1.HOTELAVAILABILITY ...
  exporting data of EXP_USER1.HOTELAVAILABILITY ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.HOTELAVAILABILITY ...
  exporting constraints on EXP_USER1.HOTELAVAILABILITY ...

exporting table EXP_USER1.ADMIN_EMP ...
  exporting DDL of EXP_USER1.ADMIN_EMP ...
  exporting data of EXP_USER1.ADMIN_EMP ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.ADMIN_EMP ...
  exporting constraints on EXP_USER1.ADMIN_EMP ...

Exporting procedures/functions/triggers of schema EXP_USER1 ...
  exporting FUNCTION EXP_USER1.FUNC_TEST
  exporting PROCEDURE EXP_USER1.PROC_TEST
Exporting views of schema EXP_USER1 ...
  exporting view EXP_USER1.TEST_V1
Exporting synonyms of schema EXP_USER1 ...
Exporting package of schema EXP_USER1 ...
End of export schema EXP_USER1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_USER1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = EXP_USER1;
DROP TABLE IF EXISTS "HOTELAVAILABILITY" CASCADE CONSTRAINTS;
CREATE TABLE "HOTELAVAILABILITY"
(
  "HOTEL_ID" BINARY_INTEGER NOT NULL,
  "BOOKING_DATE" DATE NOT NULL,
  "ROOMS_TAKEN" BINARY_INTEGER DEFAULT 0
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;

DROP TABLE IF EXISTS "ADMIN_EMP" CASCADE CONSTRAINTS;
CREATE TABLE "ADMIN_EMP"
(
  "EMPNO" NUMBER(5) NOT NULL,
  "ENAME" VARCHAR(15 BYTE) NOT NULL,
  "SSN" NUMBER(9),
  "JOB" VARCHAR(10 BYTE),
  "MGR" NUMBER(5),
  "HIREDATE" DATE DEFAULT (sysdate),
  "BIRTHDAY" TIMESTAMP(5) DEFAULT current_timestamp(2),
  "PHOTO" BLOB,
  "SAL" NUMBER(7, 2),
  "HRLY_RATE" NUMBER(7, 2),
  "COMM" NUMBER(7, 2),
  "abc" NUMBER(7, 2),
  "DEPTNO" NUMBER(3) NOT NULL,
  "WORKING_AGE" INTERVAL YEAR(4) TO MONTH DEFAULT '0-0',
  "WORKING_AGE2" INTERVAL DAY(7) TO SECOND(4) DEFAULT numtodsinterval(0, 'second')
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "ADMIN_EMP" ADD PRIMARY KEY("EMPNO");

CREATE OR REPLACE FUNCTION "FUNC_TEST"
return varchar2
IS
 cunt int := 0;
 Begin
 select count(*) into cunt from dual;
 dbe_output.print_line(cunt);
 IF SQL % FOUND
  then
 return cunt;
 end if;
 End func_test;
/


CREATE OR REPLACE PROCEDURE "PROC_TEST"
(param1 out varchar2)
IS
    tmp varchar2(20) :='12345678';
begin
    dbe_output.print_line('OUT PUT RESULT:'||param1);
end proc_test;
/


CREATE OR REPLACE FORCE VIEW "TEST_V1"
(
  "USER#",
  "ID",
  "NAME",
  "SPACE#",
  "ORG_SCN",
  "CHG_SCN",
  "TYPE",
  "COLS",
  "INDEXES",
  "PARTITIONED",
  "ENTRY",
  "INITRANS",
  "PCTFREE",
  "CR_MODE",
  "RECYCLED",
  "APPENDONLY",
  "NUM_ROWS",
  "BLOCKS",
  "EMPTY_BLOCKS",
  "AVG_ROW_LEN",
  "SAMPLESIZE",
  "ANALYZETIME",
  "SERIAL_START",
  "OPTIONS",
  "OBJ#",
  "VERSION",
  "FLAG"
) AS
select "USER#","ID","NAME","SPACE#","ORG_SCN","CHG_SCN","TYPE","COLS","INDEXES","PARTITIONED","ENTRY","INITRANS","PCTFREE","CR_MODE","RECYCLED","APPENDONLY","NUM_ROWS","BLOCKS","EMPTY_BLOCKS","AVG_ROW_LEN","SAMPLESIZE","ANALYZETIME","SERIAL_START","OPTIONS","OBJ#","VERSION","FLAG" from SYS_TABLES
/

Logical export succeeded.

SQL> -- use exp_user1 to export exp_user2
SQL> exp users = exp_user2 file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_USER2
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema EXP_USER2 ...
Exporting sequence of schema EXP_USER2 ...
Exporting profile of schema EXP_USER2 ...
Exporting type of schema EXP_USER2 ...
Exporting tables of schema EXP_USER2 ...
Reading table objects of EXP_USER2

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
T_PROC_1                                                         1         

Exporting tables (scripts or data) of EXP_USER2
exporting table EXP_USER2.T_PROC_1 ...
  exporting DDL of EXP_USER2.T_PROC_1 ...
  exporting data of EXP_USER2.T_PROC_1 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER2.T_PROC_1 ...
  exporting constraints on EXP_USER2.T_PROC_1 ...

Exporting procedures/functions/triggers of schema EXP_USER2 ...
Exporting views of schema EXP_USER2 ...
Exporting synonyms of schema EXP_USER2 ...
Exporting package of schema EXP_USER2 ...
End of export schema EXP_USER2 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_USER2
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = EXP_USER2;
DROP TABLE IF EXISTS "T_PROC_1" CASCADE CONSTRAINTS;
CREATE TABLE "T_PROC_1"
(
  "F_INT1" BINARY_INTEGER
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;

Logical export succeeded.

SQL> 
SQL> -- export distribute rules
SQL> exp help;
The syntax of logic export is: 

     Format:  EXP KEYWORD=value or KEYWORD=value1,value2,...,valueN;
     Example: EXP TABLES=EMP,DEPT,MGR;
               or EXP USERS=USER_A,USER_B;
               or EXP DIST_RULES=RULE_1,RULE_2;

Keyword                 Description (Default)
---------------------------------------------------------------------------------------------------------------------------
USERS                   List of schema names. Specify a percent sign (%) to export all users.
TABLES                  List of table names. Specify a percent sign (%) to export all tables.
DIST_RULES              List of distribute rule names. Specify a percent sign (%) to export all distribution rules. Supported only for sharding.
TABLESPACE_FILTER       List of tablespace names, the data or objects in these tablespaces will be exported. Case-sensitive words enclosed by '`' or '"'.
FILE                    Output file (EXPDAT.DMP) 
FILETYPE                Output file type: (TXT), BIN
LOG                     Log file of screen output
COMPRESS                Compress output file (0), only for FILETYPE=BIN, value range: 0-9, the smaller the value, the faster the speed, 0: no compression.
CONTENT                 Specifies data to unload where the valid keyword, values are: (ALL), DATA_ONLY, and METADATA_ONLY. 
QUERY                   Predicate clause used to export a subset of a table, eg. "where rownum <= 10" 
SKIP_COMMENTS           Do not add comments to dump file. (N)
FORCE                   Continue even if an SQL error occurs during a table dump. (N)
SKIP_ADD_DROP_TABLE     Do not add a DROP TABLE statement before each CREATE TABLE statement. (N)
SKIP_TRIGGERS           Do not dump triggers. (N)
QUOTE_NAMES             Quote identifiers. (Y)
TABLESPACE              Default transport all tablespaces except for system reserved. (N)
COMMIT_BATCH            Batch commit rows, commit once if set 0. (1000)
INSERT_BATCH            Batch insert rows. (1)
FEEDBACK                Feedback row count, feedback once if set 0 (10000)
PARALLEL                Table data export parallelism settings, range 2~16, The default value is 0
CONSISTENT              Cross - table consistency(N)
CREATE_USER             Export user definition(N),Used in conjunction with USERS.
ROLE                    Export user roles expect system preset roles (N),Used in conjunction with USERS.
GRANT                   Grant role and permission to USER (N),Used in conjunction with USERS and ROLE.
WITH_CR_MODE            Export tables and indexes with CR_MODE options (N)
WITH_FORMAT_CSF         Export tables and part tables with FORMAT CSF option (Y)
ENCRYPT                 Export files will be encrypted.
REMAP_TABLES            Table's name will remapped to another tablename.
PARTITIONS              Export tables's data within the input partition.
EXCLUDE                 Export exclude objects.
INDEX_PARTITIONS        Export index's partition informations (N).

SQL> 
SQL> exp dist_rules=% tables=% file="stdout";
Parsing export options ... 

CT-00601, Sql syntax error: Too many export types have been provided, only one of USERS, TABLES and DIST_RULES is allowed
Logical export failed.

SQL> 
SQL> conn exp_user1/exp_user123@127.0.0.1:1611

connected.

SQL> 
SQL> --test constraint
SQL> create table t_test_cons1
  2 (
  3     id1 int, 
  4     id2 int, 
  5     name1 varchar2(100), 
  6     name2 varchar2(100),
  7     primary key(id1, name1),
  8     unique(name2),
  9     check(name2 in ('a', 'b', 'c'))
 10 );

Succeed.

SQL> 
SQL> create unique index ix_test_cons1 on t_test_cons1(id2, name2);

Succeed.

SQL> alter table t_test_cons1 add constraint cons1_xxx unique(id2);

Succeed.

SQL> alter table t_test_cons1 add constraint cons1_yyy check(id2 in (1, 2, 3));

Succeed.

SQL> 
SQL> create table t_test_cons2
  2 (
  3     id1 int, 
  4     id2 int, 
  5     name1 varchar2(100), 
  6     name2 varchar2(100),
  7     foreign key(id1, name1) references t_test_cons1(id1, name1) on delete cascade
  8 );

Succeed.

SQL> 
SQL> alter table t_test_cons2 add constraint cons2_aaa primary key(id1);

Succeed.

SQL> alter table t_test_cons2 add constraint cons2_bbb foreign key(name2) references t_test_cons1(name2) on delete set null;

Succeed.

SQL> 
SQL> exp tables=t_test_cons1, t_test_cons2 file = "stdout";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = T_TEST_CONS1, T_TEST_CONS2
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER1.T_TEST_CONS1 ...
  exporting DDL of EXP_USER1.T_TEST_CONS1 ...
  exporting data of EXP_USER1.T_TEST_CONS1 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.T_TEST_CONS1 ...
  exporting constraints on EXP_USER1.T_TEST_CONS1 ...

exporting table EXP_USER1.T_TEST_CONS2 ...
  exporting DDL of EXP_USER1.T_TEST_CONS2 ...
  exporting data of EXP_USER1.T_TEST_CONS2 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.T_TEST_CONS2 ...
  exporting constraints on EXP_USER1.T_TEST_CONS2 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = T_TEST_CONS1, T_TEST_CONS2
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "T_TEST_CONS1" CASCADE CONSTRAINTS;
CREATE TABLE "T_TEST_CONS1"
(
  "ID1" BINARY_INTEGER NOT NULL,
  "ID2" BINARY_INTEGER,
  "NAME1" VARCHAR(100 BYTE) NOT NULL,
  "NAME2" VARCHAR(100 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
CREATE UNIQUE INDEX "IX_TEST_CONS1" ON "T_TEST_CONS1"("ID2", "NAME2")
TABLESPACE "USERS"
INITRANS 2
PCTFREE 8;
ALTER TABLE "T_TEST_CONS1" ADD CONSTRAINT "CONS1_XXX" UNIQUE("ID2");
ALTER TABLE "T_TEST_CONS1" ADD CONSTRAINT "CONS1_YYY" CHECK(id2 in (1, 2, 3));
ALTER TABLE "T_TEST_CONS1" ADD PRIMARY KEY("ID1", "NAME1");
ALTER TABLE "T_TEST_CONS1" ADD UNIQUE("NAME2");
ALTER TABLE "T_TEST_CONS1" ADD CHECK(name2 in ('a', 'b', 'c'));

DROP TABLE IF EXISTS "T_TEST_CONS2" CASCADE CONSTRAINTS;
CREATE TABLE "T_TEST_CONS2"
(
  "ID1" BINARY_INTEGER NOT NULL,
  "ID2" BINARY_INTEGER,
  "NAME1" VARCHAR(100 BYTE),
  "NAME2" VARCHAR(100 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "T_TEST_CONS2" ADD CONSTRAINT "CONS2_AAA" PRIMARY KEY("ID1");

ALTER TABLE "T_TEST_CONS2" ADD CONSTRAINT "CONS2_BBB" FOREIGN KEY("NAME2") REFERENCES "T_TEST_CONS1"("NAME2") ON DELETE SET NULL;
ALTER TABLE "T_TEST_CONS2" ADD FOREIGN KEY("ID1", "NAME1") REFERENCES "T_TEST_CONS1"("ID1", "NAME1") ON DELETE CASCADE;


Logical export succeeded.

SQL> 
SQL> --test commit_batch
SQL> create table t_test_batch
  2 (
  3     id int, name varchar2(100)
  4 );

Succeed.

SQL> 
SQL> alter table t_test_batch add constraint pk_t_test_batch primary key(id);

Succeed.

SQL> create index ix_t_test_batch on t_test_batch(name);

Succeed.

SQL> 
SQL> begin
  2     for i in 1 .. 20 loop
  3         insert into t_test_batch(id) values(i);
  4     end loop;
  5     commit;
  6 end;
  7 /

PL/SQL procedure successfully completed.

SQL> 
SQL> exp tables= t_test_batch commit_batch=5 file = "stdout";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = T_TEST_BATCH
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 5
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER1.T_TEST_BATCH ...
  exporting DDL of EXP_USER1.T_TEST_BATCH ...
  exporting data of EXP_USER1.T_TEST_BATCH ...
    data exporting success, 20 rows are dumped.

  exporting indexes on EXP_USER1.T_TEST_BATCH ...
  exporting constraints on EXP_USER1.T_TEST_BATCH ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = T_TEST_BATCH
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 5
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "T_TEST_BATCH" CASCADE CONSTRAINTS;
CREATE TABLE "T_TEST_BATCH"
(
  "ID" BINARY_INTEGER NOT NULL,
  "NAME" VARCHAR(100 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (1,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (2,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (3,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (4,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (5,null);
COMMIT;
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (6,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (7,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (8,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (9,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (10,null);
COMMIT;
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (11,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (12,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (13,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (14,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (15,null);
COMMIT;
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (16,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (17,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (18,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (19,null);
INSERT INTO "T_TEST_BATCH" ("ID","NAME") values
  (20,null);
COMMIT;
CREATE INDEX "IX_T_TEST_BATCH" ON "T_TEST_BATCH"("NAME")
TABLESPACE "USERS"
INITRANS 2
PCTFREE 8;
ALTER TABLE "T_TEST_BATCH" ADD CONSTRAINT "PK_T_TEST_BATCH" PRIMARY KEY("ID");


Logical export succeeded.

SQL> 
SQL> conn sys/Huawei@123@127.0.0.1:1611

connected.

SQL> create user exp_user3 IDENTIFIED by 'exp_user123';

Succeed.

SQL> grant create session to exp_user3;

Succeed.

SQL> grant create table to exp_user3;

Succeed.

SQL> grant create sequence to exp_user3;

Succeed.

SQL> 
SQL> --test sequence
SQL> create sequence exp_user3.S_CONFIGID_1
  2 minvalue 1
  3 maxvalue 99999999999999999999
  4 start with 21
  5 increment by 1
  6 cache 20
  7 order
  8 cycle;

Succeed.

SQL> 
SQL> exp users=exp_user3 file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_USER3
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema EXP_USER3 ...
Exporting sequence of schema EXP_USER3 ...
Exporting profile of schema EXP_USER3 ...
Exporting type of schema EXP_USER3 ...
Exporting tables of schema EXP_USER3 ...
Reading table objects of EXP_USER3

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of EXP_USER3
Exporting procedures/functions/triggers of schema EXP_USER3 ...
Exporting views of schema EXP_USER3 ...
Exporting synonyms of schema EXP_USER3 ...
Exporting package of schema EXP_USER3 ...
End of export schema EXP_USER3 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_USER3
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = EXP_USER3;
DROP SEQUENCE IF EXISTS "S_CONFIGID_1";
CREATE SEQUENCE "S_CONFIGID_1" MINVALUE 1 MAXVALUE 9223372036854775807 START WITH 21 INCREMENT BY 1 CYCLE CACHE 20 ORDER ;

Logical export succeeded.

SQL> 
SQL> create user northwind IDENTIFIED by 'exp_user123';

Succeed.

SQL> grant create session to northwind;

Succeed.

SQL> grant create table to northwind;

Succeed.

SQL> 
SQL> CREATE TABLE northwind.Region 
  2 ( 
  3   RegionID  NUMBER NOT NULL, 
  4   RegionDescription  CHAR(50 CHAR) NOT NULL, 
  5 CONSTRAINT PK_Region 
  6   PRIMARY KEY (RegionID)
  7 ) 
  8 / 

Succeed.

SQL> 
SQL> CREATE TABLE northwind.Territories 
  2 ( 
  3   TerritoryID  VARCHAR2(20) NOT NULL, 
  4   TerritoryDescription  NCHAR(50) NOT NULL, 
  5   RegionID  NUMBER NOT NULL, 
  6 CONSTRAINT PK_Territories 
  7   PRIMARY KEY (TerritoryID), 
  8 CONSTRAINT FK_Territories_Region FOREIGN KEY (RegionID) REFERENCES northwind.Region(RegionID)
  9 ) 
 10 / 

Succeed.

SQL> 
SQL> CREATE TABLE northwind.Categories 
  2 ( 
  3   CategoryID  NUMBER NOT NULL, 
  4   SuperID     NUMBER, -- parent Categories
  5   CategoryName  VARCHAR2(15) NOT NULL, 
  6   Description  NVARCHAR(300), 
  7   Picture  BLOB, 
  8 CONSTRAINT PK_Categories 
  9   PRIMARY KEY (CategoryID)
 10 ) 
 11 / 

Succeed.

SQL> 
SQL> ALTER TABLE northwind.Categories ADD CONSTRAINT FK_Categories_Categories FOREIGN KEY (SuperID) REFERENCES northwind.Categories(CategoryID)
  2 /

Succeed.

SQL> 
SQL> CREATE TABLE northwind.Suppliers 
  2 ( 
  3   SupplierID  NUMBER NOT NULL, 
  4   CompanyName  VARCHAR2(40 CHAR) NOT NULL, 
  5   ContactName  VARCHAR2(30), 
  6   ContactTitle  VARCHAR2(30), 
  7   Address  VARCHAR2(60), 
  8   City  VARCHAR2(15), 
  9   Region  VARCHAR2(15), 
 10   PostalCode  VARCHAR2(10), 
 11   Country  VARCHAR2(15), 
 12   Phone  VARCHAR2(24), 
 13   Fax  VARCHAR2(24), 
 14   HomePage  VARCHAR2(200), 
 15 CONSTRAINT PK_Suppliers 
 16   PRIMARY KEY (SupplierID)
 17 ) 
 18 / 

Succeed.

SQL> 
SQL> CREATE TABLE northwind.Products 
  2 ( 
  3   ProductID  NUMBER NOT NULL, 
  4   ProductName  VARCHAR2(40) NOT NULL, 
  5   SupplierID  NUMBER, 
  6   CategoryID  NUMBER, 
  7   QuantityPerUnit  VARCHAR2(20), 
  8   UnitPrice  NUMBER, 
  9   UnitsInStock  NUMBER, 
 10   UnitsOnOrder  NUMBER, 
 11   ReorderLevel  NUMBER, 
 12   Discontinued  NUMBER(1) NOT NULL, 
 13 CONSTRAINT PK_Products 
 14   PRIMARY KEY (ProductID), 
 15 CONSTRAINT CK_Products_UnitPrice   CHECK ((UnitPrice >= 0)), 
 16 CONSTRAINT CK_ReorderLevel   CHECK ((ReorderLevel >= 0)), 
 17 CONSTRAINT CK_UnitsInStock   CHECK ((UnitsInStock >= 0)), 
 18 CONSTRAINT CK_UnitsOnOrder   CHECK ((UnitsOnOrder >= 0)), 
 19 CONSTRAINT FK_Products_Categories FOREIGN KEY (CategoryID) REFERENCES northwind.Categories(CategoryID), 
 20 CONSTRAINT FK_Products_Suppliers FOREIGN KEY (SupplierID) REFERENCES northwind.Suppliers(SupplierID)
 21 ) 
 22 / 

Succeed.

SQL> 
SQL> CREATE TABLE northwind.Shippers 
  2 ( 
  3   ShipperID  NUMBER NOT NULL, 
  4   CompanyName  VARCHAR2(40) NOT NULL, 
  5   Phone  VARCHAR2(24), 
  6 CONSTRAINT PK_Shippers 
  7   PRIMARY KEY (ShipperID)
  8 ) 
  9 / 

Succeed.

SQL> 
SQL> CREATE TABLE northwind.Customers 
  2 ( 
  3   CustomerID  CHAR(5) NOT NULL, 
  4   CompanyName  VARCHAR2(40) NOT NULL, 
  5   ContactName  VARCHAR2(30), 
  6   ContactTitle  VARCHAR2(30), 
  7   Address  VARCHAR2(60), 
  8   City  VARCHAR2(15), 
  9   Region  VARCHAR2(15), 
 10   PostalCode  VARCHAR2(10), 
 11   Country  VARCHAR2(15), 
 12   Phone  VARCHAR2(24), 
 13   Fax  VARCHAR2(24), 
 14 CONSTRAINT PK_Customers 
 15   PRIMARY KEY (CustomerID)
 16 ) 
 17 / 

Succeed.

SQL> 
SQL> CREATE TABLE northwind.Employees 
  2 ( 
  3   EmployeeID  NUMBER NOT NULL, 
  4   LastName  VARCHAR2(20) NOT NULL, 
  5   FirstName  VARCHAR2(10) NOT NULL, 
  6   Title  VARCHAR2(30), 
  7   TitleOfCourtesy  VARCHAR2(25), 
  8   BirthDate  DATE, 
  9   HireDate  DATE, 
 10   Address  VARCHAR2(60), 
 11   City  VARCHAR2(15), 
 12   Region  VARCHAR2(15), 
 13   PostalCode  VARCHAR2(10), 
 14   Country  VARCHAR2(15), 
 15   HomePhone  VARCHAR2(24), 
 16   Extension  VARCHAR2(4), 
 17   Photo  BLOB, 
 18   Notes  VARCHAR2(600), 
 19   ReportsTo  NUMBER, 
 20   PhotoPath  VARCHAR2(255), 
 21 CONSTRAINT PK_Employees 
 22   PRIMARY KEY (EmployeeID)
 23 ) 
 24 / 

Succeed.

SQL> 
SQL> ALTER TABLE northwind.Employees  ADD CONSTRAINT FK_Employees_Employees FOREIGN KEY (ReportsTo) REFERENCES northwind.Employees(EmployeeID)
  2 /

Succeed.

SQL> 
SQL> CREATE TABLE northwind.EmployeeTerritories 
  2 ( 
  3   EmployeeID  NUMBER NOT NULL, 
  4   TerritoryID  VARCHAR2(20) NOT NULL, 
  5 CONSTRAINT PK_EmpTerritories 
  6   PRIMARY KEY (EmployeeID, TerritoryID), 
  7 CONSTRAINT FK_EmpTerri_Employees FOREIGN KEY (EmployeeID) REFERENCES northwind.Employees(EmployeeID), 
  8 CONSTRAINT FK_EmpTerri_Territories FOREIGN KEY (TerritoryID) REFERENCES northwind.Territories(TerritoryID)
  9 ) 
 10 / 

Succeed.

SQL> 
SQL> CREATE TABLE northwind.CustomerDemographics 
  2 ( 
  3   CustomerTypeID  CHAR(10) NOT NULL, 
  4   CustomerDesc  varchar(4000 CHAR), 
  5 CONSTRAINT PK_CustomerDemographics 
  6   PRIMARY KEY (CustomerTypeID)
  7 ) 
  8 / 

Succeed.

SQL> 
SQL> CREATE TABLE northwind.CustomerCustomerDemo 
  2 ( 
  3   CustomerID  CHAR(5) NOT NULL, 
  4   CustomerTypeID  CHAR(10) NOT NULL, 
  5 CONSTRAINT PK_CustomerDemo 
  6   PRIMARY KEY (CustomerID, CustomerTypeID), 
  7 CONSTRAINT FK_CustomerDemo FOREIGN KEY (CustomerTypeID) REFERENCES northwind.CustomerDemographics(CustomerTypeID), 
  8 CONSTRAINT FK_CustomerDemo_Customers FOREIGN KEY (CustomerID) REFERENCES northwind.Customers(CustomerID)
  9 ) 
 10 / 

Succeed.

SQL> 
SQL> CREATE TABLE northwind.Orders 
  2 ( 
  3   OrderID  NUMBER NOT NULL, 
  4   CustomerID  CHAR(5), 
  5   EmployeeID  NUMBER, 
  6   TerritoryID  VARCHAR2(20), 
  7   OrderDate  timestamp, 
  8   RequiredDate  timestamp, 
  9   ShippedDate  timestamp, 
 10   ShipVia  NUMBER, 
 11   Freight  NUMBER, 
 12   ShipName  VARCHAR2(40), 
 13   ShipAddress  VARCHAR2(60 BYTE), 
 14   ShipCity  VARCHAR2(15), 
 15   ShipRegion  VARCHAR2(15), 
 16   ShipPostalCode  VARCHAR2(10), 
 17   ShipCountry  VARCHAR2(15), 
 18 CONSTRAINT PK_Orders 
 19   PRIMARY KEY (OrderID), 
 20 CONSTRAINT FK_Orders_Customers FOREIGN KEY (CustomerID) REFERENCES northwind.Customers(CustomerID), 
 21 CONSTRAINT FK_Orders_Employees FOREIGN KEY (EmployeeID) REFERENCES northwind.Employees(EmployeeID), 
 22 CONSTRAINT FK_Orders_Shippers FOREIGN KEY (ShipVia) REFERENCES northwind.Shippers(ShipperID),
 23 CONSTRAINT FK_Orders_Territories FOREIGN KEY (TerritoryID) REFERENCES northwind.Territories(TerritoryID)
 24 ) 
 25 / 

Succeed.

SQL> 
SQL> CREATE TABLE northwind.OrderDetails 
  2 ( 
  3   OrderID  NUMBER NOT NULL, 
  4   ProductID  NUMBER NOT NULL, 
  5   UnitPrice  NUMBER NOT NULL, 
  6   Quantity  NUMBER NOT NULL, 
  7   Discount  NUMBER NOT NULL, 
  8 CONSTRAINT PK_Order_Details 
  9   PRIMARY KEY (OrderID, ProductID), 
 10 CONSTRAINT CK_Discount   CHECK ((Discount >= 0 and Discount <= 1)), 
 11 CONSTRAINT CK_Quantity   CHECK ((Quantity > 0)), 
 12 CONSTRAINT CK_UnitPrice   CHECK ((UnitPrice >= 0)), 
 13 CONSTRAINT FK_OrderDetails_Orders FOREIGN KEY (OrderID) REFERENCES northwind.Orders(OrderID), 
 14 CONSTRAINT FK_OrderDetails_Products FOREIGN KEY (ProductID) REFERENCES northwind.Products(ProductID)
 15 ) CRMODE page
 16 /

Succeed.

SQL> 
SQL> CREATE TABLE northwind.WebScripts
  2 (
  3   name    varchar(60) not null,
  4   script  CLOB
  5 ) CRMODE row
  6 / 

Succeed.

SQL> 
SQL> CREATE TABLE northwind.tb_supplier
  2 (
  3   supplier_id number not null,
  4   supplier_name varchar2(50) not null,
  5   contact_name varchar2(50),
  6   CONSTRAINT pk_supplier PRIMARY KEY (supplier_id)
  7 )
  8 /

Succeed.

SQL> 
SQL> CREATE TABLE northwind.tb_products
  2 (
  3   product_id number not null,
  4   product_name varchar2(100),
  5   supplier_id number not null references northwind.tb_supplier(supplier_id)
  6 )
  7 /

Succeed.

SQL> 
SQL> exp users=northwind file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = NORTHWIND
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema NORTHWIND ...
Exporting sequence of schema NORTHWIND ...
Exporting profile of schema NORTHWIND ...
Exporting type of schema NORTHWIND ...
Exporting tables of schema NORTHWIND ...
Reading table objects of NORTHWIND

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
REGION                                                           1         
TERRITORIES                                                      2         
CATEGORIES                                                       3         
SUPPLIERS                                                        4         
PRODUCTS                                                         5         
SHIPPERS                                                         6         
CUSTOMERS                                                        7         
EMPLOYEES                                                        8         
EMPLOYEETERRITORIES                                              9         
CUSTOMERDEMOGRAPHICS                                             10        
CUSTOMERCUSTOMERDEMO                                             11        
ORDERS                                                           12        
ORDERDETAILS                                                     13        
WEBSCRIPTS                                                       14        
TB_SUPPLIER                                                      15        
TB_PRODUCTS                                                      16        

Exporting tables (scripts or data) of NORTHWIND
exporting table NORTHWIND.REGION ...
  exporting DDL of NORTHWIND.REGION ...
  exporting data of NORTHWIND.REGION ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.REGION ...
  exporting constraints on NORTHWIND.REGION ...

exporting table NORTHWIND.TERRITORIES ...
  exporting DDL of NORTHWIND.TERRITORIES ...
  exporting data of NORTHWIND.TERRITORIES ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.TERRITORIES ...
  exporting constraints on NORTHWIND.TERRITORIES ...

exporting table NORTHWIND.CATEGORIES ...
  exporting DDL of NORTHWIND.CATEGORIES ...
  exporting data of NORTHWIND.CATEGORIES ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.CATEGORIES ...
  exporting constraints on NORTHWIND.CATEGORIES ...

exporting table NORTHWIND.SUPPLIERS ...
  exporting DDL of NORTHWIND.SUPPLIERS ...
  exporting data of NORTHWIND.SUPPLIERS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.SUPPLIERS ...
  exporting constraints on NORTHWIND.SUPPLIERS ...

exporting table NORTHWIND.PRODUCTS ...
  exporting DDL of NORTHWIND.PRODUCTS ...
  exporting data of NORTHWIND.PRODUCTS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.PRODUCTS ...
  exporting constraints on NORTHWIND.PRODUCTS ...

exporting table NORTHWIND.SHIPPERS ...
  exporting DDL of NORTHWIND.SHIPPERS ...
  exporting data of NORTHWIND.SHIPPERS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.SHIPPERS ...
  exporting constraints on NORTHWIND.SHIPPERS ...

exporting table NORTHWIND.CUSTOMERS ...
  exporting DDL of NORTHWIND.CUSTOMERS ...
  exporting data of NORTHWIND.CUSTOMERS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.CUSTOMERS ...
  exporting constraints on NORTHWIND.CUSTOMERS ...

exporting table NORTHWIND.EMPLOYEES ...
  exporting DDL of NORTHWIND.EMPLOYEES ...
  exporting data of NORTHWIND.EMPLOYEES ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.EMPLOYEES ...
  exporting constraints on NORTHWIND.EMPLOYEES ...

exporting table NORTHWIND.EMPLOYEETERRITORIES ...
  exporting DDL of NORTHWIND.EMPLOYEETERRITORIES ...
  exporting data of NORTHWIND.EMPLOYEETERRITORIES ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.EMPLOYEETERRITORIES ...
  exporting constraints on NORTHWIND.EMPLOYEETERRITORIES ...

exporting table NORTHWIND.CUSTOMERDEMOGRAPHICS ...
  exporting DDL of NORTHWIND.CUSTOMERDEMOGRAPHICS ...
  exporting data of NORTHWIND.CUSTOMERDEMOGRAPHICS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.CUSTOMERDEMOGRAPHICS ...
  exporting constraints on NORTHWIND.CUSTOMERDEMOGRAPHICS ...

exporting table NORTHWIND.CUSTOMERCUSTOMERDEMO ...
  exporting DDL of NORTHWIND.CUSTOMERCUSTOMERDEMO ...
  exporting data of NORTHWIND.CUSTOMERCUSTOMERDEMO ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.CUSTOMERCUSTOMERDEMO ...
  exporting constraints on NORTHWIND.CUSTOMERCUSTOMERDEMO ...

exporting table NORTHWIND.ORDERS ...
  exporting DDL of NORTHWIND.ORDERS ...
  exporting data of NORTHWIND.ORDERS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.ORDERS ...
  exporting constraints on NORTHWIND.ORDERS ...

exporting table NORTHWIND.ORDERDETAILS ...
  exporting DDL of NORTHWIND.ORDERDETAILS ...
  exporting data of NORTHWIND.ORDERDETAILS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.ORDERDETAILS ...
  exporting constraints on NORTHWIND.ORDERDETAILS ...

exporting table NORTHWIND.WEBSCRIPTS ...
  exporting DDL of NORTHWIND.WEBSCRIPTS ...
  exporting data of NORTHWIND.WEBSCRIPTS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.WEBSCRIPTS ...
  exporting constraints on NORTHWIND.WEBSCRIPTS ...

exporting table NORTHWIND.TB_SUPPLIER ...
  exporting DDL of NORTHWIND.TB_SUPPLIER ...
  exporting data of NORTHWIND.TB_SUPPLIER ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.TB_SUPPLIER ...
  exporting constraints on NORTHWIND.TB_SUPPLIER ...

exporting table NORTHWIND.TB_PRODUCTS ...
  exporting DDL of NORTHWIND.TB_PRODUCTS ...
  exporting data of NORTHWIND.TB_PRODUCTS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.TB_PRODUCTS ...
  exporting constraints on NORTHWIND.TB_PRODUCTS ...

Exporting procedures/functions/triggers of schema NORTHWIND ...
Exporting views of schema NORTHWIND ...
Exporting synonyms of schema NORTHWIND ...
Exporting package of schema NORTHWIND ...
End of export schema NORTHWIND ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = NORTHWIND
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = NORTHWIND;
DROP TABLE IF EXISTS "REGION" CASCADE CONSTRAINTS;
CREATE TABLE "REGION"
(
  "REGIONID" NUMBER NOT NULL,
  "REGIONDESCRIPTION" CHAR(50 CHAR) NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "REGION" ADD CONSTRAINT "PK_REGION" PRIMARY KEY("REGIONID");

DROP TABLE IF EXISTS "TERRITORIES" CASCADE CONSTRAINTS;
CREATE TABLE "TERRITORIES"
(
  "TERRITORYID" VARCHAR(20 BYTE) NOT NULL,
  "TERRITORYDESCRIPTION" CHAR(50 CHAR) NOT NULL,
  "REGIONID" NUMBER NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "TERRITORIES" ADD CONSTRAINT "PK_TERRITORIES" PRIMARY KEY("TERRITORYID");

DROP TABLE IF EXISTS "CATEGORIES" CASCADE CONSTRAINTS;
CREATE TABLE "CATEGORIES"
(
  "CATEGORYID" NUMBER NOT NULL,
  "SUPERID" NUMBER,
  "CATEGORYNAME" VARCHAR(15 BYTE) NOT NULL,
  "DESCRIPTION" VARCHAR(300 CHAR),
  "PICTURE" BLOB
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "CATEGORIES" ADD CONSTRAINT "PK_CATEGORIES" PRIMARY KEY("CATEGORYID");

DROP TABLE IF EXISTS "SUPPLIERS" CASCADE CONSTRAINTS;
CREATE TABLE "SUPPLIERS"
(
  "SUPPLIERID" NUMBER NOT NULL,
  "COMPANYNAME" VARCHAR(40 CHAR) NOT NULL,
  "CONTACTNAME" VARCHAR(30 BYTE),
  "CONTACTTITLE" VARCHAR(30 BYTE),
  "ADDRESS" VARCHAR(60 BYTE),
  "CITY" VARCHAR(15 BYTE),
  "REGION" VARCHAR(15 BYTE),
  "POSTALCODE" VARCHAR(10 BYTE),
  "COUNTRY" VARCHAR(15 BYTE),
  "PHONE" VARCHAR(24 BYTE),
  "FAX" VARCHAR(24 BYTE),
  "HOMEPAGE" VARCHAR(200 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "SUPPLIERS" ADD CONSTRAINT "PK_SUPPLIERS" PRIMARY KEY("SUPPLIERID");

DROP TABLE IF EXISTS "PRODUCTS" CASCADE CONSTRAINTS;
CREATE TABLE "PRODUCTS"
(
  "PRODUCTID" NUMBER NOT NULL,
  "PRODUCTNAME" VARCHAR(40 BYTE) NOT NULL,
  "SUPPLIERID" NUMBER,
  "CATEGORYID" NUMBER,
  "QUANTITYPERUNIT" VARCHAR(20 BYTE),
  "UNITPRICE" NUMBER,
  "UNITSINSTOCK" NUMBER,
  "UNITSONORDER" NUMBER,
  "REORDERLEVEL" NUMBER,
  "DISCONTINUED" NUMBER(1) NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "PRODUCTS" ADD CONSTRAINT "PK_PRODUCTS" PRIMARY KEY("PRODUCTID");
ALTER TABLE "PRODUCTS" ADD CONSTRAINT "CK_PRODUCTS_UNITPRICE" CHECK((UnitPrice >= 0));
ALTER TABLE "PRODUCTS" ADD CONSTRAINT "CK_REORDERLEVEL" CHECK((ReorderLevel >= 0));
ALTER TABLE "PRODUCTS" ADD CONSTRAINT "CK_UNITSINSTOCK" CHECK((UnitsInStock >= 0));
ALTER TABLE "PRODUCTS" ADD CONSTRAINT "CK_UNITSONORDER" CHECK((UnitsOnOrder >= 0));

DROP TABLE IF EXISTS "SHIPPERS" CASCADE CONSTRAINTS;
CREATE TABLE "SHIPPERS"
(
  "SHIPPERID" NUMBER NOT NULL,
  "COMPANYNAME" VARCHAR(40 BYTE) NOT NULL,
  "PHONE" VARCHAR(24 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "SHIPPERS" ADD CONSTRAINT "PK_SHIPPERS" PRIMARY KEY("SHIPPERID");

DROP TABLE IF EXISTS "CUSTOMERS" CASCADE CONSTRAINTS;
CREATE TABLE "CUSTOMERS"
(
  "CUSTOMERID" CHAR(5 BYTE) NOT NULL,
  "COMPANYNAME" VARCHAR(40 BYTE) NOT NULL,
  "CONTACTNAME" VARCHAR(30 BYTE),
  "CONTACTTITLE" VARCHAR(30 BYTE),
  "ADDRESS" VARCHAR(60 BYTE),
  "CITY" VARCHAR(15 BYTE),
  "REGION" VARCHAR(15 BYTE),
  "POSTALCODE" VARCHAR(10 BYTE),
  "COUNTRY" VARCHAR(15 BYTE),
  "PHONE" VARCHAR(24 BYTE),
  "FAX" VARCHAR(24 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "CUSTOMERS" ADD CONSTRAINT "PK_CUSTOMERS" PRIMARY KEY("CUSTOMERID");

DROP TABLE IF EXISTS "EMPLOYEES" CASCADE CONSTRAINTS;
CREATE TABLE "EMPLOYEES"
(
  "EMPLOYEEID" NUMBER NOT NULL,
  "LASTNAME" VARCHAR(20 BYTE) NOT NULL,
  "FIRSTNAME" VARCHAR(10 BYTE) NOT NULL,
  "TITLE" VARCHAR(30 BYTE),
  "TITLEOFCOURTESY" VARCHAR(25 BYTE),
  "BIRTHDATE" DATE,
  "HIREDATE" DATE,
  "ADDRESS" VARCHAR(60 BYTE),
  "CITY" VARCHAR(15 BYTE),
  "REGION" VARCHAR(15 BYTE),
  "POSTALCODE" VARCHAR(10 BYTE),
  "COUNTRY" VARCHAR(15 BYTE),
  "HOMEPHONE" VARCHAR(24 BYTE),
  "EXTENSION" VARCHAR(4 BYTE),
  "PHOTO" BLOB,
  "NOTES" VARCHAR(600 BYTE),
  "REPORTSTO" NUMBER,
  "PHOTOPATH" VARCHAR(255 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "EMPLOYEES" ADD CONSTRAINT "PK_EMPLOYEES" PRIMARY KEY("EMPLOYEEID");

DROP TABLE IF EXISTS "EMPLOYEETERRITORIES" CASCADE CONSTRAINTS;
CREATE TABLE "EMPLOYEETERRITORIES"
(
  "EMPLOYEEID" NUMBER NOT NULL,
  "TERRITORYID" VARCHAR(20 BYTE) NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "EMPLOYEETERRITORIES" ADD CONSTRAINT "PK_EMPTERRITORIES" PRIMARY KEY("EMPLOYEEID", "TERRITORYID");

DROP TABLE IF EXISTS "CUSTOMERDEMOGRAPHICS" CASCADE CONSTRAINTS;
CREATE TABLE "CUSTOMERDEMOGRAPHICS"
(
  "CUSTOMERTYPEID" CHAR(10 BYTE) NOT NULL,
  "CUSTOMERDESC" VARCHAR(4000 CHAR)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "CUSTOMERDEMOGRAPHICS" ADD CONSTRAINT "PK_CUSTOMERDEMOGRAPHICS" PRIMARY KEY("CUSTOMERTYPEID");

DROP TABLE IF EXISTS "CUSTOMERCUSTOMERDEMO" CASCADE CONSTRAINTS;
CREATE TABLE "CUSTOMERCUSTOMERDEMO"
(
  "CUSTOMERID" CHAR(5 BYTE) NOT NULL,
  "CUSTOMERTYPEID" CHAR(10 BYTE) NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "CUSTOMERCUSTOMERDEMO" ADD CONSTRAINT "PK_CUSTOMERDEMO" PRIMARY KEY("CUSTOMERID", "CUSTOMERTYPEID");

DROP TABLE IF EXISTS "ORDERS" CASCADE CONSTRAINTS;
CREATE TABLE "ORDERS"
(
  "ORDERID" NUMBER NOT NULL,
  "CUSTOMERID" CHAR(5 BYTE),
  "EMPLOYEEID" NUMBER,
  "TERRITORYID" VARCHAR(20 BYTE),
  "ORDERDATE" TIMESTAMP(6),
  "REQUIREDDATE" TIMESTAMP(6),
  "SHIPPEDDATE" TIMESTAMP(6),
  "SHIPVIA" NUMBER,
  "FREIGHT" NUMBER,
  "SHIPNAME" VARCHAR(40 BYTE),
  "SHIPADDRESS" VARCHAR(60 BYTE),
  "SHIPCITY" VARCHAR(15 BYTE),
  "SHIPREGION" VARCHAR(15 BYTE),
  "SHIPPOSTALCODE" VARCHAR(10 BYTE),
  "SHIPCOUNTRY" VARCHAR(15 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "ORDERS" ADD CONSTRAINT "PK_ORDERS" PRIMARY KEY("ORDERID");

DROP TABLE IF EXISTS "ORDERDETAILS" CASCADE CONSTRAINTS;
CREATE TABLE "ORDERDETAILS"
(
  "ORDERID" NUMBER NOT NULL,
  "PRODUCTID" NUMBER NOT NULL,
  "UNITPRICE" NUMBER NOT NULL,
  "QUANTITY" NUMBER NOT NULL,
  "DISCOUNT" NUMBER NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "PK_ORDER_DETAILS" PRIMARY KEY("ORDERID", "PRODUCTID");
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "CK_DISCOUNT" CHECK((Discount >= 0 and Discount <= 1));
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "CK_QUANTITY" CHECK((Quantity > 0));
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "CK_UNITPRICE" CHECK((UnitPrice >= 0));

DROP TABLE IF EXISTS "WEBSCRIPTS" CASCADE CONSTRAINTS;
CREATE TABLE "WEBSCRIPTS"
(
  "NAME" VARCHAR(60 BYTE) NOT NULL,
  "SCRIPT" CLOB
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;

DROP TABLE IF EXISTS "TB_SUPPLIER" CASCADE CONSTRAINTS;
CREATE TABLE "TB_SUPPLIER"
(
  "SUPPLIER_ID" NUMBER NOT NULL,
  "SUPPLIER_NAME" VARCHAR(50 BYTE) NOT NULL,
  "CONTACT_NAME" VARCHAR(50 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "TB_SUPPLIER" ADD CONSTRAINT "PK_SUPPLIER" PRIMARY KEY("SUPPLIER_ID");

DROP TABLE IF EXISTS "TB_PRODUCTS" CASCADE CONSTRAINTS;
CREATE TABLE "TB_PRODUCTS"
(
  "PRODUCT_ID" NUMBER NOT NULL,
  "PRODUCT_NAME" VARCHAR(100 BYTE),
  "SUPPLIER_ID" NUMBER NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;

ALTER TABLE "TERRITORIES" ADD CONSTRAINT "FK_TERRITORIES_REGION" FOREIGN KEY("REGIONID") REFERENCES "REGION"("REGIONID");
ALTER TABLE "CATEGORIES" ADD CONSTRAINT "FK_CATEGORIES_CATEGORIES" FOREIGN KEY("SUPERID") REFERENCES "CATEGORIES"("CATEGORYID");
ALTER TABLE "PRODUCTS" ADD CONSTRAINT "FK_PRODUCTS_CATEGORIES" FOREIGN KEY("CATEGORYID") REFERENCES "CATEGORIES"("CATEGORYID");
ALTER TABLE "PRODUCTS" ADD CONSTRAINT "FK_PRODUCTS_SUPPLIERS" FOREIGN KEY("SUPPLIERID") REFERENCES "SUPPLIERS"("SUPPLIERID");
ALTER TABLE "EMPLOYEES" ADD CONSTRAINT "FK_EMPLOYEES_EMPLOYEES" FOREIGN KEY("REPORTSTO") REFERENCES "EMPLOYEES"("EMPLOYEEID");
ALTER TABLE "EMPLOYEETERRITORIES" ADD CONSTRAINT "FK_EMPTERRI_EMPLOYEES" FOREIGN KEY("EMPLOYEEID") REFERENCES "EMPLOYEES"("EMPLOYEEID");
ALTER TABLE "EMPLOYEETERRITORIES" ADD CONSTRAINT "FK_EMPTERRI_TERRITORIES" FOREIGN KEY("TERRITORYID") REFERENCES "TERRITORIES"("TERRITORYID");
ALTER TABLE "CUSTOMERCUSTOMERDEMO" ADD CONSTRAINT "FK_CUSTOMERDEMO" FOREIGN KEY("CUSTOMERTYPEID") REFERENCES "CUSTOMERDEMOGRAPHICS"("CUSTOMERTYPEID");
ALTER TABLE "CUSTOMERCUSTOMERDEMO" ADD CONSTRAINT "FK_CUSTOMERDEMO_CUSTOMERS" FOREIGN KEY("CUSTOMERID") REFERENCES "CUSTOMERS"("CUSTOMERID");
ALTER TABLE "ORDERS" ADD CONSTRAINT "FK_ORDERS_CUSTOMERS" FOREIGN KEY("CUSTOMERID") REFERENCES "CUSTOMERS"("CUSTOMERID");
ALTER TABLE "ORDERS" ADD CONSTRAINT "FK_ORDERS_EMPLOYEES" FOREIGN KEY("EMPLOYEEID") REFERENCES "EMPLOYEES"("EMPLOYEEID");
ALTER TABLE "ORDERS" ADD CONSTRAINT "FK_ORDERS_SHIPPERS" FOREIGN KEY("SHIPVIA") REFERENCES "SHIPPERS"("SHIPPERID");
ALTER TABLE "ORDERS" ADD CONSTRAINT "FK_ORDERS_TERRITORIES" FOREIGN KEY("TERRITORYID") REFERENCES "TERRITORIES"("TERRITORYID");
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "FK_ORDERDETAILS_ORDERS" FOREIGN KEY("ORDERID") REFERENCES "ORDERS"("ORDERID");
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "FK_ORDERDETAILS_PRODUCTS" FOREIGN KEY("PRODUCTID") REFERENCES "PRODUCTS"("PRODUCTID");
ALTER TABLE "TB_PRODUCTS" ADD FOREIGN KEY("SUPPLIER_ID") REFERENCES "TB_SUPPLIER"("SUPPLIER_ID");
Logical export succeeded.

SQL> exp users=northwind file = "stdout" quote_names=Y;
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = NORTHWIND
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema NORTHWIND ...
Exporting sequence of schema NORTHWIND ...
Exporting profile of schema NORTHWIND ...
Exporting type of schema NORTHWIND ...
Exporting tables of schema NORTHWIND ...
Reading table objects of NORTHWIND

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
REGION                                                           1         
TERRITORIES                                                      2         
CATEGORIES                                                       3         
SUPPLIERS                                                        4         
PRODUCTS                                                         5         
SHIPPERS                                                         6         
CUSTOMERS                                                        7         
EMPLOYEES                                                        8         
EMPLOYEETERRITORIES                                              9         
CUSTOMERDEMOGRAPHICS                                             10        
CUSTOMERCUSTOMERDEMO                                             11        
ORDERS                                                           12        
ORDERDETAILS                                                     13        
WEBSCRIPTS                                                       14        
TB_SUPPLIER                                                      15        
TB_PRODUCTS                                                      16        

Exporting tables (scripts or data) of NORTHWIND
exporting table NORTHWIND.REGION ...
  exporting DDL of NORTHWIND.REGION ...
  exporting data of NORTHWIND.REGION ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.REGION ...
  exporting constraints on NORTHWIND.REGION ...

exporting table NORTHWIND.TERRITORIES ...
  exporting DDL of NORTHWIND.TERRITORIES ...
  exporting data of NORTHWIND.TERRITORIES ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.TERRITORIES ...
  exporting constraints on NORTHWIND.TERRITORIES ...

exporting table NORTHWIND.CATEGORIES ...
  exporting DDL of NORTHWIND.CATEGORIES ...
  exporting data of NORTHWIND.CATEGORIES ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.CATEGORIES ...
  exporting constraints on NORTHWIND.CATEGORIES ...

exporting table NORTHWIND.SUPPLIERS ...
  exporting DDL of NORTHWIND.SUPPLIERS ...
  exporting data of NORTHWIND.SUPPLIERS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.SUPPLIERS ...
  exporting constraints on NORTHWIND.SUPPLIERS ...

exporting table NORTHWIND.PRODUCTS ...
  exporting DDL of NORTHWIND.PRODUCTS ...
  exporting data of NORTHWIND.PRODUCTS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.PRODUCTS ...
  exporting constraints on NORTHWIND.PRODUCTS ...

exporting table NORTHWIND.SHIPPERS ...
  exporting DDL of NORTHWIND.SHIPPERS ...
  exporting data of NORTHWIND.SHIPPERS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.SHIPPERS ...
  exporting constraints on NORTHWIND.SHIPPERS ...

exporting table NORTHWIND.CUSTOMERS ...
  exporting DDL of NORTHWIND.CUSTOMERS ...
  exporting data of NORTHWIND.CUSTOMERS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.CUSTOMERS ...
  exporting constraints on NORTHWIND.CUSTOMERS ...

exporting table NORTHWIND.EMPLOYEES ...
  exporting DDL of NORTHWIND.EMPLOYEES ...
  exporting data of NORTHWIND.EMPLOYEES ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.EMPLOYEES ...
  exporting constraints on NORTHWIND.EMPLOYEES ...

exporting table NORTHWIND.EMPLOYEETERRITORIES ...
  exporting DDL of NORTHWIND.EMPLOYEETERRITORIES ...
  exporting data of NORTHWIND.EMPLOYEETERRITORIES ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.EMPLOYEETERRITORIES ...
  exporting constraints on NORTHWIND.EMPLOYEETERRITORIES ...

exporting table NORTHWIND.CUSTOMERDEMOGRAPHICS ...
  exporting DDL of NORTHWIND.CUSTOMERDEMOGRAPHICS ...
  exporting data of NORTHWIND.CUSTOMERDEMOGRAPHICS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.CUSTOMERDEMOGRAPHICS ...
  exporting constraints on NORTHWIND.CUSTOMERDEMOGRAPHICS ...

exporting table NORTHWIND.CUSTOMERCUSTOMERDEMO ...
  exporting DDL of NORTHWIND.CUSTOMERCUSTOMERDEMO ...
  exporting data of NORTHWIND.CUSTOMERCUSTOMERDEMO ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.CUSTOMERCUSTOMERDEMO ...
  exporting constraints on NORTHWIND.CUSTOMERCUSTOMERDEMO ...

exporting table NORTHWIND.ORDERS ...
  exporting DDL of NORTHWIND.ORDERS ...
  exporting data of NORTHWIND.ORDERS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.ORDERS ...
  exporting constraints on NORTHWIND.ORDERS ...

exporting table NORTHWIND.ORDERDETAILS ...
  exporting DDL of NORTHWIND.ORDERDETAILS ...
  exporting data of NORTHWIND.ORDERDETAILS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.ORDERDETAILS ...
  exporting constraints on NORTHWIND.ORDERDETAILS ...

exporting table NORTHWIND.WEBSCRIPTS ...
  exporting DDL of NORTHWIND.WEBSCRIPTS ...
  exporting data of NORTHWIND.WEBSCRIPTS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.WEBSCRIPTS ...
  exporting constraints on NORTHWIND.WEBSCRIPTS ...

exporting table NORTHWIND.TB_SUPPLIER ...
  exporting DDL of NORTHWIND.TB_SUPPLIER ...
  exporting data of NORTHWIND.TB_SUPPLIER ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.TB_SUPPLIER ...
  exporting constraints on NORTHWIND.TB_SUPPLIER ...

exporting table NORTHWIND.TB_PRODUCTS ...
  exporting DDL of NORTHWIND.TB_PRODUCTS ...
  exporting data of NORTHWIND.TB_PRODUCTS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.TB_PRODUCTS ...
  exporting constraints on NORTHWIND.TB_PRODUCTS ...

Exporting procedures/functions/triggers of schema NORTHWIND ...
Exporting views of schema NORTHWIND ...
Exporting synonyms of schema NORTHWIND ...
Exporting package of schema NORTHWIND ...
End of export schema NORTHWIND ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = NORTHWIND
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = NORTHWIND;
DROP TABLE IF EXISTS "REGION" CASCADE CONSTRAINTS;
CREATE TABLE "REGION"
(
  "REGIONID" NUMBER NOT NULL,
  "REGIONDESCRIPTION" CHAR(50 CHAR) NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "REGION" ADD CONSTRAINT "PK_REGION" PRIMARY KEY("REGIONID");

DROP TABLE IF EXISTS "TERRITORIES" CASCADE CONSTRAINTS;
CREATE TABLE "TERRITORIES"
(
  "TERRITORYID" VARCHAR(20 BYTE) NOT NULL,
  "TERRITORYDESCRIPTION" CHAR(50 CHAR) NOT NULL,
  "REGIONID" NUMBER NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "TERRITORIES" ADD CONSTRAINT "PK_TERRITORIES" PRIMARY KEY("TERRITORYID");

DROP TABLE IF EXISTS "CATEGORIES" CASCADE CONSTRAINTS;
CREATE TABLE "CATEGORIES"
(
  "CATEGORYID" NUMBER NOT NULL,
  "SUPERID" NUMBER,
  "CATEGORYNAME" VARCHAR(15 BYTE) NOT NULL,
  "DESCRIPTION" VARCHAR(300 CHAR),
  "PICTURE" BLOB
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "CATEGORIES" ADD CONSTRAINT "PK_CATEGORIES" PRIMARY KEY("CATEGORYID");

DROP TABLE IF EXISTS "SUPPLIERS" CASCADE CONSTRAINTS;
CREATE TABLE "SUPPLIERS"
(
  "SUPPLIERID" NUMBER NOT NULL,
  "COMPANYNAME" VARCHAR(40 CHAR) NOT NULL,
  "CONTACTNAME" VARCHAR(30 BYTE),
  "CONTACTTITLE" VARCHAR(30 BYTE),
  "ADDRESS" VARCHAR(60 BYTE),
  "CITY" VARCHAR(15 BYTE),
  "REGION" VARCHAR(15 BYTE),
  "POSTALCODE" VARCHAR(10 BYTE),
  "COUNTRY" VARCHAR(15 BYTE),
  "PHONE" VARCHAR(24 BYTE),
  "FAX" VARCHAR(24 BYTE),
  "HOMEPAGE" VARCHAR(200 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "SUPPLIERS" ADD CONSTRAINT "PK_SUPPLIERS" PRIMARY KEY("SUPPLIERID");

DROP TABLE IF EXISTS "PRODUCTS" CASCADE CONSTRAINTS;
CREATE TABLE "PRODUCTS"
(
  "PRODUCTID" NUMBER NOT NULL,
  "PRODUCTNAME" VARCHAR(40 BYTE) NOT NULL,
  "SUPPLIERID" NUMBER,
  "CATEGORYID" NUMBER,
  "QUANTITYPERUNIT" VARCHAR(20 BYTE),
  "UNITPRICE" NUMBER,
  "UNITSINSTOCK" NUMBER,
  "UNITSONORDER" NUMBER,
  "REORDERLEVEL" NUMBER,
  "DISCONTINUED" NUMBER(1) NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "PRODUCTS" ADD CONSTRAINT "PK_PRODUCTS" PRIMARY KEY("PRODUCTID");
ALTER TABLE "PRODUCTS" ADD CONSTRAINT "CK_PRODUCTS_UNITPRICE" CHECK((UnitPrice >= 0));
ALTER TABLE "PRODUCTS" ADD CONSTRAINT "CK_REORDERLEVEL" CHECK((ReorderLevel >= 0));
ALTER TABLE "PRODUCTS" ADD CONSTRAINT "CK_UNITSINSTOCK" CHECK((UnitsInStock >= 0));
ALTER TABLE "PRODUCTS" ADD CONSTRAINT "CK_UNITSONORDER" CHECK((UnitsOnOrder >= 0));

DROP TABLE IF EXISTS "SHIPPERS" CASCADE CONSTRAINTS;
CREATE TABLE "SHIPPERS"
(
  "SHIPPERID" NUMBER NOT NULL,
  "COMPANYNAME" VARCHAR(40 BYTE) NOT NULL,
  "PHONE" VARCHAR(24 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "SHIPPERS" ADD CONSTRAINT "PK_SHIPPERS" PRIMARY KEY("SHIPPERID");

DROP TABLE IF EXISTS "CUSTOMERS" CASCADE CONSTRAINTS;
CREATE TABLE "CUSTOMERS"
(
  "CUSTOMERID" CHAR(5 BYTE) NOT NULL,
  "COMPANYNAME" VARCHAR(40 BYTE) NOT NULL,
  "CONTACTNAME" VARCHAR(30 BYTE),
  "CONTACTTITLE" VARCHAR(30 BYTE),
  "ADDRESS" VARCHAR(60 BYTE),
  "CITY" VARCHAR(15 BYTE),
  "REGION" VARCHAR(15 BYTE),
  "POSTALCODE" VARCHAR(10 BYTE),
  "COUNTRY" VARCHAR(15 BYTE),
  "PHONE" VARCHAR(24 BYTE),
  "FAX" VARCHAR(24 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "CUSTOMERS" ADD CONSTRAINT "PK_CUSTOMERS" PRIMARY KEY("CUSTOMERID");

DROP TABLE IF EXISTS "EMPLOYEES" CASCADE CONSTRAINTS;
CREATE TABLE "EMPLOYEES"
(
  "EMPLOYEEID" NUMBER NOT NULL,
  "LASTNAME" VARCHAR(20 BYTE) NOT NULL,
  "FIRSTNAME" VARCHAR(10 BYTE) NOT NULL,
  "TITLE" VARCHAR(30 BYTE),
  "TITLEOFCOURTESY" VARCHAR(25 BYTE),
  "BIRTHDATE" DATE,
  "HIREDATE" DATE,
  "ADDRESS" VARCHAR(60 BYTE),
  "CITY" VARCHAR(15 BYTE),
  "REGION" VARCHAR(15 BYTE),
  "POSTALCODE" VARCHAR(10 BYTE),
  "COUNTRY" VARCHAR(15 BYTE),
  "HOMEPHONE" VARCHAR(24 BYTE),
  "EXTENSION" VARCHAR(4 BYTE),
  "PHOTO" BLOB,
  "NOTES" VARCHAR(600 BYTE),
  "REPORTSTO" NUMBER,
  "PHOTOPATH" VARCHAR(255 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "EMPLOYEES" ADD CONSTRAINT "PK_EMPLOYEES" PRIMARY KEY("EMPLOYEEID");

DROP TABLE IF EXISTS "EMPLOYEETERRITORIES" CASCADE CONSTRAINTS;
CREATE TABLE "EMPLOYEETERRITORIES"
(
  "EMPLOYEEID" NUMBER NOT NULL,
  "TERRITORYID" VARCHAR(20 BYTE) NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "EMPLOYEETERRITORIES" ADD CONSTRAINT "PK_EMPTERRITORIES" PRIMARY KEY("EMPLOYEEID", "TERRITORYID");

DROP TABLE IF EXISTS "CUSTOMERDEMOGRAPHICS" CASCADE CONSTRAINTS;
CREATE TABLE "CUSTOMERDEMOGRAPHICS"
(
  "CUSTOMERTYPEID" CHAR(10 BYTE) NOT NULL,
  "CUSTOMERDESC" VARCHAR(4000 CHAR)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "CUSTOMERDEMOGRAPHICS" ADD CONSTRAINT "PK_CUSTOMERDEMOGRAPHICS" PRIMARY KEY("CUSTOMERTYPEID");

DROP TABLE IF EXISTS "CUSTOMERCUSTOMERDEMO" CASCADE CONSTRAINTS;
CREATE TABLE "CUSTOMERCUSTOMERDEMO"
(
  "CUSTOMERID" CHAR(5 BYTE) NOT NULL,
  "CUSTOMERTYPEID" CHAR(10 BYTE) NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "CUSTOMERCUSTOMERDEMO" ADD CONSTRAINT "PK_CUSTOMERDEMO" PRIMARY KEY("CUSTOMERID", "CUSTOMERTYPEID");

DROP TABLE IF EXISTS "ORDERS" CASCADE CONSTRAINTS;
CREATE TABLE "ORDERS"
(
  "ORDERID" NUMBER NOT NULL,
  "CUSTOMERID" CHAR(5 BYTE),
  "EMPLOYEEID" NUMBER,
  "TERRITORYID" VARCHAR(20 BYTE),
  "ORDERDATE" TIMESTAMP(6),
  "REQUIREDDATE" TIMESTAMP(6),
  "SHIPPEDDATE" TIMESTAMP(6),
  "SHIPVIA" NUMBER,
  "FREIGHT" NUMBER,
  "SHIPNAME" VARCHAR(40 BYTE),
  "SHIPADDRESS" VARCHAR(60 BYTE),
  "SHIPCITY" VARCHAR(15 BYTE),
  "SHIPREGION" VARCHAR(15 BYTE),
  "SHIPPOSTALCODE" VARCHAR(10 BYTE),
  "SHIPCOUNTRY" VARCHAR(15 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "ORDERS" ADD CONSTRAINT "PK_ORDERS" PRIMARY KEY("ORDERID");

DROP TABLE IF EXISTS "ORDERDETAILS" CASCADE CONSTRAINTS;
CREATE TABLE "ORDERDETAILS"
(
  "ORDERID" NUMBER NOT NULL,
  "PRODUCTID" NUMBER NOT NULL,
  "UNITPRICE" NUMBER NOT NULL,
  "QUANTITY" NUMBER NOT NULL,
  "DISCOUNT" NUMBER NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "PK_ORDER_DETAILS" PRIMARY KEY("ORDERID", "PRODUCTID");
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "CK_DISCOUNT" CHECK((Discount >= 0 and Discount <= 1));
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "CK_QUANTITY" CHECK((Quantity > 0));
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "CK_UNITPRICE" CHECK((UnitPrice >= 0));

DROP TABLE IF EXISTS "WEBSCRIPTS" CASCADE CONSTRAINTS;
CREATE TABLE "WEBSCRIPTS"
(
  "NAME" VARCHAR(60 BYTE) NOT NULL,
  "SCRIPT" CLOB
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;

DROP TABLE IF EXISTS "TB_SUPPLIER" CASCADE CONSTRAINTS;
CREATE TABLE "TB_SUPPLIER"
(
  "SUPPLIER_ID" NUMBER NOT NULL,
  "SUPPLIER_NAME" VARCHAR(50 BYTE) NOT NULL,
  "CONTACT_NAME" VARCHAR(50 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "TB_SUPPLIER" ADD CONSTRAINT "PK_SUPPLIER" PRIMARY KEY("SUPPLIER_ID");

DROP TABLE IF EXISTS "TB_PRODUCTS" CASCADE CONSTRAINTS;
CREATE TABLE "TB_PRODUCTS"
(
  "PRODUCT_ID" NUMBER NOT NULL,
  "PRODUCT_NAME" VARCHAR(100 BYTE),
  "SUPPLIER_ID" NUMBER NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;

ALTER TABLE "TERRITORIES" ADD CONSTRAINT "FK_TERRITORIES_REGION" FOREIGN KEY("REGIONID") REFERENCES "REGION"("REGIONID");
ALTER TABLE "CATEGORIES" ADD CONSTRAINT "FK_CATEGORIES_CATEGORIES" FOREIGN KEY("SUPERID") REFERENCES "CATEGORIES"("CATEGORYID");
ALTER TABLE "PRODUCTS" ADD CONSTRAINT "FK_PRODUCTS_CATEGORIES" FOREIGN KEY("CATEGORYID") REFERENCES "CATEGORIES"("CATEGORYID");
ALTER TABLE "PRODUCTS" ADD CONSTRAINT "FK_PRODUCTS_SUPPLIERS" FOREIGN KEY("SUPPLIERID") REFERENCES "SUPPLIERS"("SUPPLIERID");
ALTER TABLE "EMPLOYEES" ADD CONSTRAINT "FK_EMPLOYEES_EMPLOYEES" FOREIGN KEY("REPORTSTO") REFERENCES "EMPLOYEES"("EMPLOYEEID");
ALTER TABLE "EMPLOYEETERRITORIES" ADD CONSTRAINT "FK_EMPTERRI_EMPLOYEES" FOREIGN KEY("EMPLOYEEID") REFERENCES "EMPLOYEES"("EMPLOYEEID");
ALTER TABLE "EMPLOYEETERRITORIES" ADD CONSTRAINT "FK_EMPTERRI_TERRITORIES" FOREIGN KEY("TERRITORYID") REFERENCES "TERRITORIES"("TERRITORYID");
ALTER TABLE "CUSTOMERCUSTOMERDEMO" ADD CONSTRAINT "FK_CUSTOMERDEMO" FOREIGN KEY("CUSTOMERTYPEID") REFERENCES "CUSTOMERDEMOGRAPHICS"("CUSTOMERTYPEID");
ALTER TABLE "CUSTOMERCUSTOMERDEMO" ADD CONSTRAINT "FK_CUSTOMERDEMO_CUSTOMERS" FOREIGN KEY("CUSTOMERID") REFERENCES "CUSTOMERS"("CUSTOMERID");
ALTER TABLE "ORDERS" ADD CONSTRAINT "FK_ORDERS_CUSTOMERS" FOREIGN KEY("CUSTOMERID") REFERENCES "CUSTOMERS"("CUSTOMERID");
ALTER TABLE "ORDERS" ADD CONSTRAINT "FK_ORDERS_EMPLOYEES" FOREIGN KEY("EMPLOYEEID") REFERENCES "EMPLOYEES"("EMPLOYEEID");
ALTER TABLE "ORDERS" ADD CONSTRAINT "FK_ORDERS_SHIPPERS" FOREIGN KEY("SHIPVIA") REFERENCES "SHIPPERS"("SHIPPERID");
ALTER TABLE "ORDERS" ADD CONSTRAINT "FK_ORDERS_TERRITORIES" FOREIGN KEY("TERRITORYID") REFERENCES "TERRITORIES"("TERRITORYID");
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "FK_ORDERDETAILS_ORDERS" FOREIGN KEY("ORDERID") REFERENCES "ORDERS"("ORDERID");
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "FK_ORDERDETAILS_PRODUCTS" FOREIGN KEY("PRODUCTID") REFERENCES "PRODUCTS"("PRODUCTID");
ALTER TABLE "TB_PRODUCTS" ADD FOREIGN KEY("SUPPLIER_ID") REFERENCES "TB_SUPPLIER"("SUPPLIER_ID");
Logical export succeeded.

SQL> 
SQL> conn northwind/exp_user123@127.0.0.1:1611

connected.

SQL> comment on table OrderDetails is 'Test table comments'
  2 /

Succeed.

SQL> comment on column OrderDetails.ProductID is 'Test column comments'
  2 /

Succeed.

SQL> 
SQL> exp tables=OrderDetails file = "stdout" quote_names=Y;
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = ORDERDETAILS
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table NORTHWIND.ORDERDETAILS ...
  exporting DDL of NORTHWIND.ORDERDETAILS ...
  exporting data of NORTHWIND.ORDERDETAILS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.ORDERDETAILS ...
  exporting constraints on NORTHWIND.ORDERDETAILS ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = ORDERDETAILS
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "ORDERDETAILS" CASCADE CONSTRAINTS;
CREATE TABLE "ORDERDETAILS"
(
  "ORDERID" NUMBER NOT NULL,
  "PRODUCTID" NUMBER NOT NULL,
  "UNITPRICE" NUMBER NOT NULL,
  "QUANTITY" NUMBER NOT NULL,
  "DISCOUNT" NUMBER NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
COMMENT ON TABLE "ORDERDETAILS" IS 'Test table comments';
COMMENT ON COLUMN "ORDERDETAILS"."PRODUCTID" IS 'Test column comments';
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "PK_ORDER_DETAILS" PRIMARY KEY("ORDERID", "PRODUCTID");
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "CK_DISCOUNT" CHECK((Discount >= 0 and Discount <= 1));
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "CK_QUANTITY" CHECK((Quantity > 0));
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "CK_UNITPRICE" CHECK((UnitPrice >= 0));

ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "FK_ORDERDETAILS_ORDERS" FOREIGN KEY("ORDERID") REFERENCES "ORDERS"("ORDERID");
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "FK_ORDERDETAILS_PRODUCTS" FOREIGN KEY("PRODUCTID") REFERENCES "PRODUCTS"("PRODUCTID");

Logical export succeeded.

SQL> exp tables=OrderDetails file = "stdout" quote_names=Y skip_comments=y;
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = ORDERDETAILS
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = Y
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table NORTHWIND.ORDERDETAILS ...
  exporting DDL of NORTHWIND.ORDERDETAILS ...
  exporting data of NORTHWIND.ORDERDETAILS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on NORTHWIND.ORDERDETAILS ...
  exporting constraints on NORTHWIND.ORDERDETAILS ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = ORDERDETAILS
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = Y
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "ORDERDETAILS" CASCADE CONSTRAINTS;
CREATE TABLE "ORDERDETAILS"
(
  "ORDERID" NUMBER NOT NULL,
  "PRODUCTID" NUMBER NOT NULL,
  "UNITPRICE" NUMBER NOT NULL,
  "QUANTITY" NUMBER NOT NULL,
  "DISCOUNT" NUMBER NOT NULL
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "PK_ORDER_DETAILS" PRIMARY KEY("ORDERID", "PRODUCTID");
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "CK_DISCOUNT" CHECK((Discount >= 0 and Discount <= 1));
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "CK_QUANTITY" CHECK((Quantity > 0));
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "CK_UNITPRICE" CHECK((UnitPrice >= 0));

ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "FK_ORDERDETAILS_ORDERS" FOREIGN KEY("ORDERID") REFERENCES "ORDERS"("ORDERID");
ALTER TABLE "ORDERDETAILS" ADD CONSTRAINT "FK_ORDERDETAILS_PRODUCTS" FOREIGN KEY("PRODUCTID") REFERENCES "PRODUCTS"("PRODUCTID");

Logical export succeeded.

SQL> 
SQL> conn exp_user1/exp_user123@127.0.0.1:1611

connected.

SQL> exp tables=t_test_batch feedback=5 file = "t_test_batch.sql";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = T_TEST_BATCH
-- FILE TYPE = TXT
-- DUMP FILE = t_test_batch.sql
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 5
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER1.T_TEST_BATCH ...
  exporting DDL of EXP_USER1.T_TEST_BATCH ...
  exporting data of EXP_USER1.T_TEST_BATCH ...
    5 rows are dumped.
    10 rows are dumped.
    15 rows are dumped.
    20 rows are dumped.
    data exporting success, 20 rows are dumped.

  exporting indexes on EXP_USER1.T_TEST_BATCH ...
  exporting constraints on EXP_USER1.T_TEST_BATCH ...

Logical export succeeded.

SQL> 
SQL> exp tables=% file = "all_table.sql";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = ALL_TABLES
-- EXPORT OBJECTS = 
-- FILE TYPE = TXT
-- DUMP FILE = all_table.sql
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Reading table objects of EXP_USER1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
HOTELAVAILABILITY                                                1         
ADMIN_EMP                                                        2         
T_TEST_CONS1                                                     3         
T_TEST_CONS2                                                     4         
T_TEST_BATCH                                                     5         

Exporting tables (scripts or data) of EXP_USER1
exporting table EXP_USER1.HOTELAVAILABILITY ...
  exporting DDL of EXP_USER1.HOTELAVAILABILITY ...
  exporting data of EXP_USER1.HOTELAVAILABILITY ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.HOTELAVAILABILITY ...
  exporting constraints on EXP_USER1.HOTELAVAILABILITY ...

exporting table EXP_USER1.ADMIN_EMP ...
  exporting DDL of EXP_USER1.ADMIN_EMP ...
  exporting data of EXP_USER1.ADMIN_EMP ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.ADMIN_EMP ...
  exporting constraints on EXP_USER1.ADMIN_EMP ...

exporting table EXP_USER1.T_TEST_CONS1 ...
  exporting DDL of EXP_USER1.T_TEST_CONS1 ...
  exporting data of EXP_USER1.T_TEST_CONS1 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.T_TEST_CONS1 ...
  exporting constraints on EXP_USER1.T_TEST_CONS1 ...

exporting table EXP_USER1.T_TEST_CONS2 ...
  exporting DDL of EXP_USER1.T_TEST_CONS2 ...
  exporting data of EXP_USER1.T_TEST_CONS2 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.T_TEST_CONS2 ...
  exporting constraints on EXP_USER1.T_TEST_CONS2 ...

exporting table EXP_USER1.T_TEST_BATCH ...
  exporting DDL of EXP_USER1.T_TEST_BATCH ...
  exporting data of EXP_USER1.T_TEST_BATCH ...
    data exporting success, 20 rows are dumped.

  exporting indexes on EXP_USER1.T_TEST_BATCH ...
  exporting constraints on EXP_USER1.T_TEST_BATCH ...

Logical export succeeded.

SQL> 
SQL> CREATE GLOBAL TEMPORARY TABLE T_TEST_TEMP11
  2 (
  3   ID INTEGER
  4 )ON COMMIT PRESERVE ROWS;

Succeed.

SQL> 
SQL> CREATE GLOBAL TEMPORARY TABLE T_TEST_TEMP22
  2 (
  3   ID INTEGER
  4 )ON COMMIT DELETE ROWS;

Succeed.

SQL> 
SQL> exp tables=T_TEST_TEMP11,T_TEST_TEMP22 file = "temp_table.sql";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = T_TEST_TEMP11, T_TEST_TEMP22
-- FILE TYPE = TXT
-- DUMP FILE = temp_table.sql
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER1.T_TEST_TEMP11 ...
  exporting DDL of EXP_USER1.T_TEST_TEMP11 ...
  skipping to export the data of temporary table
  exporting indexes on EXP_USER1.T_TEST_TEMP11 ...
  exporting constraints on EXP_USER1.T_TEST_TEMP11 ...

exporting table EXP_USER1.T_TEST_TEMP22 ...
  exporting DDL of EXP_USER1.T_TEST_TEMP22 ...
  skipping to export the data of temporary table
  exporting indexes on EXP_USER1.T_TEST_TEMP22 ...
  exporting constraints on EXP_USER1.T_TEST_TEMP22 ...

Logical export succeeded.

SQL> 
SQL> CREATE TABLE part_andy111
  2 (
  3     andy_ID NUMBER,
  4     andy_ID2 NUMBER,
  5     FIRST_NAME  VARCHAR2(30) NOT NULL,
  6     LAST_NAME   VARCHAR2(30) NOT NULL,
  7     PHONE        VARCHAR2(15) NOT NULL,
  8     EMAIL        VARCHAR2(80),
  9     STATUS       CHAR(1)
 10 )
 11 PARTITION BY RANGE (andy_ID, andy_ID2)
 12 (
 13     PARTITION PART1 VALUES LESS THAN (10000, 100) tablespace users,
 14     PARTITION PART2 VALUES LESS THAN (20000, 200),
 15     PARTITION PART3 VALUES LESS THAN (30000, 300)
 16 )
 17 TABLESPACE USERS;

Succeed.

SQL> 
SQL> create index ix_part_andy111 on part_andy111(andy_ID, andy_ID2) local;

Succeed.

SQL> alter table part_andy111 add constraint ix_part_andy111 primary key(andy_ID, andy_ID2);

Succeed.

SQL> alter table part_andy111 add constraint uk_part_andy111 unique(andy_ID, andy_ID2, FIRST_NAME);

Succeed.

SQL> 
SQL> create table T_BILLDATAINFOHIS
  2 (
  3   dataid           NUMBER(20) not null,
  4   undologid        NUMBER(20) default 0 not null,
  5   writetime        DATE default sysdate not null,
  6   movetime         DATE,
  7   content          VARCHAR2(4000) not null,
  8   servicetype      NUMBER(3) not null,
  9   datatype         NUMBER(6) not null,
 10   datastatus       NUMBER(3) default 1 not null,
 11   dataindex        NUMBER(20) default 0 not null,
 12   loopcount        INTEGER default 0,
 13   latestfailtime   DATE,
 14   phonenumber      VARCHAR2(24),
 15   latestfailreason VARCHAR2(500)
 16 )
 17 partition by range (WRITETIME)
 18 (
 19   partition JUN2019 values less than (TO_DATE('2019-07-01 00:00:00', 'YYYY-MM-DD HH24:MI:SS')),
 20   partition JUL2019 values less than (TO_DATE('2019-08-01 00:00:00', 'YYYY-MM-DD HH24:MI:SS')),
 21   partition AUG2019 values less than (TO_DATE('2019-09-01 00:00:00', 'YYYY-MM-DD HH24:MI:SS'))
 22 );

Succeed.

SQL> 
SQL> create table t_groupmemberinfo1
  2 (
  3   subgroupid        number(20)                       not null,
  4   groupid           number(20)                       not null,
  5   msisdn            varchar2(20)                     not null,
  6   updatetime        date           default sysdate   not null
  7 )
  8 partition by list(msisdn)
  9 (
 10     partition GRP0 values('ab'),
 11     partition GRP1 values('cd')
 12 ); 

Succeed.

SQL> 
SQL> exp tables=part_andy111,T_BILLDATAINFOHIS,t_groupmemberinfo1  file="stdout";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = PART_ANDY111, T_BILLDATAINFOHIS, T_GROUPMEMBERINFO1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER1.PART_ANDY111 ...
  exporting DDL of EXP_USER1.PART_ANDY111 ...
  exporting data of EXP_USER1.PART_ANDY111 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.PART_ANDY111 ...
  exporting constraints on EXP_USER1.PART_ANDY111 ...

exporting table EXP_USER1.T_BILLDATAINFOHIS ...
  exporting DDL of EXP_USER1.T_BILLDATAINFOHIS ...
  exporting data of EXP_USER1.T_BILLDATAINFOHIS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.T_BILLDATAINFOHIS ...
  exporting constraints on EXP_USER1.T_BILLDATAINFOHIS ...

exporting table EXP_USER1.T_GROUPMEMBERINFO1 ...
  exporting DDL of EXP_USER1.T_GROUPMEMBERINFO1 ...
  exporting data of EXP_USER1.T_GROUPMEMBERINFO1 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.T_GROUPMEMBERINFO1 ...
  exporting constraints on EXP_USER1.T_GROUPMEMBERINFO1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = PART_ANDY111, T_BILLDATAINFOHIS, T_GROUPMEMBERINFO1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "PART_ANDY111" CASCADE CONSTRAINTS;
CREATE TABLE "PART_ANDY111"
(
  "ANDY_ID" NUMBER NOT NULL,
  "ANDY_ID2" NUMBER NOT NULL,
  "FIRST_NAME" VARCHAR(30 BYTE) NOT NULL,
  "LAST_NAME" VARCHAR(30 BYTE) NOT NULL,
  "PHONE" VARCHAR(15 BYTE) NOT NULL,
  "EMAIL" VARCHAR(80 BYTE),
  "STATUS" CHAR(1 BYTE)
)
PARTITION BY RANGE ("ANDY_ID", "ANDY_ID2")
(
    PARTITION PART1 VALUES LESS THAN (10000, 100) TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF,
    PARTITION PART2 VALUES LESS THAN (20000, 200) TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF,
    PARTITION PART3 VALUES LESS THAN (30000, 300) TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
CREATE INDEX "IX_PART_ANDY111" ON "PART_ANDY111"("ANDY_ID", "ANDY_ID2")
LOCAL
TABLESPACE "USERS"
INITRANS 2
PCTFREE 8;
ALTER TABLE "PART_ANDY111" ADD CONSTRAINT "IX_PART_ANDY111" PRIMARY KEY("ANDY_ID", "ANDY_ID2") USING INDEX LOCAL;
ALTER TABLE "PART_ANDY111" ADD CONSTRAINT "UK_PART_ANDY111" UNIQUE("ANDY_ID", "ANDY_ID2", "FIRST_NAME");

DROP TABLE IF EXISTS "T_BILLDATAINFOHIS" CASCADE CONSTRAINTS;
CREATE TABLE "T_BILLDATAINFOHIS"
(
  "DATAID" NUMBER(20) NOT NULL,
  "UNDOLOGID" NUMBER(20) NOT NULL DEFAULT 0,
  "WRITETIME" DATE NOT NULL DEFAULT sysdate,
  "MOVETIME" DATE,
  "CONTENT" VARCHAR(4000 BYTE) NOT NULL,
  "SERVICETYPE" NUMBER(3) NOT NULL,
  "DATATYPE" NUMBER(6) NOT NULL,
  "DATASTATUS" NUMBER(3) NOT NULL DEFAULT 1,
  "DATAINDEX" NUMBER(20) NOT NULL DEFAULT 0,
  "LOOPCOUNT" BINARY_INTEGER DEFAULT 0,
  "LATESTFAILTIME" DATE,
  "PHONENUMBER" VARCHAR(24 BYTE),
  "LATESTFAILREASON" VARCHAR(500 BYTE)
)
PARTITION BY RANGE ("WRITETIME")
(
    PARTITION JUN2019 VALUES LESS THAN (TO_DATE('2019-07-01 00:00:00', 'YYYY-MM-DD HH24:MI:SS')) TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF,
    PARTITION JUL2019 VALUES LESS THAN (TO_DATE('2019-08-01 00:00:00', 'YYYY-MM-DD HH24:MI:SS')) TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF,
    PARTITION AUG2019 VALUES LESS THAN (TO_DATE('2019-09-01 00:00:00', 'YYYY-MM-DD HH24:MI:SS')) TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;

DROP TABLE IF EXISTS "T_GROUPMEMBERINFO1" CASCADE CONSTRAINTS;
CREATE TABLE "T_GROUPMEMBERINFO1"
(
  "SUBGROUPID" NUMBER(20) NOT NULL,
  "GROUPID" NUMBER(20) NOT NULL,
  "MSISDN" VARCHAR(20 BYTE) NOT NULL,
  "UPDATETIME" DATE NOT NULL DEFAULT sysdate
)
PARTITION BY LIST ("MSISDN")
(
    PARTITION GRP0 VALUES('ab') TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF,
    PARTITION GRP1 VALUES('cd') TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;




Logical export succeeded.

SQL> 
SQL> CREATE TABLE t_userinfo_t3 (
  2 userid             number(20)                      not null,
  3 phonenumber        varchar2(128)                    not null,
  4 passwd             varchar2(40)                    not null,
  5 partition_num      integer not null
  6 )
  7 PARTITION BY HASH (userid,phonenumber)
  8 (
  9 PARTITION p1 tablespace users,
 10 PARTITION p2 tablespace users,
 11 PARTITION p3 tablespace users,
 12 PARTITION p4 tablespace users,
 13 PARTITION p5 tablespace users,
 14 PARTITION p6 tablespace users
 15 );

Succeed.

SQL> 
SQL> create table MS_BIGTABLE_LOG
  2 (
  3   record_date DATE,
  4   col_1       VARCHAR2(200)
  5  )
  6 PARTITION BY RANGE (record_date)
  7 INTERVAL (numtodsinterval(1,'day'))
  8 STORE IN(tablespace users)
  9 (
 10    PARTITION P1 VALUES LESS THAN (TO_DATE('2014-1-1', 'YYYY-MM-DD'))
 11 );

Succeed.

SQL> insert into MS_BIGTABLE_LOG values(TO_DATE('2018-1-1', 'YYYY-MM-DD'), '1');

1 rows affected.

SQL> 
SQL> create table MS_BIGTABLE_LOG2
  2 (
  3   record_date DATE,
  4   col_1       VARCHAR2(200)
  5  )
  6 PARTITION BY RANGE (record_date)
  7 INTERVAL (numtodsinterval(1,'day'))
  8 (
  9    PARTITION P1 VALUES LESS THAN (TO_DATE('2014-1-1', 'YYYY-MM-DD'))
 10 );

Succeed.

SQL> 
SQL> insert into MS_BIGTABLE_LOG2 values(TO_DATE('2018-1-1', 'YYYY-MM-DD'), '1');

1 rows affected.

SQL> 
SQL> create index MS_BIGTABLE_LOG2_idx on MS_BIGTABLE_LOG2(record_date) local
  2 /

Succeed.

SQL> 
SQL> exp tables=t_userinfo_t3,MS_BIGTABLE_LOG,MS_BIGTABLE_LOG2  file="stdout";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = T_USERINFO_T3, MS_BIGTABLE_LOG, MS_BIGTABLE_LOG2
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER1.T_USERINFO_T3 ...
  exporting DDL of EXP_USER1.T_USERINFO_T3 ...
  exporting data of EXP_USER1.T_USERINFO_T3 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER1.T_USERINFO_T3 ...
  exporting constraints on EXP_USER1.T_USERINFO_T3 ...

exporting table EXP_USER1.MS_BIGTABLE_LOG ...
  exporting DDL of EXP_USER1.MS_BIGTABLE_LOG ...
  exporting data of EXP_USER1.MS_BIGTABLE_LOG ...
    data exporting success, 1 rows are dumped.

  exporting indexes on EXP_USER1.MS_BIGTABLE_LOG ...
  exporting constraints on EXP_USER1.MS_BIGTABLE_LOG ...

exporting table EXP_USER1.MS_BIGTABLE_LOG2 ...
  exporting DDL of EXP_USER1.MS_BIGTABLE_LOG2 ...
  exporting data of EXP_USER1.MS_BIGTABLE_LOG2 ...
    data exporting success, 1 rows are dumped.

  exporting indexes on EXP_USER1.MS_BIGTABLE_LOG2 ...
  exporting constraints on EXP_USER1.MS_BIGTABLE_LOG2 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = T_USERINFO_T3, MS_BIGTABLE_LOG, MS_BIGTABLE_LOG2
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "T_USERINFO_T3" CASCADE CONSTRAINTS;
CREATE TABLE "T_USERINFO_T3"
(
  "USERID" NUMBER(20) NOT NULL,
  "PHONENUMBER" VARCHAR(128 BYTE) NOT NULL,
  "PASSWD" VARCHAR(40 BYTE) NOT NULL,
  "PARTITION_NUM" BINARY_INTEGER NOT NULL
)
PARTITION BY HASH ("USERID", "PHONENUMBER")
(
    PARTITION P1 TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF,
    PARTITION P2 TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF,
    PARTITION P3 TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF,
    PARTITION P4 TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF,
    PARTITION P5 TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF,
    PARTITION P6 TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;

DROP TABLE IF EXISTS "MS_BIGTABLE_LOG" CASCADE CONSTRAINTS;
CREATE TABLE "MS_BIGTABLE_LOG"
(
  "RECORD_DATE" DATE,
  "COL_1" VARCHAR(200 BYTE)
)
PARTITION BY RANGE ("RECORD_DATE")
INTERVAL(numtodsinterval(1,'day'))
STORE IN("TABLESPACE USERS")
(
    PARTITION P1 VALUES LESS THAN (TO_DATE('2014-1-1', 'YYYY-MM-DD')) TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "MS_BIGTABLE_LOG" ("RECORD_DATE","COL_1") values
  ('2018-01-01 00:00:00','1');
COMMIT;

DROP TABLE IF EXISTS "MS_BIGTABLE_LOG2" CASCADE CONSTRAINTS;
CREATE TABLE "MS_BIGTABLE_LOG2"
(
  "RECORD_DATE" DATE,
  "COL_1" VARCHAR(200 BYTE)
)
PARTITION BY RANGE ("RECORD_DATE")
INTERVAL(numtodsinterval(1,'day'))
(
    PARTITION P1 VALUES LESS THAN (TO_DATE('2014-1-1', 'YYYY-MM-DD')) TABLESPACE "USERS" INITRANS 2 PCTFREE 8 FORMAT ASF
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "MS_BIGTABLE_LOG2" ("RECORD_DATE","COL_1") values
  ('2018-01-01 00:00:00','1');
COMMIT;
CREATE INDEX "MS_BIGTABLE_LOG2_IDX" ON "MS_BIGTABLE_LOG2"("RECORD_DATE")
LOCAL
TABLESPACE "USERS"
INITRANS 2
PCTFREE 8;




Logical export succeeded.

SQL> 
SQL> drop table if exists TEST_T1;

Succeed.

SQL> CREATE GLOBAL TEMPORARY TABLE TEST_T1
  2 (
  3   ID INTEGER,num INTEGER
  4 )ON COMMIT PRESERVE ROWS; 

Succeed.

SQL> insert into TEST_T1(id,num) values(1,111);

1 rows affected.

SQL> insert into TEST_T1(id,num) values(2,222);

1 rows affected.

SQL> insert into TEST_T1(id,num) values(3,333);

1 rows affected.

SQL> insert into TEST_T1(id,num) values(4,444);

1 rows affected.

SQL> insert into TEST_T1(id,num) values(5,555); 

1 rows affected.

SQL> select * from TEST_T1;

ID           NUM         
------------ ------------
1            111         
2            222         
3            333         
4            444         
5            555         

5 rows fetched.

SQL> exp tables=TEST_T1 file = "stdout" filetype = txt;
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TEST_T1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER1.TEST_T1 ...
  exporting DDL of EXP_USER1.TEST_T1 ...
  skipping to export the data of temporary table
  exporting indexes on EXP_USER1.TEST_T1 ...
  exporting constraints on EXP_USER1.TEST_T1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TEST_T1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "TEST_T1" CASCADE CONSTRAINTS;
CREATE GLOBAL TEMPORARY TABLE "TEST_T1"
(
  "ID" BINARY_INTEGER,
  "NUM" BINARY_INTEGER
)ON COMMIT PRESERVE ROWS;


Logical export succeeded.

SQL> exp tables=TEST_T1 file = "stdout" filetype = bin;
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TEST_T1
-- FILE TYPE = BIN
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER1.TEST_T1 ...
  exporting DDL of EXP_USER1.TEST_T1 ...
  skipping to export the data of temporary table
  exporting indexes on EXP_USER1.TEST_T1 ...
  exporting constraints on EXP_USER1.TEST_T1 ...

Logical export succeeded.

SQL> 
SQL> conn exp_user1/exp_user123@127.0.0.1:1611

connected.

SQL> drop table if exists test_part_t1;

Succeed.

SQL> create table test_part_t1(f1 int, f2 real, f3 number, f4 char(30), f5 varchar(30), f6 date, f7 timestamp)
  2 PARTITION BY RANGE(f1)
  3 (
  4  PARTITION p1 values less than(10),
  5  PARTITION p2 values less than(20),
  6  PARTITION p3 values less than(30),
  7  PARTITION p4 values less than(MAXVALUE)
  8 ) crmode row;

Succeed.

SQL> create index idx_t1_1 on test_part_t1(f2,f3);

Succeed.

SQL> create index idx_t1_2 on test_part_t1(f4,f5) local;

Succeed.

SQL> insert into test_part_t1 values(5, 15, 28, 'abcd', 'abcd', to_date('2018/01/24', 'YYYY/MM/DD'), to_timestamp('2018-01-24 16:00:00.00', 'YYYY-MM-DD HH24:MI:SS.FF3'));

1 rows affected.

SQL> insert into test_part_t1 values(6, 16, 29, '16', '29', to_date('2018/01/24', 'YYYY/MM/DD'), to_timestamp('2018-01-24 16:00:00.00', 'YYYY-MM-DD HH24:MI:SS.FF3'));

1 rows affected.

SQL> insert into test_part_t1 values(16, 26, 39, '26', '39', to_date('2018/01/24', 'YYYY/MM/DD'), to_timestamp('2018-01-24 16:00:00.00', 'YYYY-MM-DD HH24:MI:SS.FF3'));

1 rows affected.

SQL> insert into test_part_t1 values(26, 36, 49, '36', '49', to_date('2018/01/24', 'YYYY/MM/DD'), to_timestamp('2018-01-24 16:00:00.00', 'YYYY-MM-DD HH24:MI:SS.FF3'));

1 rows affected.

SQL> insert into test_part_t1 values(36, 46, 59, '46', '59', to_date('2018/01/24', 'YYYY/MM/DD'), to_timestamp('2018-01-24 16:00:00.00', 'YYYY-MM-DD HH24:MI:SS.FF3'));

1 rows affected.

SQL> insert into test_part_t1 values(46, 56, 69, '56', '69', to_date('2018/01/24', 'YYYY/MM/DD'), to_timestamp('2018-01-24 16:00:00.00', 'YYYY-MM-DD HH24:MI:SS.FF3'));

1 rows affected.

SQL> insert into test_part_t1 select * from test_part_t1;

6 rows affected.

SQL> insert into test_part_t1 select * from test_part_t1;

12 rows affected.

SQL> insert into test_part_t1 select * from test_part_t1;

24 rows affected.

SQL> insert into test_part_t1 select * from test_part_t1;

48 rows affected.

SQL> commit;

Succeed.

SQL> insert into test_part_t1 select * from test_part_t1;

96 rows affected.

SQL> insert into test_part_t1 select * from test_part_t1;

192 rows affected.

SQL> insert into test_part_t1 select * from test_part_t1;

384 rows affected.

SQL> insert into test_part_t1 select * from test_part_t1;

768 rows affected.

SQL> commit;

Succeed.

SQL> exp tables=test_part_t1 parallel=10 file = "test_part_t1.sql";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TEST_PART_T1
-- FILE TYPE = TXT
-- DUMP FILE = test_part_t1.sql
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 10
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER1.TEST_PART_T1 ...
  exporting DDL of EXP_USER1.TEST_PART_T1 ...
  exporting indexes on EXP_USER1.TEST_PART_T1 ...
  exporting constraints on EXP_USER1.TEST_PART_T1 ...

Logical export succeeded.

SQL> 
SQL> exp tables=test_part_t1 filetype =bin parallel=10 file = "test_part_t1_bin.sql";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TEST_PART_T1
-- FILE TYPE = BIN
-- DUMP FILE = test_part_t1_bin.sql
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 10
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER1.TEST_PART_T1 ...
  exporting DDL of EXP_USER1.TEST_PART_T1 ...
    data exporting success! 1536 rows are dumped.
  exporting indexes on EXP_USER1.TEST_PART_T1 ...
  exporting constraints on EXP_USER1.TEST_PART_T1 ...

Logical export succeeded.

SQL> drop table if exists test_part_t1;

Succeed.

SQL> 
SQL> drop table if exists test_t2;

Succeed.

SQL> create table test_t2(f1 int, f2 real, f3 number, f4 char(30), f5 varchar(30), f6 date, f7 timestamp);

Succeed.

SQL> drop table if exists test_t3;

Succeed.

SQL> create table test_t3(f1 int, f2 real, f3 number, f4 char(30), f5 varchar(30), f6 date, f7 timestamp);

Succeed.

SQL> 
SQL> insert into test_t2 values(1, 15, 28, 'abcd', 'abcd', to_date('2018/01/24', 'YYYY/MM/DD'), to_timestamp('2018-01-24 16:00:00.00', 'YYYY-MM-DD HH24:MI:SS.FF3'));

1 rows affected.

SQL> insert into test_t2 values(2, 16, 29, '16', '29', to_date('2018/01/24', 'YYYY/MM/DD'), to_timestamp('2018-01-24 16:00:00.00', 'YYYY-MM-DD HH24:MI:SS.FF3'));

1 rows affected.

SQL> insert into test_t2 values(7, 26, 39, '26', '39', to_date('2018/01/24', 'YYYY/MM/DD'), to_timestamp('2018-01-24 16:00:00.00', 'YYYY-MM-DD HH24:MI:SS.FF3'));

1 rows affected.

SQL> insert into test_t2 values(8, 36, 49, '36', '49', to_date('2018/01/24', 'YYYY/MM/DD'), to_timestamp('2018-01-24 16:00:00.00', 'YYYY-MM-DD HH24:MI:SS.FF3'));

1 rows affected.

SQL> insert into test_t2 values(12, 46, 59, '46', '59', to_date('2018/01/24', 'YYYY/MM/DD'), to_timestamp('2018-01-24 16:00:00.00', 'YYYY-MM-DD HH24:MI:SS.FF3'));

1 rows affected.

SQL> insert into test_t2 values(13, 56, 69, '56', '69', to_date('2018/01/24', 'YYYY/MM/DD'), to_timestamp('2018-01-24 16:00:00.00', 'YYYY-MM-DD HH24:MI:SS.FF3'));

1 rows affected.

SQL> insert into test_t2 select * from test_t2;

6 rows affected.

SQL> insert into test_t2 select * from test_t2;

12 rows affected.

SQL> insert into test_t2 select * from test_t2;

24 rows affected.

SQL> insert into test_t2 select * from test_t2;

48 rows affected.

SQL> insert into test_t2 select * from test_t2;

96 rows affected.

SQL> insert into test_t2 select * from test_t2;

192 rows affected.

SQL> insert into test_t2 select * from test_t2;

384 rows affected.

SQL> commit;

Succeed.

SQL> insert into test_t3 select * from test_t2;

768 rows affected.

SQL> commit;

Succeed.

SQL> 
SQL> exp tables=test_t2,test_t3 parallel=10 file = "test_t2.sql";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TEST_T2, TEST_T3
-- FILE TYPE = TXT
-- DUMP FILE = test_t2.sql
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 10
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER1.TEST_T2 ...
  exporting DDL of EXP_USER1.TEST_T2 ...
  exporting indexes on EXP_USER1.TEST_T2 ...
  exporting constraints on EXP_USER1.TEST_T2 ...

exporting table EXP_USER1.TEST_T3 ...
  exporting DDL of EXP_USER1.TEST_T3 ...
  exporting indexes on EXP_USER1.TEST_T3 ...
  exporting constraints on EXP_USER1.TEST_T3 ...

Logical export succeeded.

SQL> 
SQL> \! rm -rf exp_dir/


SQL> \! mkdir exp_dir


SQL> 
SQL> exp tables=test_t2,test_t3 parallel=10 insert_batch=10 consistent=y file="./exp_dir/exp_user_par1.sql";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TEST_T2, TEST_T3
-- FILE TYPE = TXT
-- DUMP FILE = ./exp_dir/exp_user_par1.sql
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = Y
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 10
-- FEEDBACK = 10000
-- PARALLEL = 10
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER1.TEST_T2 ...
  exporting DDL of EXP_USER1.TEST_T2 ...
  exporting indexes on EXP_USER1.TEST_T2 ...
  exporting constraints on EXP_USER1.TEST_T2 ...

exporting table EXP_USER1.TEST_T3 ...
  exporting DDL of EXP_USER1.TEST_T3 ...
  exporting indexes on EXP_USER1.TEST_T3 ...
  exporting constraints on EXP_USER1.TEST_T3 ...

Logical export succeeded.

SQL> exp tables=test_t2,test_t3 parallel=10 filetype = bin file="./exp_dir/exp_user_par2.sql";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TEST_T2, TEST_T3
-- FILE TYPE = BIN
-- DUMP FILE = ./exp_dir/exp_user_par2.sql
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 10
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER1.TEST_T2 ...
  exporting DDL of EXP_USER1.TEST_T2 ...
    data exporting success! 768 rows are dumped.
  exporting indexes on EXP_USER1.TEST_T2 ...
  exporting constraints on EXP_USER1.TEST_T2 ...

exporting table EXP_USER1.TEST_T3 ...
  exporting DDL of EXP_USER1.TEST_T3 ...
    data exporting success! 768 rows are dumped.
  exporting indexes on EXP_USER1.TEST_T3 ...
  exporting constraints on EXP_USER1.TEST_T3 ...

Logical export succeeded.

SQL> exp tables=test_t2,test_t3 parallel=30 filetype = bin file="./exp_dir/exp_user_par30.sql";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TEST_T2, TEST_T3
-- FILE TYPE = BIN
-- DUMP FILE = ./exp_dir/exp_user_par30.sql
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 16
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER1.TEST_T2 ...
  exporting DDL of EXP_USER1.TEST_T2 ...
    data exporting success! 768 rows are dumped.
  exporting indexes on EXP_USER1.TEST_T2 ...
  exporting constraints on EXP_USER1.TEST_T2 ...

exporting table EXP_USER1.TEST_T3 ...
  exporting DDL of EXP_USER1.TEST_T3 ...
    data exporting success! 768 rows are dumped.
  exporting indexes on EXP_USER1.TEST_T3 ...
  exporting constraints on EXP_USER1.TEST_T3 ...

Logical export succeeded.

SQL> 
SQL> create user exp_user_par IDENTIFIED by 'exp_user123';

Succeed.

SQL> grant dba to exp_user_par;

Succeed.

SQL> grant create session to exp_user_par;

Succeed.

SQL> grant create table to exp_user_par;

Succeed.

SQL> 
SQL> conn exp_user_par/exp_user123@127.0.0.1:1611

connected.

SQL> \! ctsql exp_user_par/exp_user123@127.0.0.1:1611 -c "@./exp_dir/exp_user_par1.sql";


SQL> 
SQL> select count(*), avg(f3) from exp_user_par.test_t2;

COUNT(*)             AVG(F3)                                 
-------------------- ----------------------------------------
768                  45.5                                    

1 rows fetched.

SQL> select count(*), avg(f3) from exp_user_par.test_t3;

COUNT(*)             AVG(F3)                                 
-------------------- ----------------------------------------
768                  45.5                                    

1 rows fetched.

SQL> 
SQL> truncate table exp_user_par.test_t2;

Succeed.

SQL> truncate table exp_user_par.test_t3;

Succeed.

SQL> 
SQL> \! ctsql exp_user_par/exp_user123@127.0.0.1:1611 -c "imp tables=test_t2,test_t3 parallel=3 filetype = bin file='./exp_dir/exp_user_par2.sql'";


SQL> 
SQL> select count(*), avg(f3) from exp_user_par.test_t2;

COUNT(*)             AVG(F3)                                 
-------------------- ----------------------------------------
768                  45.5                                    

1 rows fetched.

SQL> select count(*), avg(f3) from exp_user_par.test_t3;

COUNT(*)             AVG(F3)                                 
-------------------- ----------------------------------------
768                  45.5                                    

1 rows fetched.

SQL> 
SQL> --new requirement 
SQL> conn sys/Huawei@123@127.0.0.1:1611

connected.

SQL> 
SQL> drop USER if exists lin_u1;

Succeed.

SQL> drop USER if exists lin_u2;

Succeed.

SQL> drop USER if exists lin_u3;

Succeed.

SQL> drop USER if exists lin_u4;

Succeed.

SQL> drop USER if exists lin_u5;

Succeed.

SQL> 
SQL> create user lin_u1 IDENTIFIED by 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED;

Succeed.

SQL> create role lin_r1 identified by 'cNKM9gYA0Aedv9Lnu6tX8wiiGm8Cm/hZEzVQfpElEeyn3GCWukXeze+RKSCQeBPAADtxGAUJFxoyAP54IBTKbTwt0atPGxBE8/NHaNDaDCzhd15sja05Hw==' ENCRYPTED;

Succeed.

SQL> create role lin_r2;

Succeed.

SQL> grant dba to lin_u1;

Succeed.

SQL> grant create session to lin_r1;

Succeed.

SQL> grant create table to lin_r2;

Succeed.

SQL> grant lin_r1 to lin_u1;

Succeed.

SQL> grant lin_r2 to lin_u1;

Succeed.

SQL> grant CREATE DATABASE to lin_u1;

Succeed.

SQL> grant CREATE ANY VIEW to lin_u1;

Succeed.

SQL> 
SQL> export users = lin_u1 tablespace = y file = "lin_u1.sql" log="tmp_file.log" content = metadata_only compress = 1;
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = lin_u1.sql
-- LOG FILE = tmp_file.log
-- QUERY = ""
-- COMPRESS = Y
-- CONSISTENT = N
-- CONTENT_MODE = METADATA_ONLY
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = Y
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of LIN_U1
Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

Logical export succeeded.

SQL> drop tablespace TBS_TEST including contents and datafiles;

CT-00780, The tablespace TBS_TEST does not exist.
SQL> create tablespace tbs_test datafile 'tbs_test' size 134217728 autoextend on next  16777216 maxsize  34359730176;

Succeed.

SQL> drop tablespace tbs_atest including contents and datafiles;

CT-00780, The tablespace TBS_ATEST does not exist.
SQL> create tablespace tbs_atest datafile 'tbs_atest' size 134217728 autoextend off;

Succeed.

SQL> 
SQL> conn lin_u1/huawei_123@127.0.0.1:1611

connected.

SQL> drop table if exists EXP_LIN_TEST1;

Succeed.

SQL> create table EXP_LIN_TEST1(id int,name varchar2(10));

Succeed.

SQL> insert into EXP_LIN_TEST1 values(1,'m'),(2,'m_s'),(3,'dd');

3 rows affected.

SQL> 
SQL> exp users = lin_u1 create_user = Y role = Y grant = Y file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema LIN_U1 ...
Exporting user definition of schema LIN_U1 ...
Exporting grant role and privilege of schema LIN_U1 ...
Grant privilege to schema LIN_U1 ...
Grant role to schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

CREATE ROLE "LIN_R1" IDENTIFIED BY 'cNKM9gYA0Aedv9Lnu6tX8wiiGm8Cm/hZEzVQfpElEeyn3GCWukXeze+RKSCQeBPAADtxGAUJFxoyAP54IBTKbTwt0atPGxBE8/NHaNDaDCzhd15sja05Hw==' ENCRYPTED;
CREATE ROLE "LIN_R2";
GRANT CREATE SESSION TO "LIN_R1";
GRANT CREATE TABLE TO "LIN_R2";

CREATE USER "LIN_U1" IDENTIFIED BY 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED DEFAULT TABLESPACE "USERS";

GRANT CREATE ANY VIEW TO "LIN_U1";
GRANT CREATE DATABASE TO "LIN_U1";
GRANT DBA TO "LIN_U1";
GRANT LIN_R1 TO "LIN_U1";
GRANT LIN_R2 TO "LIN_U1";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 create_user = N role = N grant = N tablespace = N file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 role = Y grant = Y create_user = Y tablespace = N file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema LIN_U1 ...
Exporting user definition of schema LIN_U1 ...
Exporting grant role and privilege of schema LIN_U1 ...
Grant privilege to schema LIN_U1 ...
Grant role to schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

CREATE ROLE "LIN_R1" IDENTIFIED BY 'cNKM9gYA0Aedv9Lnu6tX8wiiGm8Cm/hZEzVQfpElEeyn3GCWukXeze+RKSCQeBPAADtxGAUJFxoyAP54IBTKbTwt0atPGxBE8/NHaNDaDCzhd15sja05Hw==' ENCRYPTED;
CREATE ROLE "LIN_R2";
GRANT CREATE SESSION TO "LIN_R1";
GRANT CREATE TABLE TO "LIN_R2";

CREATE USER "LIN_U1" IDENTIFIED BY 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED DEFAULT TABLESPACE "USERS";

GRANT CREATE ANY VIEW TO "LIN_U1";
GRANT CREATE DATABASE TO "LIN_U1";
GRANT DBA TO "LIN_U1";
GRANT LIN_R1 TO "LIN_U1";
GRANT LIN_R2 TO "LIN_U1";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 create_user = Y role = Y grant = Y tablespace = Y file = "temp_file_create.sql" log="tmp_file.log";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = temp_file_create.sql
-- LOG FILE = tmp_file.log
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = Y
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema LIN_U1 ...
Exporting user definition of schema LIN_U1 ...
Exporting grant role and privilege of schema LIN_U1 ...
Grant privilege to schema LIN_U1 ...
Grant role to schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

Logical export succeeded.

SQL> exp users = lin_u1 grant = Y create_user = Y role = Y file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema LIN_U1 ...
Exporting user definition of schema LIN_U1 ...
Exporting grant role and privilege of schema LIN_U1 ...
Grant privilege to schema LIN_U1 ...
Grant role to schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

CREATE ROLE "LIN_R1" IDENTIFIED BY 'cNKM9gYA0Aedv9Lnu6tX8wiiGm8Cm/hZEzVQfpElEeyn3GCWukXeze+RKSCQeBPAADtxGAUJFxoyAP54IBTKbTwt0atPGxBE8/NHaNDaDCzhd15sja05Hw==' ENCRYPTED;
CREATE ROLE "LIN_R2";
GRANT CREATE SESSION TO "LIN_R1";
GRANT CREATE TABLE TO "LIN_R2";

CREATE USER "LIN_U1" IDENTIFIED BY 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED DEFAULT TABLESPACE "USERS";

GRANT CREATE ANY VIEW TO "LIN_U1";
GRANT CREATE DATABASE TO "LIN_U1";
GRANT DBA TO "LIN_U1";
GRANT LIN_R1 TO "LIN_U1";
GRANT LIN_R2 TO "LIN_U1";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 create_user = Y file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting user definition of schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


CREATE USER "LIN_U1" IDENTIFIED BY 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED DEFAULT TABLESPACE "USERS";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 create_user = N file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 role = Y file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = Y
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = Y
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

CREATE ROLE "LIN_R1" IDENTIFIED BY 'cNKM9gYA0Aedv9Lnu6tX8wiiGm8Cm/hZEzVQfpElEeyn3GCWukXeze+RKSCQeBPAADtxGAUJFxoyAP54IBTKbTwt0atPGxBE8/NHaNDaDCzhd15sja05Hw==' ENCRYPTED;
CREATE ROLE "LIN_R2";
GRANT CREATE SESSION TO "LIN_R1";
GRANT CREATE TABLE TO "LIN_R2";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 role = N file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 grant = Y file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting grant role and privilege of schema LIN_U1 ...
Grant privilege to schema LIN_U1 ...
Grant role to schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


GRANT CREATE ANY VIEW TO "LIN_U1";
GRANT CREATE DATABASE TO "LIN_U1";
GRANT DBA TO "LIN_U1";
GRANT LIN_R1 TO "LIN_U1";
GRANT LIN_R2 TO "LIN_U1";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 grant = N file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 tablespace = Y file = "temp_file_lin.sql" log="tmp_file.log";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = temp_file_lin.sql
-- LOG FILE = tmp_file.log
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = Y
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

Logical export succeeded.

SQL> exp users = lin_u1 tablespace = N file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 role = Y grant = Y file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema LIN_U1 ...
Exporting grant role and privilege of schema LIN_U1 ...
Grant privilege to schema LIN_U1 ...
Grant role to schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

CREATE ROLE "LIN_R1" IDENTIFIED BY 'cNKM9gYA0Aedv9Lnu6tX8wiiGm8Cm/hZEzVQfpElEeyn3GCWukXeze+RKSCQeBPAADtxGAUJFxoyAP54IBTKbTwt0atPGxBE8/NHaNDaDCzhd15sja05Hw==' ENCRYPTED;
CREATE ROLE "LIN_R2";
GRANT CREATE SESSION TO "LIN_R1";
GRANT CREATE TABLE TO "LIN_R2";

GRANT CREATE ANY VIEW TO "LIN_U1";
GRANT CREATE DATABASE TO "LIN_U1";
GRANT DBA TO "LIN_U1";
GRANT LIN_R1 TO "LIN_U1";
GRANT LIN_R2 TO "LIN_U1";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 role = N grant = Y file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting grant role and privilege of schema LIN_U1 ...
Grant privilege to schema LIN_U1 ...
Grant role to schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


GRANT CREATE ANY VIEW TO "LIN_U1";
GRANT CREATE DATABASE TO "LIN_U1";
GRANT DBA TO "LIN_U1";
GRANT LIN_R1 TO "LIN_U1";
GRANT LIN_R2 TO "LIN_U1";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 role = Y grant = N file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = Y
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = Y
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

CREATE ROLE "LIN_R1" IDENTIFIED BY 'cNKM9gYA0Aedv9Lnu6tX8wiiGm8Cm/hZEzVQfpElEeyn3GCWukXeze+RKSCQeBPAADtxGAUJFxoyAP54IBTKbTwt0atPGxBE8/NHaNDaDCzhd15sja05Hw==' ENCRYPTED;
CREATE ROLE "LIN_R2";
GRANT CREATE SESSION TO "LIN_R1";
GRANT CREATE TABLE TO "LIN_R2";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 role = N grant = N file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 create_user = Y grant = Y file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = N
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting user definition of schema LIN_U1 ...
Exporting grant role and privilege of schema LIN_U1 ...
Grant privilege to schema LIN_U1 ...
Grant role to schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = N
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


CREATE USER "LIN_U1" IDENTIFIED BY 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED DEFAULT TABLESPACE "USERS";

GRANT CREATE ANY VIEW TO "LIN_U1";
GRANT CREATE DATABASE TO "LIN_U1";
GRANT DBA TO "LIN_U1";
GRANT LIN_R1 TO "LIN_U1";
GRANT LIN_R2 TO "LIN_U1";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 create_user = N grant = Y file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting grant role and privilege of schema LIN_U1 ...
Grant privilege to schema LIN_U1 ...
Grant role to schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


GRANT CREATE ANY VIEW TO "LIN_U1";
GRANT CREATE DATABASE TO "LIN_U1";
GRANT DBA TO "LIN_U1";
GRANT LIN_R1 TO "LIN_U1";
GRANT LIN_R2 TO "LIN_U1";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 create_user = Y grant = N file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting user definition of schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


CREATE USER "LIN_U1" IDENTIFIED BY 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED DEFAULT TABLESPACE "USERS";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 create_user = N grant = N file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 create_user = Y role = Y file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema LIN_U1 ...
Exporting user definition of schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

CREATE ROLE "LIN_R1" IDENTIFIED BY 'cNKM9gYA0Aedv9Lnu6tX8wiiGm8Cm/hZEzVQfpElEeyn3GCWukXeze+RKSCQeBPAADtxGAUJFxoyAP54IBTKbTwt0atPGxBE8/NHaNDaDCzhd15sja05Hw==' ENCRYPTED;
CREATE ROLE "LIN_R2";
GRANT CREATE SESSION TO "LIN_R1";
GRANT CREATE TABLE TO "LIN_R2";

CREATE USER "LIN_U1" IDENTIFIED BY 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED DEFAULT TABLESPACE "USERS";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 create_user = N role = Y file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = Y
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = Y
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

CREATE ROLE "LIN_R1" IDENTIFIED BY 'cNKM9gYA0Aedv9Lnu6tX8wiiGm8Cm/hZEzVQfpElEeyn3GCWukXeze+RKSCQeBPAADtxGAUJFxoyAP54IBTKbTwt0atPGxBE8/NHaNDaDCzhd15sja05Hw==' ENCRYPTED;
CREATE ROLE "LIN_R2";
GRANT CREATE SESSION TO "LIN_R1";
GRANT CREATE TABLE TO "LIN_R2";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 create_user = Y role = N file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting user definition of schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


CREATE USER "LIN_U1" IDENTIFIED BY 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED DEFAULT TABLESPACE "USERS";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u1 create_user = N role = N file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> 
SQL> CREATE USER lin_u2 IDENTIFIED BY 'QVMF1gYA0AfgXzVDksDDgqqhwTg6Gdqe68XDo1dC3Mysy+qTd1TnvdF0blgzqb5WfMAEjNtk77oADkUAxndmku+R6jt9rZhWX+p8dYI8W0apFiBc7K+/sw==' ENCRYPTED PASSWORD EXPIRE;

Succeed.

SQL> CREATE USER lin_u3 IDENTIFIED BY 'sQCqEwYA0Afz/Fi6DaGvwfSo6dfywDekFGFlE5kd3LDVfhNKSgGCjqDY70T2SskYtYmRxiUkqdYARa4VmCZdz2qWp1e90+BPRavV2jrG9hM29MtG6n5BmQ==' ENCRYPTED PASSWORD EXPIRE ACCOUNT LOCK;

Succeed.

SQL> CREATE USER lin_u4 IDENTIFIED BY 'M21e/AYA0AccXnwx++JF34lKwNPhXp6q9rD49N5vO0Os+p38nvgUB6EGN4Caa6r/5I7FraNJT0hSVX1lzAVGWjsLubxyG54UcKASjWVZQUWDKrsglHMVpA==' ENCRYPTED;

Succeed.

SQL> CREATE USER lin_u5 IDENTIFIED BY 'KCyY1gYA0AeSQ+MD7PjUUPMKpF/+INwzJ4GhHSmSG+bl7RxZJUk1gIZ5N38lI4ujoEXaLASjuXOC0wFZSQhRU/lXzNbiY75Vpv+PMmy5i0GrERq+WGnTzg==' ENCRYPTED;

Succeed.

SQL> grant dba to lin_u4;

Succeed.

SQL> grant create session to lin_u2;

Succeed.

SQL> grant create session to lin_u3;

Succeed.

SQL> grant create session to lin_u4;

Succeed.

SQL> grant dba to lin_u5;

Succeed.

SQL> grant create table to lin_u2;

Succeed.

SQL> grant create table to lin_u3;

Succeed.

SQL> 
SQL> 
SQL> conn lin_u3/huawei_123@127.0.0.1:1611

CT-00328, The account was locked

SQL> 
SQL> conn lin_u4/huawei_123@127.0.0.1:1611

connected.

SQL> drop table if exists EXP_LIN_TEST2;

Succeed.

SQL> create table EXP_LIN_TEST2(id int,name varchar2(10),description varchar2(100));

Succeed.

SQL> insert into EXP_LIN_TEST2 values(1,'m','lin'),(2,'ms','lin1'),(3,'dms','lin2'),(4,'ddss','lin3');

4 rows affected.

SQL> 
SQL> conn lin_u1/huawei_123@127.0.0.1:1611

connected.

SQL> exp users = lin_u2,lin_u3,lin_u4,lin_u5 create_user = Y role = Y grant = Y file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U2, LIN_U3, LIN_U4, LIN_U5
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema LIN_U2 ...
Exporting user definition of schema LIN_U2 ...
Exporting grant role and privilege of schema LIN_U2 ...
Grant privilege to schema LIN_U2 ...
Grant role to schema LIN_U2 ...
Exporting sequence of schema LIN_U2 ...
Exporting profile of schema LIN_U2 ...
Exporting type of schema LIN_U2 ...
Exporting tables of schema LIN_U2 ...
Reading table objects of LIN_U2

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of LIN_U2
Exporting procedures/functions/triggers of schema LIN_U2 ...
Exporting views of schema LIN_U2 ...
Exporting synonyms of schema LIN_U2 ...
Exporting package of schema LIN_U2 ...
End of export schema LIN_U2 ...

Exporting schema LIN_U3 ...
Exporting user definition of schema LIN_U3 ...
Exporting grant role and privilege of schema LIN_U3 ...
Grant privilege to schema LIN_U3 ...
Grant role to schema LIN_U3 ...
Exporting sequence of schema LIN_U3 ...
Exporting profile of schema LIN_U3 ...
Exporting type of schema LIN_U3 ...
Exporting tables of schema LIN_U3 ...
Reading table objects of LIN_U3

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of LIN_U3
Exporting procedures/functions/triggers of schema LIN_U3 ...
Exporting views of schema LIN_U3 ...
Exporting synonyms of schema LIN_U3 ...
Exporting package of schema LIN_U3 ...
End of export schema LIN_U3 ...

Exporting schema LIN_U4 ...
Exporting user definition of schema LIN_U4 ...
Exporting grant role and privilege of schema LIN_U4 ...
Grant privilege to schema LIN_U4 ...
Grant role to schema LIN_U4 ...
Exporting sequence of schema LIN_U4 ...
Exporting profile of schema LIN_U4 ...
Exporting type of schema LIN_U4 ...
Exporting tables of schema LIN_U4 ...
Reading table objects of LIN_U4

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST2                                                    1         

Exporting tables (scripts or data) of LIN_U4
exporting table LIN_U4.EXP_LIN_TEST2 ...
  exporting DDL of LIN_U4.EXP_LIN_TEST2 ...
  exporting data of LIN_U4.EXP_LIN_TEST2 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on LIN_U4.EXP_LIN_TEST2 ...
  exporting constraints on LIN_U4.EXP_LIN_TEST2 ...

Exporting procedures/functions/triggers of schema LIN_U4 ...
Exporting views of schema LIN_U4 ...
Exporting synonyms of schema LIN_U4 ...
Exporting package of schema LIN_U4 ...
End of export schema LIN_U4 ...

Exporting schema LIN_U5 ...
Exporting user definition of schema LIN_U5 ...
Exporting grant role and privilege of schema LIN_U5 ...
Grant privilege to schema LIN_U5 ...
Grant role to schema LIN_U5 ...
Exporting sequence of schema LIN_U5 ...
Exporting profile of schema LIN_U5 ...
Exporting type of schema LIN_U5 ...
Exporting tables of schema LIN_U5 ...
Reading table objects of LIN_U5

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of LIN_U5
Exporting procedures/functions/triggers of schema LIN_U5 ...
Exporting views of schema LIN_U5 ...
Exporting synonyms of schema LIN_U5 ...
Exporting package of schema LIN_U5 ...
End of export schema LIN_U5 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U2, LIN_U3, LIN_U4, LIN_U5
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


CREATE USER "LIN_U2" IDENTIFIED BY 'QVMF1gYA0AfgXzVDksDDgqqhwTg6Gdqe68XDo1dC3Mysy+qTd1TnvdF0blgzqb5WfMAEjNtk77oADkUAxndmku+R6jt9rZhWX+p8dYI8W0apFiBc7K+/sw==' ENCRYPTED PASSWORD EXPIRE DEFAULT TABLESPACE "USERS";

GRANT CREATE SESSION TO "LIN_U2";
GRANT CREATE TABLE TO "LIN_U2";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U2;

CREATE USER "LIN_U3" IDENTIFIED BY 'sQCqEwYA0Afz/Fi6DaGvwfSo6dfywDekFGFlE5kd3LDVfhNKSgGCjqDY70T2SskYtYmRxiUkqdYARa4VmCZdz2qWp1e90+BPRavV2jrG9hM29MtG6n5BmQ==' ENCRYPTED PASSWORD EXPIRE ACCOUNT LOCK DEFAULT TABLESPACE "USERS";

GRANT CREATE SESSION TO "LIN_U3";
GRANT CREATE TABLE TO "LIN_U3";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U3;

CREATE USER "LIN_U4" IDENTIFIED BY 'M21e/AYA0AccXnwx++JF34lKwNPhXp6q9rD49N5vO0Os+p38nvgUB6EGN4Caa6r/5I7FraNJT0hSVX1lzAVGWjsLubxyG54UcKASjWVZQUWDKrsglHMVpA==' ENCRYPTED DEFAULT TABLESPACE "USERS";

GRANT CREATE SESSION TO "LIN_U4";
GRANT DBA TO "LIN_U4";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U4;
DROP TABLE IF EXISTS "EXP_LIN_TEST2" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST2"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE),
  "DESCRIPTION" VARCHAR(100 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;


CREATE USER "LIN_U5" IDENTIFIED BY 'KCyY1gYA0AeSQ+MD7PjUUPMKpF/+INwzJ4GhHSmSG+bl7RxZJUk1gIZ5N38lI4ujoEXaLASjuXOC0wFZSQhRU/lXzNbiY75Vpv+PMmy5i0GrERq+WGnTzg==' ENCRYPTED DEFAULT TABLESPACE "USERS";

GRANT DBA TO "LIN_U5";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U5;
Logical export succeeded.

SQL> exp users = lin_u2,lin_u3,lin_u4,lin_u5 create_user = Y role = Y grant = Y tablespace = Y file = "temp_file_u2.sql" log="tmp_file.log";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U2, LIN_U3, LIN_U4, LIN_U5
-- FILE TYPE = TXT
-- DUMP FILE = temp_file_u2.sql
-- LOG FILE = tmp_file.log
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = Y
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema LIN_U2 ...
Exporting user definition of schema LIN_U2 ...
Exporting grant role and privilege of schema LIN_U2 ...
Grant privilege to schema LIN_U2 ...
Grant role to schema LIN_U2 ...
Exporting sequence of schema LIN_U2 ...
Exporting profile of schema LIN_U2 ...
Exporting type of schema LIN_U2 ...
Exporting tables of schema LIN_U2 ...
Reading table objects of LIN_U2

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of LIN_U2
Exporting procedures/functions/triggers of schema LIN_U2 ...
Exporting views of schema LIN_U2 ...
Exporting synonyms of schema LIN_U2 ...
Exporting package of schema LIN_U2 ...
End of export schema LIN_U2 ...

Exporting schema LIN_U3 ...
Exporting user definition of schema LIN_U3 ...
Exporting grant role and privilege of schema LIN_U3 ...
Grant privilege to schema LIN_U3 ...
Grant role to schema LIN_U3 ...
Exporting sequence of schema LIN_U3 ...
Exporting profile of schema LIN_U3 ...
Exporting type of schema LIN_U3 ...
Exporting tables of schema LIN_U3 ...
Reading table objects of LIN_U3

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of LIN_U3
Exporting procedures/functions/triggers of schema LIN_U3 ...
Exporting views of schema LIN_U3 ...
Exporting synonyms of schema LIN_U3 ...
Exporting package of schema LIN_U3 ...
End of export schema LIN_U3 ...

Exporting schema LIN_U4 ...
Exporting user definition of schema LIN_U4 ...
Exporting grant role and privilege of schema LIN_U4 ...
Grant privilege to schema LIN_U4 ...
Grant role to schema LIN_U4 ...
Exporting sequence of schema LIN_U4 ...
Exporting profile of schema LIN_U4 ...
Exporting type of schema LIN_U4 ...
Exporting tables of schema LIN_U4 ...
Reading table objects of LIN_U4

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST2                                                    1         

Exporting tables (scripts or data) of LIN_U4
exporting table LIN_U4.EXP_LIN_TEST2 ...
  exporting DDL of LIN_U4.EXP_LIN_TEST2 ...
  exporting data of LIN_U4.EXP_LIN_TEST2 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on LIN_U4.EXP_LIN_TEST2 ...
  exporting constraints on LIN_U4.EXP_LIN_TEST2 ...

Exporting procedures/functions/triggers of schema LIN_U4 ...
Exporting views of schema LIN_U4 ...
Exporting synonyms of schema LIN_U4 ...
Exporting package of schema LIN_U4 ...
End of export schema LIN_U4 ...

Exporting schema LIN_U5 ...
Exporting user definition of schema LIN_U5 ...
Exporting grant role and privilege of schema LIN_U5 ...
Grant privilege to schema LIN_U5 ...
Grant role to schema LIN_U5 ...
Exporting sequence of schema LIN_U5 ...
Exporting profile of schema LIN_U5 ...
Exporting type of schema LIN_U5 ...
Exporting tables of schema LIN_U5 ...
Reading table objects of LIN_U5

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of LIN_U5
Exporting procedures/functions/triggers of schema LIN_U5 ...
Exporting views of schema LIN_U5 ...
Exporting synonyms of schema LIN_U5 ...
Exporting package of schema LIN_U5 ...
End of export schema LIN_U5 ...

Logical export succeeded.

SQL> exp users = lin_u1 create_user = Y role = Y grant = Y file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema LIN_U1 ...
Exporting user definition of schema LIN_U1 ...
Exporting grant role and privilege of schema LIN_U1 ...
Grant privilege to schema LIN_U1 ...
Grant role to schema LIN_U1 ...
Exporting sequence of schema LIN_U1 ...
Exporting profile of schema LIN_U1 ...
Exporting type of schema LIN_U1 ...
Exporting tables of schema LIN_U1 ...
Reading table objects of LIN_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
EXP_LIN_TEST1                                                    1         

Exporting tables (scripts or data) of LIN_U1
exporting table LIN_U1.EXP_LIN_TEST1 ...
  exporting DDL of LIN_U1.EXP_LIN_TEST1 ...
  exporting data of LIN_U1.EXP_LIN_TEST1 ...
    data exporting success, 3 rows are dumped.

  exporting indexes on LIN_U1.EXP_LIN_TEST1 ...
  exporting constraints on LIN_U1.EXP_LIN_TEST1 ...

Exporting procedures/functions/triggers of schema LIN_U1 ...
Exporting views of schema LIN_U1 ...
Exporting synonyms of schema LIN_U1 ...
Exporting package of schema LIN_U1 ...
End of export schema LIN_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

CREATE ROLE "LIN_R1" IDENTIFIED BY 'cNKM9gYA0Aedv9Lnu6tX8wiiGm8Cm/hZEzVQfpElEeyn3GCWukXeze+RKSCQeBPAADtxGAUJFxoyAP54IBTKbTwt0atPGxBE8/NHaNDaDCzhd15sja05Hw==' ENCRYPTED;
CREATE ROLE "LIN_R2";
GRANT CREATE SESSION TO "LIN_R1";
GRANT CREATE TABLE TO "LIN_R2";

CREATE USER "LIN_U1" IDENTIFIED BY 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED DEFAULT TABLESPACE "USERS";

GRANT CREATE ANY VIEW TO "LIN_U1";
GRANT CREATE DATABASE TO "LIN_U1";
GRANT DBA TO "LIN_U1";
GRANT LIN_R1 TO "LIN_U1";
GRANT LIN_R2 TO "LIN_U1";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U1;
DROP TABLE IF EXISTS "EXP_LIN_TEST1" CASCADE CONSTRAINTS;
CREATE TABLE "EXP_LIN_TEST1"
(
  "ID" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (1,'m');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (2,'m_s');
INSERT INTO "EXP_LIN_TEST1" ("ID","NAME") values
  (3,'dd');
COMMIT;

Logical export succeeded.

SQL> exp users = lin_u2 create_user = Y role = Y grant = Y file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U2
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema LIN_U2 ...
Exporting user definition of schema LIN_U2 ...
Exporting grant role and privilege of schema LIN_U2 ...
Grant privilege to schema LIN_U2 ...
Grant role to schema LIN_U2 ...
Exporting sequence of schema LIN_U2 ...
Exporting profile of schema LIN_U2 ...
Exporting type of schema LIN_U2 ...
Exporting tables of schema LIN_U2 ...
Reading table objects of LIN_U2

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of LIN_U2
Exporting procedures/functions/triggers of schema LIN_U2 ...
Exporting views of schema LIN_U2 ...
Exporting synonyms of schema LIN_U2 ...
Exporting package of schema LIN_U2 ...
End of export schema LIN_U2 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = LIN_U2
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


CREATE USER "LIN_U2" IDENTIFIED BY 'QVMF1gYA0AfgXzVDksDDgqqhwTg6Gdqe68XDo1dC3Mysy+qTd1TnvdF0blgzqb5WfMAEjNtk77oADkUAxndmku+R6jt9rZhWX+p8dYI8W0apFiBc7K+/sw==' ENCRYPTED PASSWORD EXPIRE DEFAULT TABLESPACE "USERS";

GRANT CREATE SESSION TO "LIN_U2";
GRANT CREATE TABLE TO "LIN_U2";

ALTER SESSION SET CURRENT_SCHEMA = LIN_U2;
Logical export succeeded.

SQL> 
SQL> conn sys/Huawei@123@127.0.0.1:1611

connected.

SQL> revoke dba from lin_u5;

Succeed.

SQL> grant create session to lin_u5;

Succeed.

SQL> conn lin_u5/huawei_123@127.0.0.1:1611

connected.

SQL> exp users = lin_u2,lin_u3,lin_u4,lin_u5 create_user = Y role = Y grant = Y tablespace = N file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...

CT-01001, Permissions were insufficient
Logical export failed.

SQL> 
SQL> conn sys/Huawei@123@127.0.0.1:1611

connected.

SQL> drop tablespace TBS_TEST including contents and datafiles;

Succeed.

SQL> drop tablespace tbs_atest including contents and datafiles;

Succeed.

SQL> drop USER if exists lin_u1;

CT-00815, user objects is being used, can not drop
SQL> drop USER if exists lin_u2;

Succeed.

SQL> drop USER if exists lin_u3;

Succeed.

SQL> drop USER if exists lin_u4;

CT-00815, user objects is being used, can not drop
SQL> drop USER if exists lin_u5;

Succeed.

SQL> drop role lin_r1;

Succeed.

SQL> drop role lin_r2;

Succeed.

SQL> --tablespace filter
SQL> conn sys/Huawei@123@127.0.0.1:1611

connected.

SQL> 
SQL> drop USER if exists feng_u1;

Succeed.

SQL> create user feng_u1 IDENTIFIED by 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED;

Succeed.

SQL> grant dba to feng_u1;

Succeed.

SQL> drop USER if exists feng_u2;

Succeed.

SQL> create user feng_u2 IDENTIFIED by 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED;

Succeed.

SQL> grant dba to feng_u2;

Succeed.

SQL> 
SQL> drop tablespace TBS_FENG1 including contents and datafiles;

CT-00780, The tablespace TBS_FENG1 does not exist.
SQL> create tablespace TBS_FENG1 datafile 'tbs_feng1' size 134217728 autoextend on next  16777216 maxsize  34359730176;

Succeed.

SQL> drop tablespace "tbs_feng1" including contents and datafiles;

CT-00780, The tablespace tbs_feng1 does not exist.
SQL> create tablespace "tbs_feng1" datafile 'tbs_feng2' size 134217728 autoextend off;

Succeed.

SQL> 
SQL> conn feng_u1/huawei_123@127.0.0.1:1611

connected.

SQL> 
SQL> drop table if exists tab_faaa;

Succeed.

SQL> drop table if exists tab_fbbb;

Succeed.

SQL> create table tab_faaa(id int, num int, name varchar2(10)) tablespace TBS_FENG1;

Succeed.

SQL> insert into tab_faaa values(1,100,'m'),(2,200,'m_s'),(3,300,'dd');

3 rows affected.

SQL> create index index_aaa_id on tab_faaa(id) tablespace "tbs_feng1";

Succeed.

SQL> create index index_aaa_num on tab_faaa(num);

Succeed.

SQL> 
SQL> create table tab_fbbb(id int, num int, name varchar2(10));

Succeed.

SQL> insert into tab_fbbb values(1,100,'m'),(2,200,'m_s'),(3,300,'dd');

3 rows affected.

SQL> create index index_bbb_num on tab_fbbb(num);

Succeed.

SQL> 
SQL> exp tables=tab_faaa tablespace_filter = TBS_FENG3 file = "stdout";
Parsing export options ... 
Verify options ...

CT-00601, Sql syntax error: The tablespace name TBS_FENG3 does not exist.
Logical export failed.

SQL> exp tables=tab_faaa,tab_fbbb tablespace_filter = TBS_FENG1 file = "stdout";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TAB_FAAA, TAB_FBBB
-- TABLESPACE FILTER = TBS_FENG1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table FENG_U1.TAB_FAAA ...
  exporting DDL of FENG_U1.TAB_FAAA ...
  exporting data of FENG_U1.TAB_FAAA ...
    data exporting success, 3 rows are dumped.

  exporting indexes on FENG_U1.TAB_FAAA ...
  exporting constraints on FENG_U1.TAB_FAAA ...

Warning: the user FENG_U1 and table TAB_FBBB are not in specified tablespace filters.
Warning: the user FENG_U1 and table TAB_FBBB are not in specified tablespace filters.
--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TAB_FAAA, TAB_FBBB
-- TABLESPACE FILTER = TBS_FENG1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "TAB_FAAA" CASCADE CONSTRAINTS;
CREATE TABLE "TAB_FAAA"
(
  "ID" BINARY_INTEGER,
  "NUM" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "TBS_FENG1"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (1,100,'m');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (2,200,'m_s');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (3,300,'dd');
COMMIT;
CREATE INDEX "INDEX_AAA_NUM" ON "TAB_FAAA"("NUM")
TABLESPACE "TBS_FENG1"
INITRANS 2
PCTFREE 8;


Logical export succeeded.

SQL> exp tables=tab_faaa,tab_fbbb tablespace_filter = TBS_FENG1,USERS file = "stdout";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TAB_FAAA, TAB_FBBB
-- TABLESPACE FILTER = TBS_FENG1, USERS
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table FENG_U1.TAB_FAAA ...
  exporting DDL of FENG_U1.TAB_FAAA ...
  exporting data of FENG_U1.TAB_FAAA ...
    data exporting success, 3 rows are dumped.

  exporting indexes on FENG_U1.TAB_FAAA ...
  exporting constraints on FENG_U1.TAB_FAAA ...

exporting table FENG_U1.TAB_FBBB ...
  exporting DDL of FENG_U1.TAB_FBBB ...
  exporting data of FENG_U1.TAB_FBBB ...
    data exporting success, 3 rows are dumped.

  exporting indexes on FENG_U1.TAB_FBBB ...
  exporting constraints on FENG_U1.TAB_FBBB ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TAB_FAAA, TAB_FBBB
-- TABLESPACE FILTER = TBS_FENG1, USERS
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "TAB_FAAA" CASCADE CONSTRAINTS;
CREATE TABLE "TAB_FAAA"
(
  "ID" BINARY_INTEGER,
  "NUM" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "TBS_FENG1"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (1,100,'m');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (2,200,'m_s');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (3,300,'dd');
COMMIT;
CREATE INDEX "INDEX_AAA_NUM" ON "TAB_FAAA"("NUM")
TABLESPACE "TBS_FENG1"
INITRANS 2
PCTFREE 8;

DROP TABLE IF EXISTS "TAB_FBBB" CASCADE CONSTRAINTS;
CREATE TABLE "TAB_FBBB"
(
  "ID" BINARY_INTEGER,
  "NUM" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "TAB_FBBB" ("ID","NUM","NAME") values
  (1,100,'m');
INSERT INTO "TAB_FBBB" ("ID","NUM","NAME") values
  (2,200,'m_s');
INSERT INTO "TAB_FBBB" ("ID","NUM","NAME") values
  (3,300,'dd');
COMMIT;
CREATE INDEX "INDEX_BBB_NUM" ON "TAB_FBBB"("NUM")
TABLESPACE "USERS"
INITRANS 2
PCTFREE 8;



Logical export succeeded.

SQL> exp tables=tab_faaa tablespace_filter = TBS_FENG1 file = "stdout";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TAB_FAAA
-- TABLESPACE FILTER = TBS_FENG1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table FENG_U1.TAB_FAAA ...
  exporting DDL of FENG_U1.TAB_FAAA ...
  exporting data of FENG_U1.TAB_FAAA ...
    data exporting success, 3 rows are dumped.

  exporting indexes on FENG_U1.TAB_FAAA ...
  exporting constraints on FENG_U1.TAB_FAAA ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TAB_FAAA
-- TABLESPACE FILTER = TBS_FENG1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "TAB_FAAA" CASCADE CONSTRAINTS;
CREATE TABLE "TAB_FAAA"
(
  "ID" BINARY_INTEGER,
  "NUM" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "TBS_FENG1"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (1,100,'m');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (2,200,'m_s');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (3,300,'dd');
COMMIT;
CREATE INDEX "INDEX_AAA_NUM" ON "TAB_FAAA"("NUM")
TABLESPACE "TBS_FENG1"
INITRANS 2
PCTFREE 8;


Logical export succeeded.

SQL> exp tables=tab_faaa tablespace_filter = USERS file = "stdout";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TAB_FAAA
-- TABLESPACE FILTER = USERS
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Warning: the user FENG_U1 and table TAB_FAAA are not in specified tablespace filters.
Warning: the user FENG_U1 and table TAB_FAAA are not in specified tablespace filters.
--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TAB_FAAA
-- TABLESPACE FILTER = USERS
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

Logical export succeeded.

SQL> exp tables=tab_fbbb tablespace_filter = USERS file = "stdout";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TAB_FBBB
-- TABLESPACE FILTER = USERS
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table FENG_U1.TAB_FBBB ...
  exporting DDL of FENG_U1.TAB_FBBB ...
  exporting data of FENG_U1.TAB_FBBB ...
    data exporting success, 3 rows are dumped.

  exporting indexes on FENG_U1.TAB_FBBB ...
  exporting constraints on FENG_U1.TAB_FBBB ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TAB_FBBB
-- TABLESPACE FILTER = USERS
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "TAB_FBBB" CASCADE CONSTRAINTS;
CREATE TABLE "TAB_FBBB"
(
  "ID" BINARY_INTEGER,
  "NUM" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "TAB_FBBB" ("ID","NUM","NAME") values
  (1,100,'m');
INSERT INTO "TAB_FBBB" ("ID","NUM","NAME") values
  (2,200,'m_s');
INSERT INTO "TAB_FBBB" ("ID","NUM","NAME") values
  (3,300,'dd');
COMMIT;
CREATE INDEX "INDEX_BBB_NUM" ON "TAB_FBBB"("NUM")
TABLESPACE "USERS"
INITRANS 2
PCTFREE 8;


Logical export succeeded.

SQL> exp tables=% tablespace_filter = TBS_FENG1 file = "stdout";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = ALL_TABLES
-- EXPORT OBJECTS = 
-- TABLESPACE FILTER = TBS_FENG1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Reading table objects of FENG_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
TAB_FAAA                                                         1         

Exporting tables (scripts or data) of FENG_U1
exporting table FENG_U1.TAB_FAAA ...
  exporting DDL of FENG_U1.TAB_FAAA ...
  exporting data of FENG_U1.TAB_FAAA ...
    data exporting success, 3 rows are dumped.

  exporting indexes on FENG_U1.TAB_FAAA ...
  exporting constraints on FENG_U1.TAB_FAAA ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = ALL_TABLES
-- EXPORT OBJECTS = 
-- TABLESPACE FILTER = TBS_FENG1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "TAB_FAAA" CASCADE CONSTRAINTS;
CREATE TABLE "TAB_FAAA"
(
  "ID" BINARY_INTEGER,
  "NUM" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "TBS_FENG1"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (1,100,'m');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (2,200,'m_s');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (3,300,'dd');
COMMIT;
CREATE INDEX "INDEX_AAA_NUM" ON "TAB_FAAA"("NUM")
TABLESPACE "TBS_FENG1"
INITRANS 2
PCTFREE 8;

Logical export succeeded.

SQL> exp tables=tab_faaa tablespace_filter = TBS_FENG1 file = "stdout";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TAB_FAAA
-- TABLESPACE FILTER = TBS_FENG1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table FENG_U1.TAB_FAAA ...
  exporting DDL of FENG_U1.TAB_FAAA ...
  exporting data of FENG_U1.TAB_FAAA ...
    data exporting success, 3 rows are dumped.

  exporting indexes on FENG_U1.TAB_FAAA ...
  exporting constraints on FENG_U1.TAB_FAAA ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TAB_FAAA
-- TABLESPACE FILTER = TBS_FENG1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "TAB_FAAA" CASCADE CONSTRAINTS;
CREATE TABLE "TAB_FAAA"
(
  "ID" BINARY_INTEGER,
  "NUM" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "TBS_FENG1"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (1,100,'m');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (2,200,'m_s');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (3,300,'dd');
COMMIT;
CREATE INDEX "INDEX_AAA_NUM" ON "TAB_FAAA"("NUM")
TABLESPACE "TBS_FENG1"
INITRANS 2
PCTFREE 8;


Logical export succeeded.

SQL> 
SQL> conn sys/Huawei@123@127.0.0.1:1611

connected.

SQL> exp users = feng_u1 create_user = Y role = Y grant = Y tablespace_filter = TBS_FENG3 file = "stdout";
Parsing export options ... 
Verify options ...

CT-00601, Sql syntax error: The tablespace name TBS_FENG3 does not exist.
Logical export failed.

SQL> exp users = feng_u1 create_user = Y role = Y grant = Y tablespace_filter = TBS_FENG1 file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = FENG_U1
-- TABLESPACE FILTER = TBS_FENG1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema FENG_U1 ...
Exporting user definition of schema FENG_U1 ...
Exporting grant role and privilege of schema FENG_U1 ...
Grant privilege to schema FENG_U1 ...
Grant role to schema FENG_U1 ...
Exporting sequence of schema FENG_U1 ...
Exporting profile of schema FENG_U1 ...
Exporting type of schema FENG_U1 ...
Exporting tables of schema FENG_U1 ...
Reading table objects of FENG_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
TAB_FAAA                                                         1         

Exporting tables (scripts or data) of FENG_U1
exporting table FENG_U1.TAB_FAAA ...
  exporting DDL of FENG_U1.TAB_FAAA ...
  exporting data of FENG_U1.TAB_FAAA ...
    data exporting success, 3 rows are dumped.

  exporting indexes on FENG_U1.TAB_FAAA ...
  exporting constraints on FENG_U1.TAB_FAAA ...

Exporting procedures/functions/triggers of schema FENG_U1 ...
Exporting views of schema FENG_U1 ...
Exporting synonyms of schema FENG_U1 ...
Exporting package of schema FENG_U1 ...
End of export schema FENG_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = FENG_U1
-- TABLESPACE FILTER = TBS_FENG1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


CREATE USER "FENG_U1" IDENTIFIED BY 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED DEFAULT TABLESPACE "USERS";

GRANT DBA TO "FENG_U1";

ALTER SESSION SET CURRENT_SCHEMA = FENG_U1;
DROP TABLE IF EXISTS "TAB_FAAA" CASCADE CONSTRAINTS;
CREATE TABLE "TAB_FAAA"
(
  "ID" BINARY_INTEGER,
  "NUM" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "TBS_FENG1"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (1,100,'m');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (2,200,'m_s');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (3,300,'dd');
COMMIT;
CREATE INDEX "INDEX_AAA_NUM" ON "TAB_FAAA"("NUM")
TABLESPACE "TBS_FENG1"
INITRANS 2
PCTFREE 8;

Logical export succeeded.

SQL> exp users = feng_u1 create_user = Y role = Y grant = Y tablespace_filter = TBS_FENG1,"tbs_feng1" file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = FENG_U1
-- TABLESPACE FILTER = TBS_FENG1, tbs_feng1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema FENG_U1 ...
Exporting user definition of schema FENG_U1 ...
Exporting grant role and privilege of schema FENG_U1 ...
Grant privilege to schema FENG_U1 ...
Grant role to schema FENG_U1 ...
Exporting sequence of schema FENG_U1 ...
Exporting profile of schema FENG_U1 ...
Exporting type of schema FENG_U1 ...
Exporting tables of schema FENG_U1 ...
Reading table objects of FENG_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
TAB_FAAA                                                         1         

Exporting tables (scripts or data) of FENG_U1
exporting table FENG_U1.TAB_FAAA ...
  exporting DDL of FENG_U1.TAB_FAAA ...
  exporting data of FENG_U1.TAB_FAAA ...
    data exporting success, 3 rows are dumped.

  exporting indexes on FENG_U1.TAB_FAAA ...
  exporting constraints on FENG_U1.TAB_FAAA ...

Exporting procedures/functions/triggers of schema FENG_U1 ...
Exporting views of schema FENG_U1 ...
Exporting synonyms of schema FENG_U1 ...
Exporting package of schema FENG_U1 ...
End of export schema FENG_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = FENG_U1
-- TABLESPACE FILTER = TBS_FENG1, tbs_feng1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


CREATE USER "FENG_U1" IDENTIFIED BY 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED DEFAULT TABLESPACE "USERS";

GRANT DBA TO "FENG_U1";

ALTER SESSION SET CURRENT_SCHEMA = FENG_U1;
DROP TABLE IF EXISTS "TAB_FAAA" CASCADE CONSTRAINTS;
CREATE TABLE "TAB_FAAA"
(
  "ID" BINARY_INTEGER,
  "NUM" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "TBS_FENG1"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (1,100,'m');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (2,200,'m_s');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (3,300,'dd');
COMMIT;
CREATE INDEX "INDEX_AAA_ID" ON "TAB_FAAA"("ID")
TABLESPACE "tbs_feng1"
INITRANS 2
PCTFREE 8;
CREATE INDEX "INDEX_AAA_NUM" ON "TAB_FAAA"("NUM")
TABLESPACE "TBS_FENG1"
INITRANS 2
PCTFREE 8;

Logical export succeeded.

SQL> exp users = feng_u1 create_user = Y role = Y grant = Y tablespace_filter = "tbs_feng1" file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = FENG_U1
-- TABLESPACE FILTER = tbs_feng1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema FENG_U1 ...
Exporting user definition of schema FENG_U1 ...
Exporting grant role and privilege of schema FENG_U1 ...
Grant privilege to schema FENG_U1 ...
Grant role to schema FENG_U1 ...
Exporting sequence of schema FENG_U1 ...
Exporting profile of schema FENG_U1 ...
Exporting type of schema FENG_U1 ...
Exporting tables of schema FENG_U1 ...
Reading table objects of FENG_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of FENG_U1
Exporting procedures/functions/triggers of schema FENG_U1 ...
Exporting views of schema FENG_U1 ...
Exporting synonyms of schema FENG_U1 ...
Exporting package of schema FENG_U1 ...
End of export schema FENG_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = FENG_U1
-- TABLESPACE FILTER = tbs_feng1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


CREATE USER "FENG_U1" IDENTIFIED BY 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED DEFAULT TABLESPACE "USERS";

GRANT DBA TO "FENG_U1";

ALTER SESSION SET CURRENT_SCHEMA = FENG_U1;
Logical export succeeded.

SQL> exp users = feng_u1 create_user = Y role = Y grant = Y tablespace_filter = USERS file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = FENG_U1
-- TABLESPACE FILTER = USERS
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema FENG_U1 ...
Exporting user definition of schema FENG_U1 ...
Exporting grant role and privilege of schema FENG_U1 ...
Grant privilege to schema FENG_U1 ...
Grant role to schema FENG_U1 ...
Exporting sequence of schema FENG_U1 ...
Exporting profile of schema FENG_U1 ...
Exporting type of schema FENG_U1 ...
Exporting tables of schema FENG_U1 ...
Reading table objects of FENG_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
TAB_FBBB                                                         1         

Exporting tables (scripts or data) of FENG_U1
exporting table FENG_U1.TAB_FBBB ...
  exporting DDL of FENG_U1.TAB_FBBB ...
  exporting data of FENG_U1.TAB_FBBB ...
    data exporting success, 3 rows are dumped.

  exporting indexes on FENG_U1.TAB_FBBB ...
  exporting constraints on FENG_U1.TAB_FBBB ...

Exporting procedures/functions/triggers of schema FENG_U1 ...
Exporting views of schema FENG_U1 ...
Exporting synonyms of schema FENG_U1 ...
Exporting package of schema FENG_U1 ...
End of export schema FENG_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = FENG_U1
-- TABLESPACE FILTER = USERS
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


CREATE USER "FENG_U1" IDENTIFIED BY 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED DEFAULT TABLESPACE "USERS";

GRANT DBA TO "FENG_U1";

ALTER SESSION SET CURRENT_SCHEMA = FENG_U1;
DROP TABLE IF EXISTS "TAB_FBBB" CASCADE CONSTRAINTS;
CREATE TABLE "TAB_FBBB"
(
  "ID" BINARY_INTEGER,
  "NUM" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "TAB_FBBB" ("ID","NUM","NAME") values
  (1,100,'m');
INSERT INTO "TAB_FBBB" ("ID","NUM","NAME") values
  (2,200,'m_s');
INSERT INTO "TAB_FBBB" ("ID","NUM","NAME") values
  (3,300,'dd');
COMMIT;
CREATE INDEX "INDEX_BBB_NUM" ON "TAB_FBBB"("NUM")
TABLESPACE "USERS"
INITRANS 2
PCTFREE 8;

Logical export succeeded.

SQL> exp users = feng_u1 create_user = Y role = Y grant = Y file = "stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = FENG_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema FENG_U1 ...
Exporting user definition of schema FENG_U1 ...
Exporting grant role and privilege of schema FENG_U1 ...
Grant privilege to schema FENG_U1 ...
Grant role to schema FENG_U1 ...
Exporting sequence of schema FENG_U1 ...
Exporting profile of schema FENG_U1 ...
Exporting type of schema FENG_U1 ...
Exporting tables of schema FENG_U1 ...
Reading table objects of FENG_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
TAB_FAAA                                                         1         
TAB_FBBB                                                         2         

Exporting tables (scripts or data) of FENG_U1
exporting table FENG_U1.TAB_FAAA ...
  exporting DDL of FENG_U1.TAB_FAAA ...
  exporting data of FENG_U1.TAB_FAAA ...
    data exporting success, 3 rows are dumped.

  exporting indexes on FENG_U1.TAB_FAAA ...
  exporting constraints on FENG_U1.TAB_FAAA ...

exporting table FENG_U1.TAB_FBBB ...
  exporting DDL of FENG_U1.TAB_FBBB ...
  exporting data of FENG_U1.TAB_FBBB ...
    data exporting success, 3 rows are dumped.

  exporting indexes on FENG_U1.TAB_FBBB ...
  exporting constraints on FENG_U1.TAB_FBBB ...

Exporting procedures/functions/triggers of schema FENG_U1 ...
Exporting views of schema FENG_U1 ...
Exporting synonyms of schema FENG_U1 ...
Exporting package of schema FENG_U1 ...
End of export schema FENG_U1 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = FENG_U1
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


CREATE USER "FENG_U1" IDENTIFIED BY 'z9MZsgYA0Ac88PIqy1wrPgnKHq4izEGUzbQG29z5I1P917nkIWwBwYe9FIH6DIqRJ2vgQOYpXtMBIBC7Qx6WOVthvWX4V9/UcZLGA0/FlMPfl1YJMoQhbA==' ENCRYPTED DEFAULT TABLESPACE "USERS";

GRANT DBA TO "FENG_U1";

ALTER SESSION SET CURRENT_SCHEMA = FENG_U1;
DROP TABLE IF EXISTS "TAB_FAAA" CASCADE CONSTRAINTS;
CREATE TABLE "TAB_FAAA"
(
  "ID" BINARY_INTEGER,
  "NUM" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "TBS_FENG1"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (1,100,'m');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (2,200,'m_s');
INSERT INTO "TAB_FAAA" ("ID","NUM","NAME") values
  (3,300,'dd');
COMMIT;
CREATE INDEX "INDEX_AAA_ID" ON "TAB_FAAA"("ID")
TABLESPACE "tbs_feng1"
INITRANS 2
PCTFREE 8;
CREATE INDEX "INDEX_AAA_NUM" ON "TAB_FAAA"("NUM")
TABLESPACE "TBS_FENG1"
INITRANS 2
PCTFREE 8;

DROP TABLE IF EXISTS "TAB_FBBB" CASCADE CONSTRAINTS;
CREATE TABLE "TAB_FBBB"
(
  "ID" BINARY_INTEGER,
  "NUM" BINARY_INTEGER,
  "NAME" VARCHAR(10 BYTE)
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
INSERT INTO "TAB_FBBB" ("ID","NUM","NAME") values
  (1,100,'m');
INSERT INTO "TAB_FBBB" ("ID","NUM","NAME") values
  (2,200,'m_s');
INSERT INTO "TAB_FBBB" ("ID","NUM","NAME") values
  (3,300,'dd');
COMMIT;
CREATE INDEX "INDEX_BBB_NUM" ON "TAB_FBBB"("NUM")
TABLESPACE "USERS"
INITRANS 2
PCTFREE 8;

Logical export succeeded.

SQL> exp users = feng_u1 create_user = Y role = Y grant = Y tablespace_filter = TBS_FENG1 tablespace = Y file = "temp_file_feng.sql" log="tmp_file.log";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = FENG_U1
-- TABLESPACE FILTER = TBS_FENG1
-- FILE TYPE = TXT
-- DUMP FILE = temp_file_feng.sql
-- LOG FILE = tmp_file.log
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = Y
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = Y
-- ROLE = Y
-- GRANT = Y
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting roles  ...
Grant privilege to role ...
Exporting schema FENG_U1 ...
Exporting user definition of schema FENG_U1 ...
Exporting grant role and privilege of schema FENG_U1 ...
Grant privilege to schema FENG_U1 ...
Grant role to schema FENG_U1 ...
Exporting sequence of schema FENG_U1 ...
Exporting profile of schema FENG_U1 ...
Exporting type of schema FENG_U1 ...
Exporting tables of schema FENG_U1 ...
Reading table objects of FENG_U1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
TAB_FAAA                                                         1         

Exporting tables (scripts or data) of FENG_U1
exporting table FENG_U1.TAB_FAAA ...
  exporting DDL of FENG_U1.TAB_FAAA ...
  exporting data of FENG_U1.TAB_FAAA ...
    data exporting success, 3 rows are dumped.

  exporting indexes on FENG_U1.TAB_FAAA ...
  exporting constraints on FENG_U1.TAB_FAAA ...

Exporting procedures/functions/triggers of schema FENG_U1 ...
Exporting views of schema FENG_U1 ...
Exporting synonyms of schema FENG_U1 ...
Exporting package of schema FENG_U1 ...
End of export schema FENG_U1 ...

Logical export succeeded.

SQL> exp tables = tab_faaa tablespace_filter = TBS_FENG1 file = "stdout";
Parsing export options ... 
Verify options ...
  verify tables ...

CT-00601, Sql syntax error: can not export SYS schema
Logical export failed.

SQL> 
SQL> exp users = feng_u2 tablespace = Y file = "tbsname_case_sensitive.sql" log="tbsname_case_sensitive.log";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = FENG_U2
-- FILE TYPE = TXT
-- DUMP FILE = tbsname_case_sensitive.sql
-- LOG FILE = tbsname_case_sensitive.log
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = Y
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema FENG_U2 ...
Exporting sequence of schema FENG_U2 ...
Exporting profile of schema FENG_U2 ...
Exporting type of schema FENG_U2 ...
Exporting tables of schema FENG_U2 ...
Reading table objects of FENG_U2

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of FENG_U2
Exporting procedures/functions/triggers of schema FENG_U2 ...
Exporting views of schema FENG_U2 ...
Exporting synonyms of schema FENG_U2 ...
Exporting package of schema FENG_U2 ...
End of export schema FENG_U2 ...

Logical export succeeded.

SQL> 
SQL> drop table if exists tab_faaa;

Succeed.

SQL> drop table if exists tab_fbbb;

Succeed.

SQL> drop tablespace TBS_FENG1 including contents and datafiles;

Succeed.

SQL> drop tablespace "tbs_feng1" including contents and datafiles;

Succeed.

SQL> 
SQL> select TABLESPACE_NAME from dba_data_files where TABLESPACE_NAME in('tbs_feng1','TBS_FENG1') order by TABLESPACE_NAME;

TABLESPACE_NAME                                                 
----------------------------------------------------------------

0 rows fetched.

SQL> imp users = feng_u2 IGNORE=Y file = "./data/tbsname_case_sensitive_txt.sql" log="tbsname_case_sensitive_txt.log";
Parsing import options ... 
Verify options ...
  verify schema ...
Starting import ...
Preparing to import ...
-- IMPORT TYPE = SCHEMA
-- IMPORT OBJECTS = FENG_U2
-- DUMP FILE = ./data/tbsname_case_sensitive_txt.sql
-- LOG FILE = tbsname_case_sensitive_txt.log
-- FILE TYPE = TXT
-- SHOW = N
-- FEEDBACK = 10000
-- PARALLEL = 1
-- DDL_PARALLEL = 1
-- CONTENT_MODE = ALL
-- IGNORE = Y
-- CREATE_USER = N
-- TIMING = OFF
-- BATCH_COUNT = 10000
-- DISABLE_TRIGGER = Y
-- NOLOGGING = N

data importing success, 0 rows are loaded.
Logical import succeeded.

SQL> select TABLESPACE_NAME from dba_data_files where TABLESPACE_NAME in('tbs_feng1','TBS_FENG1') order by TABLESPACE_NAME;

TABLESPACE_NAME                                                 
----------------------------------------------------------------
TBS_FENG1                                                       
tbs_feng1                                                       

2 rows fetched.

SQL> --exp users = feng_u2 tablespace = Y file = "tbsname_case_sensitive.sql" FILETYPE=BIN log="tbsname_case_sensitive.log";
SQL> --drop tablespace TBS_FENG1 including contents and datafiles;
SQL> --drop tablespace "tbs_feng1" including contents and datafiles;
SQL> --select TABLESPACE_NAME from dba_data_files where TABLESPACE_NAME in('tbs_feng1','TBS_FENG1') order by TABLESPACE_NAME;
SQL> --imp users = feng_u2 IGNORE=Y file = "tbsname_case_sensitive.sql" FILETYPE=BIN log="tbsname_case_sensitive.log";
SQL> --select TABLESPACE_NAME from dba_data_files where TABLESPACE_NAME in('tbs_feng1','TBS_FENG1') order by TABLESPACE_NAME;
SQL> drop tablespace TBS_FENG1 including contents and datafiles;

Succeed.

SQL> drop tablespace "tbs_feng1" including contents and datafiles;

Succeed.

SQL> 
SQL> conn sys/Huawei@123@127.0.0.1:1611

connected.

SQL> drop user if exists exp_user1 cascade;

Succeed.

SQL> create user exp_user1 IDENTIFIED by 'Changme_123';

Succeed.

SQL> grant dba to exp_user1;

Succeed.

SQL> 
SQL> drop user if exists exp_user2 cascade;

Succeed.

SQL> create user exp_user2 IDENTIFIED by 'Changme_123';

Succeed.

SQL> grant dba to exp_user2;

Succeed.

SQL> 
SQL> \! ctsql exp_user1/Changme_123@127.0.0.1:1611 -c "@./sql/TBL_CLOUDSERVICE_LOGCONFIG.sql";


SQL> 
SQL> \! rm -rf exp_dir/


SQL> \! mkdir exp_dir


SQL> 
SQL> conn exp_user1/Changme_123@127.0.0.1:1611

connected.

SQL> update "TBL_CLOUDSERVICE_LOGCONFIG" set "LOGCONFIGS" = "LOGCONFIGS" || "LOGCONFIGS";

40 rows affected.

SQL> commit;

Succeed.

SQL> 
SQL> select count(*) from "TBL_CLOUDSERVICE_LOGCONFIG" where lengthb("LOGCONFIGS") > 8000;

COUNT(*)            
--------------------
5                   

1 rows fetched.

SQL> 
SQL> exp tables=TBL_CLOUDSERVICE_LOGCONFIG filetype=bin file="./exp_dir/XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX_TBL_CLOUDSERVICE_LOGCONFIG.sql";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TBL_CLOUDSERVICE_LOGCONFIG
-- FILE TYPE = BIN
-- DUMP FILE = ./exp_dir/XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX_TBL_CLOUDSERVICE_LOGCONFIG.sql
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER1.TBL_CLOUDSERVICE_LOGCONFIG ...
  exporting DDL of EXP_USER1.TBL_CLOUDSERVICE_LOGCONFIG ...
  exporting data of EXP_USER1.TBL_CLOUDSERVICE_LOGCONFIG ...
    data exporting success, 40 rows are dumped.

  exporting indexes on EXP_USER1.TBL_CLOUDSERVICE_LOGCONFIG ...
  exporting constraints on EXP_USER1.TBL_CLOUDSERVICE_LOGCONFIG ...

Logical export succeeded.

SQL> 
SQL> conn exp_user2/Changme_123@127.0.0.1:1611

connected.

SQL> imp tables=TBL_CLOUDSERVICE_LOGCONFIG filetype=bin file="./exp_dir/XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX_TBL_CLOUDSERVICE_LOGCONFIG.sql";
Parsing import options ... 
Verify options ...
  verify tables ...
Starting import ...
Preparing to import ...
-- IMPORT TYPE = TABLE
-- IMPORT OBJECTS = TBL_CLOUDSERVICE_LOGCONFIG
-- DUMP FILE = ./exp_dir/XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX_TBL_CLOUDSERVICE_LOGCONFIG.sql
-- LOG FILE = 
-- FILE TYPE = BIN
-- SHOW = N
-- FEEDBACK = 10000
-- PARALLEL = 1
-- DDL_PARALLEL = 1
-- CONTENT_MODE = ALL
-- IGNORE = N
-- CREATE_USER = N
-- TIMING = OFF
-- BATCH_COUNT = 10000
-- DISABLE_TRIGGER = Y
-- NOLOGGING = N

  Importing sequence of schema EXP_USER2 ...
    Sequence importing success, 0 rows are loaded.
  Importing profile of schema EXP_USER2 ...
    Profile importing success, 0 rows are loaded.
  Importing type of schema EXP_USER2 ...
    Type importing success, 0 rows are loaded.

  Importing tables of schema EXP_USER2 ,total number : 1 ...
    The order of importing table is:
    TABLE NAME                                                       RECORD NUMBER 
    ---------------------------------------------------------------- --------------
    TBL_CLOUDSERVICE_LOGCONFIG                                           40        

  Importing foreign key of schema EXP_USER2 ...
    Foreign key importing success, 0 rows are loaded.

  Importing fuction/procedure/trigger of schema EXP_USER2 ...
    Fuction/procedure/trigger importing success, 0 rows are loaded.

  Importing view of schema EXP_USER2 ...
    View importing success, 0 rows are loaded.

  Importing synonym of schema EXP_USER2 ...
    Synonym importing success, 0 rows are loaded.

  Importing package of schema EXP_USER2 ...
    Package importing success, 0 rows are loaded.

data importing success, 40 rows are loaded.
Logical import succeeded.

SQL> 
SQL> conn sys/Huawei@123@127.0.0.1:1611

connected.

SQL> select count(*), sum(length(LOGCONFIGID)), sum(lengthb(LOGCONFIGS)) from exp_user1.TBL_CLOUDSERVICE_LOGCONFIG 
  2 union select count(*), sum(length(LOGCONFIGID)), sum(lengthb(LOGCONFIGS)) from exp_user2.TBL_CLOUDSERVICE_LOGCONFIG;

COUNT(*)             SUM(LENGTH(LOGCONFIGID))                 SUM(LENGTHB(LOGCONFIGS))                
-------------------- ---------------------------------------- ----------------------------------------
40                   692                                      136998                                  

1 rows fetched.

SQL> 
SQL> 
SQL> -- DTS2018101807176
SQL> create user exp_user_pfa IDENTIFIED by 'exp_user123';

Succeed.

SQL> grant dba to exp_user_pfa;

Succeed.

SQL> conn exp_user_pfa/exp_user123@127.0.0.1:1611

connected.

SQL> 
SQL> show parameter upper;

NAME                                                             DATATYPE             VALUE                                                            RUNTIME_VALUE                                                    EFFECTIVE           
---------------------------------------------------------------- -------------------- ---------------------------------------------------------------- ---------------------------------------------------------------- --------------------
ARCH_CLEAN_UPPER_LIMIT                                           CT_TYPE_INTEGER      85                                                               85                                                               immediately
UPPER_CASE_TABLE_NAMES                                           CT_TYPE_BOOLEAN      TRUE                                                             TRUE                                                             reboot


SQL> 
SQL> drop table DTS2018101807176_T1;

CT-00843, The table or view EXP_USER_PFA.DTS2018101807176_T1 does not exist.
SQL> create table DTS2018101807176_T1 (
  2     `column1` int,
  3     `column2` int
  4 );

Succeed.

SQL> desc DTS2018101807176_T1;

Name                                Null?    Type                                
----------------------------------- -------- ------------------------------------
column1                                      BINARY_INTEGER                      
column2                                      BINARY_INTEGER                      

SQL> exp TABLES=DTS2018101807176_T1 CONTENT=METADATA_ONLY file="exp_user_pfa.dmp";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = DTS2018101807176_T1
-- FILE TYPE = TXT
-- DUMP FILE = exp_user_pfa.dmp
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = METADATA_ONLY
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER_PFA.DTS2018101807176_T1 ...
  exporting DDL of EXP_USER_PFA.DTS2018101807176_T1 ...

  exporting indexes on EXP_USER_PFA.DTS2018101807176_T1 ...
  exporting constraints on EXP_USER_PFA.DTS2018101807176_T1 ...

Logical export succeeded.

SQL> drop table DTS2018101807176_T1;

Succeed.

SQL> 
SQL> \! ctsql exp_user_pfa/exp_user123@127.0.0.1:1611 -c "@exp_user_pfa.dmp"


SQL> 
SQL> desc DTS2018101807176_T1;

Name                                Null?    Type                                
----------------------------------- -------- ------------------------------------
column1                                      BINARY_INTEGER                      
column2                                      BINARY_INTEGER                      

SQL> select * from DTS2018101807176_T1 where `column1` = 0;

column1      column2     
------------ ------------

0 rows fetched.

SQL> 
SQL> create table DTS2018111210546
  2 (
  3     id1 int,
  4 	id2 int
  5 );

Succeed.

SQL> 
SQL> create index ix_DTS2018111210546_id1 on DTS2018111210546(id1);

Succeed.

SQL> create unique index ix_DTS2018111210546_id2 on DTS2018111210546(id2);

Succeed.

SQL> 
SQL> create table DTS2018111210499
  2 (
  3     id1 int,
  4 	id2 int
  5 );

Succeed.

SQL> 
SQL> COMMENT ON COLUMN DTS2018111210499.id1 is 'id1 column';

Succeed.

SQL> COMMENT ON COLUMN DTS2018111210499.id2 is 'id2 column';

Succeed.

SQL> 
SQL> exp tables=DTS2018111210546,DTS2018111210499 file="stdout" quote_names=Y;
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = DTS2018111210546, DTS2018111210499
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER_PFA.DTS2018111210546 ...
  exporting DDL of EXP_USER_PFA.DTS2018111210546 ...
  exporting data of EXP_USER_PFA.DTS2018111210546 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER_PFA.DTS2018111210546 ...
  exporting constraints on EXP_USER_PFA.DTS2018111210546 ...

exporting table EXP_USER_PFA.DTS2018111210499 ...
  exporting DDL of EXP_USER_PFA.DTS2018111210499 ...
  exporting data of EXP_USER_PFA.DTS2018111210499 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER_PFA.DTS2018111210499 ...
  exporting constraints on EXP_USER_PFA.DTS2018111210499 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = DTS2018111210546, DTS2018111210499
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "DTS2018111210546" CASCADE CONSTRAINTS;
CREATE TABLE "DTS2018111210546"
(
  "ID1" BINARY_INTEGER,
  "ID2" BINARY_INTEGER
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
CREATE INDEX "IX_DTS2018111210546_ID1" ON "DTS2018111210546"("ID1")
TABLESPACE "USERS"
INITRANS 2
PCTFREE 8;
CREATE UNIQUE INDEX "IX_DTS2018111210546_ID2" ON "DTS2018111210546"("ID2")
TABLESPACE "USERS"
INITRANS 2
PCTFREE 8;

DROP TABLE IF EXISTS "DTS2018111210499" CASCADE CONSTRAINTS;
CREATE TABLE "DTS2018111210499"
(
  "ID1" BINARY_INTEGER,
  "ID2" BINARY_INTEGER
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
COMMENT ON COLUMN "DTS2018111210499"."ID1" IS 'id1 column';
COMMENT ON COLUMN "DTS2018111210499"."ID2" IS 'id2 column';



Logical export succeeded.

SQL> 
SQL> 
SQL> -- DTS2018120404189
SQL> DROP TABLE IF EXISTS "DTS2018120404189";

Succeed.

SQL> CREATE TABLE "DTS2018120404189"
  2 (
  3 "DTS2018120404189_COL_1" VARCHAR(75 BYTE) NOT NULL,
  4 "DTS2018120404189_COL_2" NUMBER(10) NOT NULL,
  5 "DTS2018120404189_COL_3" NUMBER(5) NOT NULL,
  6 "DTS2018120404189_COL_4" NUMBER(5) NOT NULL,
  7 "DTS2018120404189_COL_5" NUMBER(10) NOT NULL,
  8 "DTS2018120404189_COL_6" NUMBER(10) NOT NULL,
  9 "DTS2018120404189_COL_7" NUMBER(3) NOT NULL,
 10 "DTS2018120404189_COL_8" NUMBER(10) NOT NULL,
 11 "DTS2018120404189_COL_9" NUMBER(10) NOT NULL DEFAULT -1,
 12 "DTS2018120404189_COL_10" NUMBER(10) NOT NULL,
 13 "DTS2018120404189_COL_11" NUMBER(3) NOT NULL DEFAULT 2,
 14 PRIMARY KEY("DTS2018120404189_COL_1", "DTS2018120404189_COL_2", "DTS2018120404189_COL_3", "DTS2018120404189_COL_4", "DTS2018120404189_COL_5","DTS2018120404189_COL_6","DTS2018120404189_COL_7","DTS2018120404189_COL_8")
 15 );

Succeed.

SQL> 
SQL> exp tables="DTS2018120404189" file="stdout";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = DTS2018120404189
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER_PFA.DTS2018120404189 ...
  exporting DDL of EXP_USER_PFA.DTS2018120404189 ...
  exporting data of EXP_USER_PFA.DTS2018120404189 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER_PFA.DTS2018120404189 ...
  exporting constraints on EXP_USER_PFA.DTS2018120404189 ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = DTS2018120404189
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "DTS2018120404189" CASCADE CONSTRAINTS;
CREATE TABLE "DTS2018120404189"
(
  "DTS2018120404189_COL_1" VARCHAR(75 BYTE) NOT NULL,
  "DTS2018120404189_COL_2" NUMBER(10) NOT NULL,
  "DTS2018120404189_COL_3" NUMBER(5) NOT NULL,
  "DTS2018120404189_COL_4" NUMBER(5) NOT NULL,
  "DTS2018120404189_COL_5" NUMBER(10) NOT NULL,
  "DTS2018120404189_COL_6" NUMBER(10) NOT NULL,
  "DTS2018120404189_COL_7" NUMBER(3) NOT NULL,
  "DTS2018120404189_COL_8" NUMBER(10) NOT NULL,
  "DTS2018120404189_COL_9" NUMBER(10) NOT NULL DEFAULT -1,
  "DTS2018120404189_COL_10" NUMBER(10) NOT NULL,
  "DTS2018120404189_COL_11" NUMBER(3) NOT NULL DEFAULT 2
)
TABLESPACE "USERS"
INITRANS 2
MAXTRANS 255
PCTFREE 8
FORMAT ASF;
ALTER TABLE "DTS2018120404189" ADD PRIMARY KEY("DTS2018120404189_COL_1", "DTS2018120404189_COL_2", "DTS2018120404189_COL_3", "DTS2018120404189_COL_4", "DTS2018120404189_COL_5", "DTS2018120404189_COL_6", "DTS2018120404189_COL_7", "DTS2018120404189_COL_8");


Logical export succeeded.

SQL> 
SQL> -- DTS2018121208820, the 
SQL> DROP TABLE IF EXISTS "DTS2018121208820";

Succeed.

SQL> CREATE TABLE "DTS2018121208820"
  2 (
  3   "DTS2018121208820_COL_1" VARCHAR(1024)
  4 );

Succeed.

SQL> 
SQL> insert into "DTS2018121208820" values('000000000000000000000000000000000000');

1 rows affected.

SQL> 
SQL> exp tables="DTS2018121208820" file="DTS2018121208820.exp" log="DTS2018121208820.log";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = DTS2018121208820
-- FILE TYPE = TXT
-- DUMP FILE = DTS2018121208820.exp
-- LOG FILE = DTS2018121208820.log
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER_PFA.DTS2018121208820 ...
  exporting DDL of EXP_USER_PFA.DTS2018121208820 ...
  exporting data of EXP_USER_PFA.DTS2018121208820 ...
    data exporting success, 1 rows are dumped.

  exporting indexes on EXP_USER_PFA.DTS2018121208820 ...
  exporting constraints on EXP_USER_PFA.DTS2018121208820 ...

Logical export succeeded.

SQL> 
SQL> \! stat -c "%a %A %n" DTS2018121208820.exp DTS2018121208820.log  > DTS2018121208820.permissions 


SQL> 
SQL> dump table DTS2018121208820 into file "DTS2018121208820.dump";
1 rows dumped.

Dump TABLE successfully:
  1 rows are totally dumped.

SQL> 
SQL> \! stat -c "%a %A %n" DTS2018121208820.dump  >> DTS2018121208820.permissions


SQL> 
SQL> load data infile "DTS2018121208820.permissions" into table "DTS2018121208820";
3 rows have been committed.

Complete the data load.
totally read rows: 3
     ignored rows: 0
      loaded rows: 3
   committed rows: 3
       error rows: 0
        skip rows: 0
SQL> 
SQL> select * from "DTS2018121208820" order by 1;

DTS2018121208820_COL_1                                          
----------------------------------------------------------------
000000000000000000000000000000000000                            
600 -rw------- DTS2018121208820.dump                            
600 -rw------- DTS2018121208820.exp                             
600 -rw------- DTS2018121208820.log                             

4 rows fetched.

SQL> 
SQL> -- DTS2018112900633, DTS2018112811658
SQL> create user exp_user_long_proc IDENTIFIED by 'exp_user123';

Succeed.

SQL> grant dba to exp_user_long_proc;

Succeed.

SQL> conn exp_user_long_proc/exp_user123@127.0.0.1:1611

connected.

SQL> 
SQL> \! ctsql exp_user_long_proc/exp_user123@127.0.0.1:1611 -c "@./sql/long_source_proc_test.sql"


SQL> 
SQL> exp users=exp_user_long_proc file="stdout";
Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_USER_LONG_PROC
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema EXP_USER_LONG_PROC ...
Exporting sequence of schema EXP_USER_LONG_PROC ...
Exporting profile of schema EXP_USER_LONG_PROC ...
Exporting type of schema EXP_USER_LONG_PROC ...
Exporting tables of schema EXP_USER_LONG_PROC ...
Reading table objects of EXP_USER_LONG_PROC

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of EXP_USER_LONG_PROC
Exporting procedures/functions/triggers of schema EXP_USER_LONG_PROC ...
  exporting PROCEDURE EXP_USER_LONG_PROC.LONG_SOURCE_PROC_TEST
Exporting views of schema EXP_USER_LONG_PROC ...
Exporting synonyms of schema EXP_USER_LONG_PROC ...
Exporting package of schema EXP_USER_LONG_PROC ...
End of export schema EXP_USER_LONG_PROC ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_USER_LONG_PROC
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';


ALTER SESSION SET CURRENT_SCHEMA = EXP_USER_LONG_PROC;
CREATE OR REPLACE PROCEDURE "LONG_SOURCE_PROC_TEST"
AS
   v_t      timestamp;
   v_num    number;
BEGIN
   v_num := 1;
-- Relational Model
-- In his seminal 1970 paper "A Relational Model of Data for Large Shared Data Banks,"
-- E. F. Codd defined a relational model based on mathematical set theory. 
-- Today, the most widely accepted database model is the relational model.
-- A relational database is a database that conforms to the relational model. 
-- The relational model has the following major aspects:
-- Structures
-- Well-defined objects store or access the data of a database.
-- Operations
-- Clearly defined actions enable applications to manipulate the data and
-- structures of a database.
-- Integrity rules
-- Integrity rules govern operations on the data and structures of a database.
-- A relational database stores data in a set of simple relations. A relation 
-- is a set of tuples. A tuple is an unordered set of attribute values.
-- A table is a two-dimensional representation of a relation in the form of 
-- rows (tuples) and columns (attributes). Each row in a table has the same set of 
-- columns. A relational database is a database that stores data in 
-- relations (tables). For example, a relational database could store information 
-- about company employees in an employee table, a department table, 
-- and a salary table.
-- See Also:
-- "A Relational Model of Data for Large Shared Data Banks" for an abstract and 
-- link to Codd's paper
-- Relational Database Management System (RDBE)
-- The relational model is the basis for a relational database management 
-- system (RDBE). An RDBE moves data into a database, stores the data, 
-- and retrieves it so that applications can manipulate it.
-- An RDBE distinguishes between the following types of operations:
-- Logical operations
-- In this case, an application specifies what content is required. For example, 
-- an application requests an employee name or adds an employee record to a table.
-- Physical operations
-- In this case, the RDBE determines how things should be done and carries out 
-- the operation. For example, after an application queries a table, the database 
-- may use an index to find the requested rows, read the data into memory, and 
-- perform many other steps before returning a result to the user. The RDBE 
-- stores and retrieves data so that physical operations are transparent 
-- to database applications.
-- Oracle Database is an RDBE. An RDBE that implements object-oriented features 
-- such as user-defined types, inheritance, and polymorphism is called an 
-- object-relational database management system (ORDBE). Oracle Database has 
-- extended the relational model to an object-relational model, making it 
-- possible to store complex business models in a relational database.
-- Brief History of Oracle Database
-- The current version of Oracle Database is the result of over 35 years of 
-- innovative development.
-- Highlights in the evolution of Oracle Database include the following:
-- Founding of Oracle
-- In 1977, Larry Ellison, Bob Miner, and Ed Oates started the consultancy 
-- Software Development Laboratories, which became Relational Software, Inc. (RSI). 
-- In 1983, RSI became Oracle Systems Corporation and then later Oracle Corporation.
-- First commercially available RDBE
-- In 1979, RSI introduced Oracle V2 (Version 2) as the first commercially 
-- available SQL-based RDBE, a landmark event in the history of relational 
-- databases.
-- Portable version of Oracle Database
-- Oracle Version 3, released in 1983, was the first relational database to run on 
-- mainframes, minicomputers, and PCs. The database was written in C, enabling the 
-- database to be ported to multiple platforms.
-- Enhancements to concurrency control, data distribution, and scalability
-- Version 4 introduced multiversion read consistency. Version 5, released in 1985, 
-- supported client/server computing and distributed database systems. Version 6 
-- brought enhancements to disk I/O, row locking, scalability, and backup and
-- recovery. 
-- Also, Version 6 introduced the first version of the PL/SQL language, a 
-- proprietary procedural extension to SQL.
-- PL/SQL stored program units
-- Oracle7, released in 1992, introduced PL/SQL stored procedures and triggers.
-- Objects and partitioning
-- Oracle8 was released in 1997 as the object-relational database, supporting many 
-- new data types. Additionally, Oracle8 supported partitioning of large tables.
-- Internet computing
-- Oracle8i Database, released in 1999, provided native support for internet 
-- protocols and server-side support for Java. Oracle8i was designed for internet 
-- computing, enabling the database to be deployed in a multitier environment.
-- Oracle Real Application Clusters (Oracle RAC)
-- Oracle9i Database introduced Oracle RAC in 2001, enabling multiple instances to 
-- access a single database simultaneously. Additionally, Oracle XML Database 
-- (Oracle XML DB) introduced the ability to store and query XML.
-- Grid computing
-- Oracle Database 10g introduced grid computing in 2003. This release enabled 
-- organizations to virtualize computing resources by building a grid
-- infrastructure 
-- based on low-cost commodity servers. A key goal was to make the database 
-- self-managing and self-tuning. Oracle Automatic Storage Management (Oracle ASM) 
-- helped achieve this goal by virtualizing and simplifying database storage 
-- management.
-- Manageability, diagnosability, and availability
-- Oracle Database 11g, released in 2007, introduced a host of new features that 
-- enabled administrators and developers to adapt quickly to changing business 
-- requirements. The key to adaptability is simplifying the information 
-- infrastructure by consolidating information and using automation wherever 
-- possible.
-- Plugging In to the Cloud
-- Oracle Database 12c, released in 2013, was designed for the Cloud, featuring 
-- a new Multitenant architecture, In-Memory column store, and support for JSON 
-- documents. Oracle Database 12c helps customers make more efficient use of their 
-- IT resources, while continuing to reduce costs and improve service levels for 
-- users.
-- Schema Objects
-- One characteristic of an RDBE is the independence of physical data storage from 
-- logical data structures.
-- In Oracle Database, a database schema is a collection of logical data 
-- structures, 
-- or schema objects. A database user owns a database schema, which has the same 
-- name as the user name.
-- Schema objects are user-created structures that directly refer to the data in 
-- the database. The database supports many types of schema objects, the most 
-- important of which are tables and indexes.
-- A schema object is one type of database object. Some database objects, such as 
-- profiles and roles, do not reside in schemas.
-- See Also:
-- "Introduction to Schema Objects" to learn more about schema object types, 
-- storage, and dependencies
-- Tables
-- A table describes an entity such as employees.
-- You define a table with a table name, such as employees, and set of columns. In 
-- general, you give each column a name, a data type, and a width when you create 
-- the table.
-- A table is a set of rows. A column identifies an attribute of the entity 
-- described by the table, whereas a row identifies an instance of the entity. For 
-- example, attributes of the employees entity correspond to columns for employee 
-- ID and last name. A row identifies a specific employee.
-- You can optionally specify a rule, called an integrity constraint, for a 
-- column. One example is a NOT NULL integrity constraint. This constraint forces 
-- the column to contain a value in every row.
-- See Also:
-- "Overview of Tables" to learn about columns and rows, data types, table 
-- storage, and table compression
-- "Data Integrity" to learn about the possible types and states of constraints
-- Indexes
-- An index is an optional data structure that you can create on one or more 
-- columns of a table. Indexes can increase the performance of data retrieval.
-- When processing a request, the database can use available indexes to locate the 
-- requested rows efficiently. Indexes are useful when applications often query a 
-- specific row or range of rows.
-- Indexes are logically and physically independent of the data. Thus, you can 
-- drop and create indexes with no effect on the tables or other indexes. All 
-- applications continue to function after you drop an index.
-- See Also:
-- "Introduction to Indexes" to learn about the purpose and types of indexes
-- Data Access
-- A general requirement for a DBE is to adhere to accepted industry standards 
-- for a data access language.
-- Structured Query Language (SQL)
-- SQL is a set-based declarative language that provides an interface to an RDBE 
-- such as Oracle Database.
-- Procedural languages such as C describe how things should be done. SQL is 
-- nonprocedural and describes what should be done.
-- SQL is the ANSI standard language for relational databases. All operations on 
-- the data in an Oracle database are performed using SQL statements. For example, 
-- you use SQL to create tables and query and modify data in tables.
-- A SQL statement can be thought of as a very simple, but powerful, computer 
-- program or instruction. Users specify the result that they want (for example, 
-- the names of employees), not how to derive it. A SQL statement is a string of 
-- SQL text such as the following:
-- SELECT first_name, last_name FROM employees;
-- SQL statements enable you to perform the following tasks:
-- Query data
-- Insert, update, and delete rows in a table
-- Create, replace, alter, and drop objects
-- Control access to the database and its objects
-- Guarantee database consistency and integrity
-- SQL unifies the preceding tasks in one consistent language. Oracle SQL is an 
-- implementation of the ANSI standard. Oracle SQL supports numerous features that 
-- extend beyond standard SQL.
-- See Also:
-- "SQL" to learn more about SQL standards and the main types of SQL statements
-- PL/SQL and Java
-- PL/SQL is a procedural extension to Oracle SQL.
-- PL/SQL is integrated with Oracle Database, enabling you to use all of the 
-- Oracle Database SQL statements, functions, and data types. You can use PL/SQL 
-- to control the flow of a SQL program, use variables, and write error-handling 
-- procedures.
-- A primary benefit of PL/SQL is the ability to store application logic in the 
-- database itself. A PL/SQL procedure or function is a schema object that 
-- consists of a set of SQL statements and other PL/SQL constructs, grouped 
-- together, stored in the database, and run as a unit to solve a specific problem 
-- or to perform a set of related tasks. The principal benefit of server-side 
-- programming is that built-in functionality can be deployed anywhere.
-- Oracle Database can also store program units written in Java. A Java stored 
-- procedure is a Java method published to SQL and stored in the database for 
-- general use. You can call existing PL/SQL programs from Java and Java programs 
-- from PL/SQL.
-- See Also:
-- "Server-Side Programming: PL/SQL and Java"
-- "Client-Side Database Programming"
-- Transaction Management
-- Oracle Database is designed as a multiuser database. The database must ensure 
-- that multiple users can work concurrently without corrupting one another's data.
-- Transactions
-- A transaction is a logical, atomic unit of work that contains one or more SQL 
-- statements.
-- An RDBE must be able to group SQL statements so that they are either all 
-- committed, which means they are applied to the database, or all rolled back, 
-- which means they are undone.
-- An illustration of the need for transactions is a funds transfer from a savings 
-- account to a checking account. The transfer consists of the following separate 
-- operations:
-- Decrease the savings account.
-- Increase the checking account.
-- Record the transaction in the transaction journal.
-- Oracle Database guarantees that all three operations succeed or fail as a unit. 
-- For example, if a hardware failure prevents a statement in the transaction from 
-- executing, then the other statements must be rolled back.
-- 
-- Transactions are one feature that set Oracle Database apart from a file system. 
-- If you perform an atomic operation that updates several files, and if the 
-- system fails halfway through, then the files will not be consistent. In 
-- contrast, a transaction moves an Oracle database from one consistent state to 
-- another. The basic principle of a transaction is "all or nothing": an atomic 
-- operation succeeds or fails as a whole.
-- See Also:
-- "Transactions" to learn about the definition of a transaction, statement-level 
-- atomicity, and transaction control
-- Data Concurrency
-- A requirement of a multiuser RDBE is the control of data concurrency, which is 
-- the simultaneous access of the same data by multiple users.
-- Without concurrency controls, users could change data improperly, compromising 
-- data integrity. For example, one user could update a row while a different user 
-- simultaneously updates it.
-- If multiple users access the same data, then one way of managing concurrency is 
-- to make users wait. However, the goal of a DBE is to reduce wait time so it is 
-- either nonexistent or negligible. All SQL statements that modify data must 
-- proceed with as little interference as possible. Destructive interactions, 
-- which are interactions that incorrectly update data or alter underlying data 
-- structures, must be avoided.
-- Oracle Database uses locks to control concurrent access to data. A lock is a 
-- mechanism that prevents destructive interaction between transactions accessing 
-- a shared resource. Locks help ensure data integrity while allowing maximum 
-- concurrent access to data.
-- See Also:
-- "Overview of the Oracle Database Locking Mechanism"
-- Data Consistency
-- In Oracle Database, each user must see a consistent view of the data, including 
-- visible changes made by a user's own transactions and committed transactions of 
-- other users.
-- For example, the database must prevent the lost update problem, which occurs 
-- when one transaction sees uncommitted changes made by another concurrent 
-- transaction.
-- Oracle Database always enforces statement-level read consistency, which 
-- guarantees that the data that a single query returns is committed and 
-- consistent for a single point in time. Depending on the transaction isolation 
-- level, this point is the time at which the statement was opened or the time the 
-- transaction began. The Oracle Flashback Query feature enables you to specify 
-- this point in time explicitly.
-- The database can also provide read consistency to all queries in a transaction, 
-- known as transaction-level read consistency. In this case, each statement in a 
-- transaction sees data from the same point in time, which is the time at which 
-- the transaction began.
-- See Also:
-- "Data Concurrency and Consistency" to learn more about lost updates
-- Oracle Database Development Guide to learn about Oracle Flashback Query
-- 
   v_num := v_num + 1;
-- /* Per thread arena example. */
-- #include <stdio.h>
-- #include <stdlib.h>
-- #include <pthread.h>
-- #include <unistd.h>
-- #include <sys/types.h>
-- 
-- void* threadFunc(void* arg) {
--         printf("Before malloc in thread 1\n");
--         getchar();
--         char* addr = (char*) malloc(1000);
--         printf("After malloc and before free in thread 1\n");
--         getchar();
--         free(addr);
--         printf("After free in thread 1\n");
--         getchar();
-- }
-- 
-- int main() {
--         pthread_t t1;
--         void* s;
--         int ret;
--         char* addr;
-- 
--         printf("Welcome to per thread arena example::%d\n",getpid());
--         printf("Before malloc in main thread\n");
--         getchar();
--         addr = (char*) malloc(1000);
--         printf("After malloc and before free in main thread\n");
--         getchar();
--         free(addr);
--         printf("After free in main thread\n");
--         getchar();
--         ret = pthread_create(&t1, NULL, threadFunc, NULL);
--         if(ret)
--         {
--                 printf("Thread creation error\n");
--                 return -1;
--         }
--         ret = pthread_join(t1, &s);
--         if(ret)
--         {
--                 printf("Thread join error\n");
--                 return -1;
--         }
--         return 0;
-- }
-- 
-- Database Storage Structures
-- A database can be considered from both a physical and logical perspective.
-- 
-- Physical data is data viewable at the operating system level. For example, 
-- operating system utilities such as the Linux ls and ps can list database files 
-- and processes. Logical data such as a table is meaningful only for the 
-- database. A SQL statement can list the tables in an Oracle database, but an 
-- operating system utility cannot.
-- 
-- The database has physical structures and logical structures. Because the 
-- physical and logical structures are separate, you can manage the physical 
-- storage of data without affecting access to logical storage structures. For 
-- example, renaming a physical database file does not rename the tables whose 
-- data is stored in this file.
-- 
-- Physical Storage Structures
-- The physical database structures are the files that store the data.
-- 
-- When you execute a CREATE DATABASE statement, the following files are created:
-- 
   v_num := v_num + 1;
-- Data files
-- 
-- Every Oracle database has one or more physical data files, which contain all 
-- the database data. The data of logical database structures, such as tables and 
-- indexes, is physically stored in the data files.
-- 
-- Control files
-- 
-- Every Oracle database has a control file. A control file contains metadata 
-- specifying the physical structure of the database, including the database name 
-- and the names and locations of the database files.
-- 
-- Online redo log files
-- 
-- Every Oracle Database has an online redo log, which is a set of two or more 
-- online redo log files. An online redo log is made up of redo entries (also 
-- called redo log records), which record all changes made to data.
-- 
-- Many other files are important for the functioning of an Oracle database 
-- server. These include parameter files and networking files. Backup files and 
-- archived redo log files are offline files important for backup and recovery.
-- 
-- See Also:
-- 
-- "Physical Storage Structures"
-- 
-- Logical Storage Structures
-- Logical storage structures enable Oracle Database to have fine-grained control 
-- of disk space use.
-- 
-- This topic discusses logical storage structures:
-- 
-- Data blocks
-- 
-- At the finest level of granularity, Oracle Database data is stored in data 
-- blocks. One data block corresponds to a specific number of bytes on disk.
-- 
-- Extents
-- 
-- An extent is a specific number of logically contiguous data blocks, obtained in 
-- a single allocation, used to store a specific type of information.
-- 
-- Segments
-- 
-- A segment is a set of extents allocated for a user object (for example, a table 
-- or index), undo data, or temporary data.
-- 
-- Tablespaces
-- 
-- A database is divided into logical storage units called tablespaces. A 
-- tablespace is the logical container for segments. Each tablespace consists of 
-- at least one data file.
-- 
-- See Also:
-- 
-- "Logical Storage Structures"
-- 
-- Database Instance Structures
-- An Oracle database uses memory structures and processes to manage and access 
-- the database. All memory structures exist in the main memory of the computers 
-- that constitute the RDBE.
-- 
-- When applications connect to an Oracle database, they connect to a database 
-- instance. The instance services applications by allocating other memory areas 
-- in addition to the SGA, and starting other processes in addition to background 
-- processes.
-- 
-- Oracle Database Processes
-- A process is a mechanism in an operating system that can run a series of steps. 
-- Some operating systems use the terms job, task, or thread.
-- 
-- For the purposes of this topic, a thread is equivalent to a process. An Oracle 
-- database instance has the following types of processes:
-- 
-- Client processes
-- 
-- These processes are created and maintained to run the software code of an 
-- application program or an Oracle tool. Most environments have separate 
-- computers for client processes.
-- 
-- Background processes
-- 
-- These processes consolidate functions that would otherwise be handled by 
-- multiple Oracle Database programs running for each client process. Background 
-- processes asynchronously perform I/O and monitor other Oracle Database 
-- processes to provide increased parallelism for better performance and 
-- reliability.
-- 
-- Server processes
-- 
-- These processes communicate with client processes and interact with Oracle 
-- Database to fulfill requests.
-- 
-- Oracle processes include server processes and background processes. In most 
-- environments, Oracle processes and client processes run on separate computers.
-- 
-- See Also:
-- 
-- "Process Architecture"
-- 
-- Instance Memory Structures
-- Oracle Database creates and uses memory structures for program code, data 
-- shared among users, and private data areas for each connected user.
-- 
-- The following memory structures are associated with a database instance:
-- 
-- System Global Area (SGA)
-- 
-- The SGA is a group of shared memory structures that contain data and control 
-- information for one database instance. Examples of SGA components include the 
-- database buffer cache and shared SQL areas. Starting in Oracle Database 12c 
-- Release 1 (12.1.0.2), the SGA can contain an optional In-Memory Column Store 
-- (IM column store), which enables data to be populated in memory in a columnar 
-- format.
-- 
-- Program Global Areas (PGA)
-- 
-- A PGA is a memory region that contains data and control information for a 
-- server or background process. Access to the PGA is exclusive to the process. 
-- Each server process and background process has its own PGA.
-- 
-- See Also:
-- 
-- "Memory Architecture"
-- 
-- Application and Networking Architecture
-- To take full advantage of a given computer system or network, Oracle Database 
-- enables processing to be split between the database server and the client 
-- programs. The computer running the RDBE handles the database server 
-- responsibilities while the computers running the applications handle the 
-- interpretation and display of data.
-- 
-- Application Architecture
-- The application architecture is the computing environment in which a database 
-- application connects to an Oracle database. The two most common database 
-- architectures are client/server and multitier.
-- 
-- In a client/server architecture, the client application initiates a request for 
-- an operation to be performed on the database server.
-- 
-- The server runs Oracle Database software and handles the functions required for 
-- concurrent, shared data access. The server receives and processes requests that 
-- originate from clients.
-- 
-- In a traditional multitier architecture, one or more application servers 
-- perform parts of the operation.
-- 
-- An application server contains a large part of the application logic, provides 
-- access to the data for the client, and performs some query processing. In this 
-- way, the load on the database decreases. The application server can serve as an 
-- interface between clients and multiple databases and provide an additional 
-- level of security.
-- 
-- A service-oriented architecture (SOA) is a multitier architecture in which 
-- application functionality is encapsulated in services. SOA services are usually 
-- implemented as Web services. Web services are accessible through HTTP and are 
-- based on XML-based standards such as Web Services Description Language (WSDL) 
-- and SOAP.
-- 
-- Oracle Database can act as a Web service provider in a traditional multitier or 
-- SOA environment.
-- 
-- See Also:
-- 
-- "Overview of Multitier Architecture"
-- 
-- Oracle XML DB Developer’s Guide for more information about using Web services 
-- with the database
-- 
-- Oracle Net Services Architecture
-- Oracle Net Services is the interface between the database and the network 
-- communication protocols that facilitate distributed processing and distributed 
-- databases.
-- 
-- Communication protocols define the way that data is transmitted and received on 
-- a network. Oracle Net Services supports communications on all major network 
-- protocols, including TCP/IP, HTTP, FTP, and WebDAV.
-- 
-- Oracle Net, a component of Oracle Net Services, establishes and maintains a 
-- network session from a client application to a database server. After a network 
-- session is established, Oracle Net acts as the data courier for both the client 
-- application and the database server, exchanging messages between them. Oracle 
-- Net can perform these jobs because it is located on each computer in the network.
-- 
-- An important component of Net Services is the Oracle Net Listener (called the 
-- listener), which is a process that runs on the database or elsewhere in the 
-- network. Client applications send connection requests to the listener, which 
-- manages the traffic of these requests to the database. When a connection is 
-- established, the client and database communicate directly.
-- 
-- The most common ways to configure an Oracle database to service client requests 
-- are:
-- 
-- Dedicated server architecture
-- 
-- Each client process connects to a dedicated server process. The server process 
-- is not shared by any other client for the duration of the client's session. 
-- Each new session is assigned a dedicated server process.
-- 
-- Shared server architecture
-- 
-- The database uses a pool of shared server processes for multiple sessions. A 
-- client process communicates with a dispatcher, which is a process that enables 
-- many clients to connect to the same database instance without the need for a 
-- dedicated server process for each client.
-- 
-- See Also:
-- 
-- "Overview of Oracle Net Services Architecture"
-- 
-- Oracle Database Net Services Administrator's Guide to learn more about Oracle 
-- Net architecture
-- 
-- Oracle XML DB Developer’s Guide for information about using WebDAV with the 
-- database
-- 
-- Oracle Database Documentation Roadmap
-- The documentation set is designed with specific access paths to ensure that 
-- users are able to find the information they need as efficiently as possible.
-- 
-- The documentation set is divided into three layers or groups: basic, 
-- intermediate, and advanced. Users begin with the manuals in the basic group, 
-- proceed to the manuals in the intermediate group (the 2 Day + series), and 
-- finally to the advanced manuals, which include the remainder of the 
-- documentation.
-- 
-- Oracle Database Documentation: Basic Group
-- Technical users who are new to Oracle Database begin by reading one or more 
-- manuals in the basic group from cover to cover. Each manual in this group is 
-- designed to be read in two days.
-- 
-- In addition to this manual, the basic group includes the manuals shown in the 
-- following table.

--------------------------------------------------------------------------------
-- Relational Model
-- In his seminal 1970 paper "A Relational Model of Data for Large Shared Data Banks,"
-- E. F. Codd defined a relational model based on mathematical set theory. 
-- Today, the most widely accepted database model is the relational model.
-- A relational database is a database that conforms to the relational model. 
-- The relational model has the following major aspects:
-- Structures
-- Well-defined objects store or access the data of a database.
-- Operations
-- Clearly defined actions enable applications to manipulate the data and
-- structures of a database.
-- Integrity rules
-- Integrity rules govern operations on the data and structures of a database.
-- A relational database stores data in a set of simple relations. A relation 
-- is a set of tuples. A tuple is an unordered set of attribute values.
-- A table is a two-dimensional representation of a relation in the form of 
-- rows (tuples) and columns (attributes). Each row in a table has the same set of 
-- columns. A relational database is a database that stores data in 
-- relations (tables). For example, a relational database could store information 
-- about company employees in an employee table, a department table, 
-- and a salary table.
-- See Also:
-- "A Relational Model of Data for Large Shared Data Banks" for an abstract and 
-- link to Codd's paper
-- Relational Database Management System (RDBE)
-- The relational model is the basis for a relational database management 
-- system (RDBE). An RDBE moves data into a database, stores the data, 
-- and retrieves it so that applications can manipulate it.
-- An RDBE distinguishes between the following types of operations:
-- Logical operations
-- In this case, an application specifies what content is required. For example, 
-- an application requests an employee name or adds an employee record to a table.
-- Physical operations
-- In this case, the RDBE determines how things should be done and carries out 
-- the operation. For example, after an application queries a table, the database 
-- may use an index to find the requested rows, read the data into memory, and 
-- perform many other steps before returning a result to the user. The RDBE 
-- stores and retrieves data so that physical operations are transparent 
-- to database applications.
-- Oracle Database is an RDBE. An RDBE that implements object-oriented features 
-- such as user-defined types, inheritance, and polymorphism is called an 
-- object-relational database management system (ORDBE). Oracle Database has 
-- extended the relational model to an object-relational model, making it 
-- possible to store complex business models in a relational database.
-- Brief History of Oracle Database
-- The current version of Oracle Database is the result of over 35 years of 
-- innovative development.
-- Highlights in the evolution of Oracle Database include the following:
-- Founding of Oracle
-- In 1977, Larry Ellison, Bob Miner, and Ed Oates started the consultancy 
-- Software Development Laboratories, which became Relational Software, Inc. (RSI). 
-- In 1983, RSI became Oracle Systems Corporation and then later Oracle Corporation.
-- First commercially available RDBE
-- In 1979, RSI introduced Oracle V2 (Version 2) as the first commercially 
-- available SQL-based RDBE, a landmark event in the history of relational 
-- databases.
-- Portable version of Oracle Database
-- Oracle Version 3, released in 1983, was the first relational database to run on 
-- mainframes, minicomputers, and PCs. The database was written in C, enabling the 
-- database to be ported to multiple platforms.
-- Enhancements to concurrency control, data distribution, and scalability
-- Version 4 introduced multiversion read consistency. Version 5, released in 1985, 
-- supported client/server computing and distributed database systems. Version 6 
-- brought enhancements to disk I/O, row locking, scalability, and backup and
-- recovery. 
-- Also, Version 6 introduced the first version of the PL/SQL language, a 
-- proprietary procedural extension to SQL.
-- PL/SQL stored program units
-- Oracle7, released in 1992, introduced PL/SQL stored procedures and triggers.
-- Objects and partitioning
-- Oracle8 was released in 1997 as the object-relational database, supporting many 
-- new data types. Additionally, Oracle8 supported partitioning of large tables.
-- Internet computing
-- Oracle8i Database, released in 1999, provided native support for internet 
-- protocols and server-side support for Java. Oracle8i was designed for internet 
-- computing, enabling the database to be deployed in a multitier environment.
-- Oracle Real Application Clusters (Oracle RAC)
-- Oracle9i Database introduced Oracle RAC in 2001, enabling multiple instances to 
-- access a single database simultaneously. Additionally, Oracle XML Database 
-- (Oracle XML DB) introduced the ability to store and query XML.
-- Grid computing
-- Oracle Database 10g introduced grid computing in 2003. This release enabled 
-- organizations to virtualize computing resources by building a grid
-- infrastructure 
-- based on low-cost commodity servers. A key goal was to make the database 
-- self-managing and self-tuning. Oracle Automatic Storage Management (Oracle ASM) 
-- helped achieve this goal by virtualizing and simplifying database storage 
-- management.
-- Manageability, diagnosability, and availability
-- Oracle Database 11g, released in 2007, introduced a host of new features that 
-- enabled administrators and developers to adapt quickly to changing business 
-- requirements. The key to adaptability is simplifying the information 
-- infrastructure by consolidating information and using automation wherever 
-- possible.
-- Plugging In to the Cloud
-- Oracle Database 12c, released in 2013, was designed for the Cloud, featuring 
-- a new Multitenant architecture, In-Memory column store, and support for JSON 
-- documents. Oracle Database 12c helps customers make more efficient use of their 
-- IT resources, while continuing to reduce costs and improve service levels for 
-- users.
-- Schema Objects
-- One characteristic of an RDBE is the independence of physical data storage from 
-- logical data structures.
-- In Oracle Database, a database schema is a collection of logical data 
-- structures, 
-- or schema objects. A database user owns a database schema, which has the same 
-- name as the user name.
-- Schema objects are user-created structures that directly refer to the data in 
-- the database. The database supports many types of schema objects, the most 
-- important of which are tables and indexes.
-- A schema object is one type of database object. Some database objects, such as 
-- profiles and roles, do not reside in schemas.
-- See Also:
-- "Introduction to Schema Objects" to learn more about schema object types, 
-- storage, and dependencies
-- Tables
-- A table describes an entity such as employees.
-- You define a table with a table name, such as employees, and set of columns. In 
-- general, you give each column a name, a data type, and a width when you create 
-- the table.
-- A table is a set of rows. A column identifies an attribute of the entity 
-- described by the table, whereas a row identifies an instance of the entity. For 
-- example, attributes of the employees entity correspond to columns for employee 
-- ID and last name. A row identifies a specific employee.
-- You can optionally specify a rule, called an integrity constraint, for a 
-- column. One example is a NOT NULL integrity constraint. This constraint forces 
-- the column to contain a value in every row.
-- See Also:
-- "Overview of Tables" to learn about columns and rows, data types, table 
-- storage, and table compression
-- "Data Integrity" to learn about the possible types and states of constraints
-- Indexes
-- An index is an optional data structure that you can create on one or more 
-- columns of a table. Indexes can increase the performance of data retrieval.
-- When processing a request, the database can use available indexes to locate the 
-- requested rows efficiently. Indexes are useful when applications often query a 
-- specific row or range of rows.
-- Indexes are logically and physically independent of the data. Thus, you can 
-- drop and create indexes with no effect on the tables or other indexes. All 
-- applications continue to function after you drop an index.
-- See Also:
-- "Introduction to Indexes" to learn about the purpose and types of indexes
-- Data Access
-- A general requirement for a DBE is to adhere to accepted industry standards 
-- for a data access language.
-- Structured Query Language (SQL)
-- SQL is a set-based declarative language that provides an interface to an RDBE 
-- such as Oracle Database.
-- Procedural languages such as C describe how things should be done. SQL is 
-- nonprocedural and describes what should be done.
-- SQL is the ANSI standard language for relational databases. All operations on 
-- the data in an Oracle database are performed using SQL statements. For example, 
-- you use SQL to create tables and query and modify data in tables.
-- A SQL statement can be thought of as a very simple, but powerful, computer 
-- program or instruction. Users specify the result that they want (for example, 
-- the names of employees), not how to derive it. A SQL statement is a string of 
-- SQL text such as the following:
-- SELECT first_name, last_name FROM employees;
-- SQL statements enable you to perform the following tasks:
-- Query data
-- Insert, update, and delete rows in a table
-- Create, replace, alter, and drop objects
-- Control access to the database and its objects
-- Guarantee database consistency and integrity
-- SQL unifies the preceding tasks in one consistent language. Oracle SQL is an 
-- implementation of the ANSI standard. Oracle SQL supports numerous features that 
-- extend beyond standard SQL.
-- See Also:
-- "SQL" to learn more about SQL standards and the main types of SQL statements
-- PL/SQL and Java
-- PL/SQL is a procedural extension to Oracle SQL.
-- PL/SQL is integrated with Oracle Database, enabling you to use all of the 
-- Oracle Database SQL statements, functions, and data types. You can use PL/SQL 
-- to control the flow of a SQL program, use variables, and write error-handling 
-- procedures.
-- A primary benefit of PL/SQL is the ability to store application logic in the 
-- database itself. A PL/SQL procedure or function is a schema object that 
-- consists of a set of SQL statements and other PL/SQL constructs, grouped 
-- together, stored in the database, and run as a unit to solve a specific problem 
-- or to perform a set of related tasks. The principal benefit of server-side 
-- programming is that built-in functionality can be deployed anywhere.
-- Oracle Database can also store program units written in Java. A Java stored 
-- procedure is a Java method published to SQL and stored in the database for 
-- general use. You can call existing PL/SQL programs from Java and Java programs 
-- from PL/SQL.
-- See Also:
-- "Server-Side Programming: PL/SQL and Java"
-- "Client-Side Database Programming"
-- Transaction Management
-- Oracle Database is designed as a multiuser database. The database must ensure 
-- that multiple users can work concurrently without corrupting one another's data.
-- Transactions
-- A transaction is a logical, atomic unit of work that contains one or more SQL 
-- statements.
-- An RDBE must be able to group SQL statements so that they are either all 
-- committed, which means they are applied to the database, or all rolled back, 
-- which means they are undone.
-- An illustration of the need for transactions is a funds transfer from a savings 
-- account to a checking account. The transfer consists of the following separate 
-- operations:
-- Decrease the savings account.
-- Increase the checking account.
-- Record the transaction in the transaction journal.
-- Oracle Database guarantees that all three operations succeed or fail as a unit. 
-- For example, if a hardware failure prevents a statement in the transaction from 
-- executing, then the other statements must be rolled back.
-- 
-- Transactions are one feature that set Oracle Database apart from a file system. 
-- If you perform an atomic operation that updates several files, and if the 
-- system fails halfway through, then the files will not be consistent. In 
-- contrast, a transaction moves an Oracle database from one consistent state to 
-- another. The basic principle of a transaction is "all or nothing": an atomic 
-- operation succeeds or fails as a whole.
-- See Also:
-- "Transactions" to learn about the definition of a transaction, statement-level 
-- atomicity, and transaction control
-- Data Concurrency
-- A requirement of a multiuser RDBE is the control of data concurrency, which is 
-- the simultaneous access of the same data by multiple users.
-- Without concurrency controls, users could change data improperly, compromising 
-- data integrity. For example, one user could update a row while a different user 
-- simultaneously updates it.
-- If multiple users access the same data, then one way of managing concurrency is 
-- to make users wait. However, the goal of a DBE is to reduce wait time so it is 
-- either nonexistent or negligible. All SQL statements that modify data must 
-- proceed with as little interference as possible. Destructive interactions, 
-- which are interactions that incorrectly update data or alter underlying data 
-- structures, must be avoided.
-- Oracle Database uses locks to control concurrent access to data. A lock is a 
-- mechanism that prevents destructive interaction between transactions accessing 
-- a shared resource. Locks help ensure data integrity while allowing maximum 
-- concurrent access to data.
-- See Also:
-- "Overview of the Oracle Database Locking Mechanism"
-- Data Consistency
-- In Oracle Database, each user must see a consistent view of the data, including 
-- visible changes made by a user's own transactions and committed transactions of 
-- other users.
-- For example, the database must prevent the lost update problem, which occurs 
-- when one transaction sees uncommitted changes made by another concurrent 
-- transaction.
-- Oracle Database always enforces statement-level read consistency, which 
-- guarantees that the data that a single query returns is committed and 
-- consistent for a single point in time. Depending on the transaction isolation 
-- level, this point is the time at which the statement was opened or the time the 
-- transaction began. The Oracle Flashback Query feature enables you to specify 
-- this point in time explicitly.
-- The database can also provide read consistency to all queries in a transaction, 
-- known as transaction-level read consistency. In this case, each statement in a 
-- transaction sees data from the same point in time, which is the time at which 
-- the transaction began.
-- See Also:
-- "Data Concurrency and Consistency" to learn more about lost updates
-- Oracle Database Development Guide to learn about Oracle Flashback Query
-- 
-- /* Per thread arena example. */
-- #include <stdio.h>
-- #include <stdlib.h>
-- #include <pthread.h>
-- #include <unistd.h>
-- #include <sys/types.h>
-- 
-- void* threadFunc(void* arg) {
--         printf("Before malloc in thread 1\n");
--         getchar();
--         char* addr = (char*) malloc(1000);
--         printf("After malloc and before free in thread 1\n");
--         getchar();
--         free(addr);
--         printf("After free in thread 1\n");
--         getchar();
-- }
-- 
-- int main() {
--         pthread_t t1;
--         void* s;
--         int ret;
--         char* addr;
-- 
--         printf("Welcome to per thread arena example::%d\n",getpid());
--         printf("Before malloc in main thread\n");
--         getchar();
--         addr = (char*) malloc(1000);
--         printf("After malloc and before free in main thread\n");
--         getchar();
--         free(addr);
--         printf("After free in main thread\n");
--         getchar();
--         ret = pthread_create(&t1, NULL, threadFunc, NULL);
--         if(ret)
--         {
--                 printf("Thread creation error\n");
--                 return -1;
--         }
--         ret = pthread_join(t1, &s);
--         if(ret)
--         {
--                 printf("Thread join error\n");
--                 return -1;
--         }
--         return 0;
-- }
-- 
-- Database Storage Structures
-- A database can be considered from both a physical and logical perspective.
-- 
-- Physical data is data viewable at the operating system level. For example, 
-- operating system utilities such as the Linux ls and ps can list database files 
-- and processes. Logical data such as a table is meaningful only for the 
-- database. A SQL statement can list the tables in an Oracle database, but an 
-- operating system utility cannot.
-- 
-- The database has physical structures and logical structures. Because the 
-- physical and logical structures are separate, you can manage the physical 
-- storage of data without affecting access to logical storage structures. For 
-- example, renaming a physical database file does not rename the tables whose 
-- data is stored in this file.
-- 
-- Physical Storage Structures
-- The physical database structures are the files that store the data.
-- 
-- When you execute a CREATE DATABASE statement, the following files are created:
-- 
-- Data files
-- 
-- Every Oracle database has one or more physical data files, which contain all 
-- the database data. The data of logical database structures, such as tables and 
-- indexes, is physically stored in the data files.
-- 
-- Control files
-- 
-- Every Oracle database has a control file. A control file contains metadata 
-- specifying the physical structure of the database, including the database name 
-- and the names and locations of the database files.
-- 
-- Online redo log files
-- 
-- Every Oracle Database has an online redo log, which is a set of two or more 
-- online redo log files. An online redo log is made up of redo entries (also 
-- called redo log records), which record all changes made to data.
-- 
-- Many other files are important for the functioning of an Oracle database 
-- server. These include parameter files and networking files. Backup files and 
-- archived redo log files are offline files important for backup and recovery.
-- 
-- See Also:
-- 
-- "Physical Storage Structures"
-- 
-- Logical Storage Structures
-- Logical storage structures enable Oracle Database to have fine-grained control 
-- of disk space use.
-- 
-- This topic discusses logical storage structures:
-- 
-- Data blocks
-- 
-- At the finest level of granularity, Oracle Database data is stored in data 
-- blocks. One data block corresponds to a specific number of bytes on disk.
-- 
-- Extents
-- 
-- An extent is a specific number of logically contiguous data blocks, obtained in 
-- a single allocation, used to store a specific type of information.
-- 
-- Segments
-- 
-- A segment is a set of extents allocated for a user object (for example, a table 
-- or index), undo data, or temporary data.
-- 
-- Tablespaces
-- 
-- A database is divided into logical storage units called tablespaces. A 
-- tablespace is the logical container for segments. Each tablespace consists of 
-- at least one data file.
-- 
-- See Also:
-- 
-- "Logical Storage Structures"
-- 
-- Database Instance Structures
-- An Oracle database uses memory structures and processes to manage and access 
-- the database. All memory structures exist in the main memory of the computers 
-- that constitute the RDBE.
-- 
-- When applications connect to an Oracle database, they connect to a database 
-- instance. The instance services applications by allocating other memory areas 
-- in addition to the SGA, and starting other processes in addition to background 
-- processes.
-- 
-- Oracle Database Processes
-- A process is a mechanism in an operating system that can run a series of steps. 
-- Some operating systems use the terms job, task, or thread.
-- 
-- For the purposes of this topic, a thread is equivalent to a process. An Oracle 
-- database instance has the following types of processes:
-- 
-- Client processes
-- 
-- These processes are created and maintained to run the software code of an 
-- application program or an Oracle tool. Most environments have separate 
-- computers for client processes.
-- 
-- Background processes
-- 
-- These processes consolidate functions that would otherwise be handled by 
-- multiple Oracle Database programs running for each client process. Background 
-- processes asynchronously perform I/O and monitor other Oracle Database 
-- processes to provide increased parallelism for better performance and 
-- reliability.
-- 
-- Server processes
-- 
-- These processes communicate with client processes and interact with Oracle 
-- Database to fulfill requests.
-- 
-- Oracle processes include server processes and background processes. In most 
-- environments, Oracle processes and client processes run on separate computers.
-- 
-- See Also:
-- 
-- "Process Architecture"
-- 
-- Instance Memory Structures
-- Oracle Database creates and uses memory structures for program code, data 
-- shared among users, and private data areas for each connected user.
-- 
-- The following memory structures are associated with a database instance:
-- 
-- System Global Area (SGA)
-- 
-- The SGA is a group of shared memory structures that contain data and control 
-- information for one database instance. Examples of SGA components include the 
-- database buffer cache and shared SQL areas. Starting in Oracle Database 12c 
-- Release 1 (12.1.0.2), the SGA can contain an optional In-Memory Column Store 
-- (IM column store), which enables data to be populated in memory in a columnar 
-- format.
-- 
-- Program Global Areas (PGA)
-- 
-- A PGA is a memory region that contains data and control information for a 
-- server or background process. Access to the PGA is exclusive to the process. 
-- Each server process and background process has its own PGA.
-- 
-- See Also:
-- 
-- "Memory Architecture"
-- 
-- Application and Networking Architecture
-- To take full advantage of a given computer system or network, Oracle Database 
-- enables processing to be split between the database server and the client 
-- programs. The computer running the RDBE handles the database server 
-- responsibilities while the computers running the applications handle the 
-- interpretation and display of data.
-- 
-- Application Architecture
-- The application architecture is the computing environment in which a database 
-- application connects to an Oracle database. The two most common database 
-- architectures are client/server and multitier.
-- 
-- In a client/server architecture, the client application initiates a request for 
-- an operation to be performed on the database server.
-- 
-- The server runs Oracle Database software and handles the functions required for 
-- concurrent, shared data access. The server receives and processes requests that 
-- originate from clients.
-- 
-- In a traditional multitier architecture, one or more application servers 
-- perform parts of the operation.
-- 
-- An application server contains a large part of the application logic, provides 
-- access to the data for the client, and performs some query processing. In this 
-- way, the load on the database decreases. The application server can serve as an 
-- interface between clients and multiple databases and provide an additional 
-- level of security.
-- 
-- A service-oriented architecture (SOA) is a multitier architecture in which 
-- application functionality is encapsulated in services. SOA services are usually 
-- implemented as Web services. Web services are accessible through HTTP and are 
-- based on XML-based standards such as Web Services Description Language (WSDL) 
-- and SOAP.
-- 
-- Oracle Database can act as a Web service provider in a traditional multitier or 
-- SOA environment.
-- 
-- See Also:
-- 
-- "Overview of Multitier Architecture"
-- 
-- Oracle XML DB Developer’s Guide for more information about using Web services 
-- with the database
-- 
-- Oracle Net Services Architecture
-- Oracle Net Services is the interface between the database and the network 
-- communication protocols that facilitate distributed processing and distributed 
-- databases.
-- 
-- Communication protocols define the way that data is transmitted and received on 
-- a network. Oracle Net Services supports communications on all major network 
-- protocols, including TCP/IP, HTTP, FTP, and WebDAV.
-- 
-- Oracle Net, a component of Oracle Net Services, establishes and maintains a 
-- network session from a client application to a database server. After a network 
-- session is established, Oracle Net acts as the data courier for both the client 
-- application and the database server, exchanging messages between them. Oracle 
-- Net can perform these jobs because it is located on each computer in the network.
-- 
-- An important component of Net Services is the Oracle Net Listener (called the 
-- listener), which is a process that runs on the database or elsewhere in the 
-- network. Client applications send connection requests to the listener, which 
-- manages the traffic of these requests to the database. When a connection is 
-- established, the client and database communicate directly.
-- 
-- The most common ways to configure an Oracle database to service client requests 
-- are:
-- 
-- Dedicated server architecture
-- 
-- Each client process connects to a dedicated server process. The server process 
-- is not shared by any other client for the duration of the client's session. 
-- Each new session is assigned a dedicated server process.
-- 
-- Shared server architecture
-- 
-- The database uses a pool of shared server processes for multiple sessions. A 
-- client process communicates with a dispatcher, which is a process that enables 
-- many clients to connect to the same database instance without the need for a 
-- dedicated server process for each client.
-- 
-- See Also:
-- 
-- "Overview of Oracle Net Services Architecture"
-- 
-- Oracle Database Net Services Administrator's Guide to learn more about Oracle 
-- Net architecture
-- 
-- Oracle XML DB Developer’s Guide for information about using WebDAV with the 
-- database
-- 
-- Oracle Database Documentation Roadmap
-- The documentation set is designed with specific access paths to ensure that 
-- users are able to find the information they need as efficiently as possible.
-- 
-- The documentation set is divided into three layers or groups: basic, 
-- intermediate, and advanced. Users begin with the manuals in the basic group, 
-- proceed to the manuals in the intermediate group (the 2 Day + series), and 
-- finally to the advanced manuals, which include the remainder of the 
-- documentation.
-- 
-- Oracle Database Documentation: Basic Group
-- Technical users who are new to Oracle Database begin by reading one or more 
-- manuals in the basic group from cover to cover. Each manual in this group is 
-- designed to be read in two days.
-- 
-- In addition to this manual, the basic group includes the manuals shown in the 
-- following table.


--------------------------------------------------------------------------------
   v_num := v_num + 1;
-- Relational Model
-- In his seminal 1970 paper "A Relational Model of Data for Large Shared Data Banks,"
-- E. F. Codd defined a relational model based on mathematical set theory. 
-- Today, the most widely accepted database model is the relational model.
-- A relational database is a database that conforms to the relational model. 
-- The relational model has the following major aspects:
-- Structures
-- Well-defined objects store or access the data of a database.
-- Operations
-- Clearly defined actions enable applications to manipulate the data and
-- structures of a database.
-- Integrity rules
-- Integrity rules govern operations on the data and structures of a database.
-- A relational database stores data in a set of simple relations. A relation 
-- is a set of tuples. A tuple is an unordered set of attribute values.
-- A table is a two-dimensional representation of a relation in the form of 
-- rows (tuples) and columns (attributes). Each row in a table has the same set of 
-- columns. A relational database is a database that stores data in 
-- relations (tables). For example, a relational database could store information 
-- about company employees in an employee table, a department table, 
-- and a salary table.
-- See Also:
-- "A Relational Model of Data for Large Shared Data Banks" for an abstract and 
-- link to Codd's paper
-- Relational Database Management System (RDBE)
-- The relational model is the basis for a relational database management 
-- system (RDBE). An RDBE moves data into a database, stores the data, 
-- and retrieves it so that applications can manipulate it.
-- An RDBE distinguishes between the following types of operations:
-- Logical operations
-- In this case, an application specifies what content is required. For example, 
-- an application requests an employee name or adds an employee record to a table.
-- Physical operations
-- In this case, the RDBE determines how things should be done and carries out 
-- the operation. For example, after an application queries a table, the database 
-- may use an index to find the requested rows, read the data into memory, and 
-- perform many other steps before returning a result to the user. The RDBE 
-- stores and retrieves data so that physical operations are transparent 
-- to database applications.
-- Oracle Database is an RDBE. An RDBE that implements object-oriented features 
-- such as user-defined types, inheritance, and polymorphism is called an 
-- object-relational database management system (ORDBE). Oracle Database has 
-- extended the relational model to an object-relational model, making it 
-- possible to store complex business models in a relational database.
-- Brief History of Oracle Database
-- The current version of Oracle Database is the result of over 35 years of 
-- innovative development.
-- Highlights in the evolution of Oracle Database include the following:
-- Founding of Oracle
-- In 1977, Larry Ellison, Bob Miner, and Ed Oates started the consultancy 
-- Software Development Laboratories, which became Relational Software, Inc. (RSI). 
-- In 1983, RSI became Oracle Systems Corporation and then later Oracle Corporation.
-- First commercially available RDBE
-- In 1979, RSI introduced Oracle V2 (Version 2) as the first commercially 
-- available SQL-based RDBE, a landmark event in the history of relational 
-- databases.
-- Portable version of Oracle Database
-- Oracle Version 3, released in 1983, was the first relational database to run on 
-- mainframes, minicomputers, and PCs. The database was written in C, enabling the 
-- database to be ported to multiple platforms.
-- Enhancements to concurrency control, data distribution, and scalability
-- Version 4 introduced multiversion read consistency. Version 5, released in 1985, 
-- supported client/server computing and distributed database systems. Version 6 
-- brought enhancements to disk I/O, row locking, scalability, and backup and
-- recovery. 
-- Also, Version 6 introduced the first version of the PL/SQL language, a 
-- proprietary procedural extension to SQL.
-- PL/SQL stored program units
-- Oracle7, released in 1992, introduced PL/SQL stored procedures and triggers.
-- Objects and partitioning
-- Oracle8 was released in 1997 as the object-relational database, supporting many 
-- new data types. Additionally, Oracle8 supported partitioning of large tables.
-- Internet computing
-- Oracle8i Database, released in 1999, provided native support for internet 
-- protocols and server-side support for Java. Oracle8i was designed for internet 
-- computing, enabling the database to be deployed in a multitier environment.
-- Oracle Real Application Clusters (Oracle RAC)
-- Oracle9i Database introduced Oracle RAC in 2001, enabling multiple instances to 
-- access a single database simultaneously. Additionally, Oracle XML Database 
-- (Oracle XML DB) introduced the ability to store and query XML.
-- Grid computing
-- Oracle Database 10g introduced grid computing in 2003. This release enabled 
-- organizations to virtualize computing resources by building a grid
-- infrastructure 
-- based on low-cost commodity servers. A key goal was to make the database 
-- self-managing and self-tuning. Oracle Automatic Storage Management (Oracle ASM) 
-- helped achieve this goal by virtualizing and simplifying database storage 
-- management.
-- Manageability, diagnosability, and availability
-- Oracle Database 11g, released in 2007, introduced a host of new features that 
-- enabled administrators and developers to adapt quickly to changing business 
-- requirements. The key to adaptability is simplifying the information 
-- infrastructure by consolidating information and using automation wherever 
-- possible.
-- Plugging In to the Cloud
-- Oracle Database 12c, released in 2013, was designed for the Cloud, featuring 
-- a new Multitenant architecture, In-Memory column store, and support for JSON 
-- documents. Oracle Database 12c helps customers make more efficient use of their 
-- IT resources, while continuing to reduce costs and improve service levels for 
-- users.
-- Schema Objects
-- One characteristic of an RDBE is the independence of physical data storage from 
-- logical data structures.
-- In Oracle Database, a database schema is a collection of logical data 
-- structures, 
-- or schema objects. A database user owns a database schema, which has the same 
-- name as the user name.
-- Schema objects are user-created structures that directly refer to the data in 
-- the database. The database supports many types of schema objects, the most 
-- important of which are tables and indexes.
-- A schema object is one type of database object. Some database objects, such as 
-- profiles and roles, do not reside in schemas.
-- See Also:
-- "Introduction to Schema Objects" to learn more about schema object types, 
-- storage, and dependencies
-- Tables
-- A table describes an entity such as employees.
-- You define a table with a table name, such as employees, and set of columns. In 
-- general, you give each column a name, a data type, and a width when you create 
-- the table.
-- A table is a set of rows. A column identifies an attribute of the entity 
-- described by the table, whereas a row identifies an instance of the entity. For 
-- example, attributes of the employees entity correspond to columns for employee 
-- ID and last name. A row identifies a specific employee.
-- You can optionally specify a rule, called an integrity constraint, for a 
-- column. One example is a NOT NULL integrity constraint. This constraint forces 
-- the column to contain a value in every row.
-- See Also:
-- "Overview of Tables" to learn about columns and rows, data types, table 
-- storage, and table compression
-- "Data Integrity" to learn about the possible types and states of constraints
-- Indexes
-- An index is an optional data structure that you can create on one or more 
-- columns of a table. Indexes can increase the performance of data retrieval.
-- When processing a request, the database can use available indexes to locate the 
-- requested rows efficiently. Indexes are useful when applications often query a 
-- specific row or range of rows.
-- Indexes are logically and physically independent of the data. Thus, you can 
-- drop and create indexes with no effect on the tables or other indexes. All 
-- applications continue to function after you drop an index.
-- See Also:
-- "Introduction to Indexes" to learn about the purpose and types of indexes
-- Data Access
-- A general requirement for a DBE is to adhere to accepted industry standards 
-- for a data access language.
-- Structured Query Language (SQL)
-- SQL is a set-based declarative language that provides an interface to an RDBE 
-- such as Oracle Database.
-- Procedural languages such as C describe how things should be done. SQL is 
-- nonprocedural and describes what should be done.
-- SQL is the ANSI standard language for relational databases. All operations on 
-- the data in an Oracle database are performed using SQL statements. For example, 
-- you use SQL to create tables and query and modify data in tables.
-- A SQL statement can be thought of as a very simple, but powerful, computer 
-- program or instruction. Users specify the result that they want (for example, 
-- the names of employees), not how to derive it. A SQL statement is a string of 
-- SQL text such as the following:
-- SELECT first_name, last_name FROM employees;
-- SQL statements enable you to perform the following tasks:
-- Query data
-- Insert, update, and delete rows in a table
-- Create, replace, alter, and drop objects
-- Control access to the database and its objects
-- Guarantee database consistency and integrity
-- SQL unifies the preceding tasks in one consistent language. Oracle SQL is an 
-- implementation of the ANSI standard. Oracle SQL supports numerous features that 
-- extend beyond standard SQL.
-- See Also:
-- "SQL" to learn more about SQL standards and the main types of SQL statements
-- PL/SQL and Java
-- PL/SQL is a procedural extension to Oracle SQL.
-- PL/SQL is integrated with Oracle Database, enabling you to use all of the 
-- Oracle Database SQL statements, functions, and data types. You can use PL/SQL 
-- to control the flow of a SQL program, use variables, and write error-handling 
-- procedures.
-- A primary benefit of PL/SQL is the ability to store application logic in the 
-- database itself. A PL/SQL procedure or function is a schema object that 
-- consists of a set of SQL statements and other PL/SQL constructs, grouped 
-- together, stored in the database, and run as a unit to solve a specific problem 
-- or to perform a set of related tasks. The principal benefit of server-side 
-- programming is that built-in functionality can be deployed anywhere.
-- Oracle Database can also store program units written in Java. A Java stored 
-- procedure is a Java method published to SQL and stored in the database for 
-- general use. You can call existing PL/SQL programs from Java and Java programs 
-- from PL/SQL.
-- See Also:
-- "Server-Side Programming: PL/SQL and Java"
-- "Client-Side Database Programming"
-- Transaction Management
-- Oracle Database is designed as a multiuser database. The database must ensure 
-- that multiple users can work concurrently without corrupting one another's data.
-- Transactions
-- A transaction is a logical, atomic unit of work that contains one or more SQL 
-- statements.
-- An RDBE must be able to group SQL statements so that they are either all 
-- committed, which means they are applied to the database, or all rolled back, 
-- which means they are undone.
-- An illustration of the need for transactions is a funds transfer from a savings 
-- account to a checking account. The transfer consists of the following separate 
-- operations:
-- Decrease the savings account.
-- Increase the checking account.
-- Record the transaction in the transaction journal.
-- Oracle Database guarantees that all three operations succeed or fail as a unit. 
-- For example, if a hardware failure prevents a statement in the transaction from 
-- executing, then the other statements must be rolled back.
-- 
-- Transactions are one feature that set Oracle Database apart from a file system. 
-- If you perform an atomic operation that updates several files, and if the 
-- system fails halfway through, then the files will not be consistent. In 
-- contrast, a transaction moves an Oracle database from one consistent state to 
-- another. The basic principle of a transaction is "all or nothing": an atomic 
-- operation succeeds or fails as a whole.
-- See Also:
-- "Transactions" to learn about the definition of a transaction, statement-level 
-- atomicity, and transaction control
-- Data Concurrency
-- A requirement of a multiuser RDBE is the control of data concurrency, which is 
-- the simultaneous access of the same data by multiple users.
-- Without concurrency controls, users could change data improperly, compromising 
-- data integrity. For example, one user could update a row while a different user 
-- simultaneously updates it.
-- If multiple users access the same data, then one way of managing concurrency is 
-- to make users wait. However, the goal of a DBE is to reduce wait time so it is 
-- either nonexistent or negligible. All SQL statements that modify data must 
-- proceed with as little interference as possible. Destructive interactions, 
-- which are interactions that incorrectly update data or alter underlying data 
-- structures, must be avoided.
-- Oracle Database uses locks to control concurrent access to data. A lock is a 
-- mechanism that prevents destructive interaction between transactions accessing 
-- a shared resource. Locks help ensure data integrity while allowing maximum 
-- concurrent access to data.
-- See Also:
-- "Overview of the Oracle Database Locking Mechanism"
-- Data Consistency
-- In Oracle Database, each user must see a consistent view of the data, including 
-- visible changes made by a user's own transactions and committed transactions of 
-- other users.
-- For example, the database must prevent the lost update problem, which occurs 
-- when one transaction sees uncommitted changes made by another concurrent 
-- transaction.
-- Oracle Database always enforces statement-level read consistency, which 
-- guarantees that the data that a single query returns is committed and 
-- consistent for a single point in time. Depending on the transaction isolation 
-- level, this point is the time at which the statement was opened or the time the 
-- transaction began. The Oracle Flashback Query feature enables you to specify 
-- this point in time explicitly.
-- The database can also provide read consistency to all queries in a transaction, 
-- known as transaction-level read consistency. In this case, each statement in a 
-- transaction sees data from the same point in time, which is the time at which 
-- the transaction began.
-- See Also:
-- "Data Concurrency and Consistency" to learn more about lost updates
-- Oracle Database Development Guide to learn about Oracle Flashback Query
-- 
-- /* Per thread arena example. */
-- #include <stdio.h>
-- #include <stdlib.h>
-- #include <pthread.h>
-- #include <unistd.h>
-- #include <sys/types.h>
-- 
-- void* threadFunc(void* arg) {
--         printf("Before malloc in thread 1\n");
--         getchar();
--         char* addr = (char*) malloc(1000);
--         printf("After malloc and before free in thread 1\n");
--         getchar();
--         free(addr);
--         printf("After free in thread 1\n");
--         getchar();
-- }
-- 
-- int main() {
--         pthread_t t1;
--         void* s;
--         int ret;
--         char* addr;
-- 
--         printf("Welcome to per thread arena example::%d\n",getpid());
--         printf("Before malloc in main thread\n");
--         getchar();
--         addr = (char*) malloc(1000);
--         printf("After malloc and before free in main thread\n");
--         getchar();
--         free(addr);
--         printf("After free in main thread\n");
--         getchar();
--         ret = pthread_create(&t1, NULL, threadFunc, NULL);
--         if(ret)
--         {
--                 printf("Thread creation error\n");
--                 return -1;
--         }
--         ret = pthread_join(t1, &s);
--         if(ret)
--         {
--                 printf("Thread join error\n");
--                 return -1;
--         }
--         return 0;
-- }
-- 
-- Database Storage Structures
-- A database can be considered from both a physical and logical perspective.
-- 
-- Physical data is data viewable at the operating system level. For example, 
-- operating system utilities such as the Linux ls and ps can list database files 
-- and processes. Logical data such as a table is meaningful only for the 
-- database. A SQL statement can list the tables in an Oracle database, but an 
-- operating system utility cannot.
-- 
-- The database has physical structures and logical structures. Because the 
-- physical and logical structures are separate, you can manage the physical 
-- storage of data without affecting access to logical storage structures. For 
-- example, renaming a physical database file does not rename the tables whose 
-- data is stored in this file.
-- 
-- Physical Storage Structures
-- The physical database structures are the files that store the data.
-- 
-- When you execute a CREATE DATABASE statement, the following files are created:
-- 
-- Data files
-- 
-- Every Oracle database has one or more physical data files, which contain all 
-- the database data. The data of logical database structures, such as tables and 
-- indexes, is physically stored in the data files.
-- 
-- Control files
-- 
-- Every Oracle database has a control file. A control file contains metadata 
-- specifying the physical structure of the database, including the database name 
-- and the names and locations of the database files.
-- 
-- Online redo log files
-- 
-- Every Oracle Database has an online redo log, which is a set of two or more 
-- online redo log files. An online redo log is made up of redo entries (also 
-- called redo log records), which record all changes made to data.
-- 
-- Many other files are important for the functioning of an Oracle database 
-- server. These include parameter files and networking files. Backup files and 
-- archived redo log files are offline files important for backup and recovery.
-- 
-- See Also:
-- 
-- "Physical Storage Structures"
-- 
-- Logical Storage Structures
-- Logical storage structures enable Oracle Database to have fine-grained control 
-- of disk space use.
-- 
-- This topic discusses logical storage structures:
-- 
-- Data blocks
-- 
-- At the finest level of granularity, Oracle Database data is stored in data 
-- blocks. One data block corresponds to a specific number of bytes on disk.
-- 
-- Extents
-- 
-- An extent is a specific number of logically contiguous data blocks, obtained in 
-- a single allocation, used to store a specific type of information.
-- 
-- Segments
-- 
-- A segment is a set of extents allocated for a user object (for example, a table 
-- or index), undo data, or temporary data.
-- 
-- Tablespaces
-- 
-- A database is divided into logical storage units called tablespaces. A 
-- tablespace is the logical container for segments. Each tablespace consists of 
-- at least one data file.
-- 
-- See Also:
-- 
-- "Logical Storage Structures"
-- 
-- Database Instance Structures
-- An Oracle database uses memory structures and processes to manage and access 
-- the database. All memory structures exist in the main memory of the computers 
-- that constitute the RDBE.
-- 
-- When applications connect to an Oracle database, they connect to a database 
-- instance. The instance services applications by allocating other memory areas 
-- in addition to the SGA, and starting other processes in addition to background 
-- processes.
-- 
-- Oracle Database Processes
-- A process is a mechanism in an operating system that can run a series of steps. 
-- Some operating systems use the terms job, task, or thread.
-- 
-- For the purposes of this topic, a thread is equivalent to a process. An Oracle 
-- database instance has the following types of processes:
-- 
-- Client processes
-- 
-- These processes are created and maintained to run the software code of an 
-- application program or an Oracle tool. Most environments have separate 
-- computers for client processes.
-- 
-- Background processes
-- 
-- These processes consolidate functions that would otherwise be handled by 
-- multiple Oracle Database programs running for each client process. Background 
-- processes asynchronously perform I/O and monitor other Oracle Database 
-- processes to provide increased parallelism for better performance and 
-- reliability.
-- 
-- Server processes
-- 
-- These processes communicate with client processes and interact with Oracle 
-- Database to fulfill requests.
-- 
-- Oracle processes include server processes and background processes. In most 
-- environments, Oracle processes and client processes run on separate computers.
-- 
-- See Also:
-- 
-- "Process Architecture"
-- 
-- Instance Memory Structures
-- Oracle Database creates and uses memory structures for program code, data 
-- shared among users, and private data areas for each connected user.
-- 
-- The following memory structures are associated with a database instance:
-- 
-- System Global Area (SGA)
-- 
-- The SGA is a group of shared memory structures that contain data and control 
-- information for one database instance. Examples of SGA components include the 
-- database buffer cache and shared SQL areas. Starting in Oracle Database 12c 
-- Release 1 (12.1.0.2), the SGA can contain an optional In-Memory Column Store 
-- (IM column store), which enables data to be populated in memory in a columnar 
-- format.
-- 
-- Program Global Areas (PGA)
-- 
-- A PGA is a memory region that contains data and control information for a 
-- server or background process. Access to the PGA is exclusive to the process. 
-- Each server process and background process has its own PGA.
-- 
-- See Also:
-- 
-- "Memory Architecture"
-- 
-- Application and Networking Architecture
-- To take full advantage of a given computer system or network, Oracle Database 
-- enables processing to be split between the database server and the client 
-- programs. The computer running the RDBE handles the database server 
-- responsibilities while the computers running the applications handle the 
-- interpretation and display of data.
-- 
-- Application Architecture
-- The application architecture is the computing environment in which a database 
-- application connects to an Oracle database. The two most common database 
-- architectures are client/server and multitier.
-- 
-- In a client/server architecture, the client application initiates a request for 
-- an operation to be performed on the database server.
-- 
-- The server runs Oracle Database software and handles the functions required for 
-- concurrent, shared data access. The server receives and processes requests that 
-- originate from clients.
-- 
-- In a traditional multitier architecture, one or more application servers 
-- perform parts of the operation.
-- 
-- An application server contains a large part of the application logic, provides 
-- access to the data for the client, and performs some query processing. In this 
-- way, the load on the database decreases. The application server can serve as an 
-- interface between clients and multiple databases and provide an additional 
-- level of security.
-- 
-- A service-oriented architecture (SOA) is a multitier architecture in which 
-- application functionality is encapsulated in services. SOA services are usually 
-- implemented as Web services. Web services are accessible through HTTP and are 
-- based on XML-based standards such as Web Services Description Language (WSDL) 
-- and SOAP.
-- 
-- Oracle Database can act as a Web service provider in a traditional multitier or 
-- SOA environment.
-- 
-- See Also:
-- 
-- "Overview of Multitier Architecture"
-- 
-- Oracle XML DB Developer’s Guide for more information about using Web services 
-- with the database
-- 
-- Oracle Net Services Architecture
-- Oracle Net Services is the interface between the database and the network 
-- communication protocols that facilitate distributed processing and distributed 
-- databases.
-- 
-- Communication protocols define the way that data is transmitted and received on 
-- a network. Oracle Net Services supports communications on all major network 
-- protocols, including TCP/IP, HTTP, FTP, and WebDAV.
-- 
-- Oracle Net, a component of Oracle Net Services, establishes and maintains a 
-- network session from a client application to a database server. After a network 
-- session is established, Oracle Net acts as the data courier for both the client 
-- application and the database server, exchanging messages between them. Oracle 
-- Net can perform these jobs because it is located on each computer in the network.
-- 
-- An important component of Net Services is the Oracle Net Listener (called the 
-- listener), which is a process that runs on the database or elsewhere in the 
-- network. Client applications send connection requests to the listener, which 
-- manages the traffic of these requests to the database. When a connection is 
-- established, the client and database communicate directly.
-- 
-- The most common ways to configure an Oracle database to service client requests 
-- are:
-- 
-- Dedicated server architecture
-- 
-- Each client process connects to a dedicated server process. The server process 
-- is not shared by any other client for the duration of the client's session. 
-- Each new session is assigned a dedicated server process.
-- 
-- Shared server architecture
-- 
-- The database uses a pool of shared server processes for multiple sessions. A 
-- client process communicates with a dispatcher, which is a process that enables 
-- many clients to connect to the same database instance without the need for a 
-- dedicated server process for each client.
-- 
-- See Also:
-- 
-- "Overview of Oracle Net Services Architecture"
-- 
-- Oracle Database Net Services Administrator's Guide to learn more about Oracle 
-- Net architecture
-- 
-- Oracle XML DB Developer’s Guide for information about using WebDAV with the 
-- database
-- 
-- Oracle Database Documentation Roadmap
-- The documentation set is designed with specific access paths to ensure that 
-- users are able to find the information they need as efficiently as possible.
-- 
-- The documentation set is divided into three layers or groups: basic, 
-- intermediate, and advanced. Users begin with the manuals in the basic group, 
-- proceed to the manuals in the intermediate group (the 2 Day + series), and 
-- finally to the advanced manuals, which include the remainder of the 
-- documentation.
-- 
-- Oracle Database Documentation: Basic Group
-- Technical users who are new to Oracle Database begin by reading one or more 
-- manuals in the basic group from cover to cover. Each manual in this group is 
-- designed to be read in two days.
-- 
-- In addition to this manual, the basic group includes the manuals shown in the 
-- following table.

--------------------------------------------------------------------------------
-- Relational Model
-- In his seminal 1970 paper "A Relational Model of Data for Large Shared Data Banks,"
-- E. F. Codd defined a relational model based on mathematical set theory. 
-- Today, the most widely accepted database model is the relational model.
-- A relational database is a database that conforms to the relational model. 
-- The relational model has the following major aspects:
-- Structures
-- Well-defined objects store or access the data of a database.
-- Operations
-- Clearly defined actions enable applications to manipulate the data and
-- structures of a database.
-- Integrity rules
-- Integrity rules govern operations on the data and structures of a database.
-- A relational database stores data in a set of simple relations. A relation 
-- is a set of tuples. A tuple is an unordered set of attribute values.
-- A table is a two-dimensional representation of a relation in the form of 
-- rows (tuples) and columns (attributes). Each row in a table has the same set of 
-- columns. A relational database is a database that stores data in 
-- relations (tables). For example, a relational database could store information 
-- about company employees in an employee table, a department table, 
-- and a salary table.
-- See Also:
-- "A Relational Model of Data for Large Shared Data Banks" for an abstract and 
-- link to Codd's paper
-- Relational Database Management System (RDBE)
-- The relational model is the basis for a relational database management 
-- system (RDBE). An RDBE moves data into a database, stores the data, 
-- and retrieves it so that applications can manipulate it.
-- An RDBE distinguishes between the following types of operations:
-- Logical operations
-- In this case, an application specifies what content is required. For example, 
-- an application requests an employee name or adds an employee record to a table.
-- Physical operations
-- In this case, the RDBE determines how things should be done and carries out 
-- the operation. For example, after an application queries a table, the database 
-- may use an index to find the requested rows, read the data into memory, and 
-- perform many other steps before returning a result to the user. The RDBE 
-- stores and retrieves data so that physical operations are transparent 
-- to database applications.
-- Oracle Database is an RDBE. An RDBE that implements object-oriented features 
-- such as user-defined types, inheritance, and polymorphism is called an 
-- object-relational database management system (ORDBE). Oracle Database has 
-- extended the relational model to an object-relational model, making it 
-- possible to store complex business models in a relational database.
-- Brief History of Oracle Database
-- The current version of Oracle Database is the result of over 35 years of 
-- innovative development.
-- Highlights in the evolution of Oracle Database include the following:
-- Founding of Oracle
-- In 1977, Larry Ellison, Bob Miner, and Ed Oates started the consultancy 
-- Software Development Laboratories, which became Relational Software, Inc. (RSI). 
-- In 1983, RSI became Oracle Systems Corporation and then later Oracle Corporation.
-- First commercially available RDBE
-- In 1979, RSI introduced Oracle V2 (Version 2) as the first commercially 
-- available SQL-based RDBE, a landmark event in the history of relational 
-- databases.
-- Portable version of Oracle Database
-- Oracle Version 3, released in 1983, was the first relational database to run on 
-- mainframes, minicomputers, and PCs. The database was written in C, enabling the 
-- database to be ported to multiple platforms.
-- Enhancements to concurrency control, data distribution, and scalability
-- Version 4 introduced multiversion read consistency. Version 5, released in 1985, 
-- supported client/server computing and distributed database systems. Version 6 
-- brought enhancements to disk I/O, row locking, scalability, and backup and
-- recovery. 
-- Also, Version 6 introduced the first version of the PL/SQL language, a 
-- proprietary procedural extension to SQL.
-- PL/SQL stored program units
-- Oracle7, released in 1992, introduced PL/SQL stored procedures and triggers.
-- Objects and partitioning
-- Oracle8 was released in 1997 as the object-relational database, supporting many 
-- new data types. Additionally, Oracle8 supported partitioning of large tables.
-- Internet computing
-- Oracle8i Database, released in 1999, provided native support for internet 
-- protocols and server-side support for Java. Oracle8i was designed for internet 
-- computing, enabling the database to be deployed in a multitier environment.
-- Oracle Real Application Clusters (Oracle RAC)
-- Oracle9i Database introduced Oracle RAC in 2001, enabling multiple instances to 
-- access a single database simultaneously. Additionally, Oracle XML Database 
-- (Oracle XML DB) introduced the ability to store and query XML.
-- Grid computing
-- Oracle Database 10g introduced grid computing in 2003. This release enabled 
-- organizations to virtualize computing resources by building a grid
-- infrastructure 
-- based on low-cost commodity servers. A key goal was to make the database 
-- self-managing and self-tuning. Oracle Automatic Storage Management (Oracle ASM) 
-- helped achieve this goal by virtualizing and simplifying database storage 
-- management.
-- Manageability, diagnosability, and availability
-- Oracle Database 11g, released in 2007, introduced a host of new features that 
-- enabled administrators and developers to adapt quickly to changing business 
-- requirements. The key to adaptability is simplifying the information 
-- infrastructure by consolidating information and using automation wherever 
-- possible.
-- Plugging In to the Cloud
-- Oracle Database 12c, released in 2013, was designed for the Cloud, featuring 
-- a new Multitenant architecture, In-Memory column store, and support for JSON 
-- documents. Oracle Database 12c helps customers make more efficient use of their 
-- IT resources, while continuing to reduce costs and improve service levels for 
-- users.
-- Schema Objects
-- One characteristic of an RDBE is the independence of physical data storage from 
-- logical data structures.
-- In Oracle Database, a database schema is a collection of logical data 
-- structures, 
-- or schema objects. A database user owns a database schema, which has the same 
-- name as the user name.
-- Schema objects are user-created structures that directly refer to the data in 
-- the database. The database supports many types of schema objects, the most 
-- important of which are tables and indexes.
-- A schema object is one type of database object. Some database objects, such as 
-- profiles and roles, do not reside in schemas.
-- See Also:
-- "Introduction to Schema Objects" to learn more about schema object types, 
-- storage, and dependencies
-- Tables
-- A table describes an entity such as employees.
-- You define a table with a table name, such as employees, and set of columns. In 
-- general, you give each column a name, a data type, and a width when you create 
-- the table.
-- A table is a set of rows. A column identifies an attribute of the entity 
-- described by the table, whereas a row identifies an instance of the entity. For 
-- example, attributes of the employees entity correspond to columns for employee 
-- ID and last name. A row identifies a specific employee.
-- You can optionally specify a rule, called an integrity constraint, for a 
-- column. One example is a NOT NULL integrity constraint. This constraint forces 
-- the column to contain a value in every row.
-- See Also:
-- "Overview of Tables" to learn about columns and rows, data types, table 
-- storage, and table compression
-- "Data Integrity" to learn about the possible types and states of constraints
-- Indexes
-- An index is an optional data structure that you can create on one or more 
-- columns of a table. Indexes can increase the performance of data retrieval.
-- When processing a request, the database can use available indexes to locate the 
-- requested rows efficiently. Indexes are useful when applications often query a 
-- specific row or range of rows.
-- Indexes are logically and physically independent of the data. Thus, you can 
-- drop and create indexes with no effect on the tables or other indexes. All 
-- applications continue to function after you drop an index.
-- See Also:
-- "Introduction to Indexes" to learn about the purpose and types of indexes
-- Data Access
-- A general requirement for a DBE is to adhere to accepted industry standards 
-- for a data access language.
-- Structured Query Language (SQL)
-- SQL is a set-based declarative language that provides an interface to an RDBE 
-- such as Oracle Database.
-- Procedural languages such as C describe how things should be done. SQL is 
-- nonprocedural and describes what should be done.
-- SQL is the ANSI standard language for relational databases. All operations on 
-- the data in an Oracle database are performed using SQL statements. For example, 
-- you use SQL to create tables and query and modify data in tables.
-- A SQL statement can be thought of as a very simple, but powerful, computer 
-- program or instruction. Users specify the result that they want (for example, 
-- the names of employees), not how to derive it. A SQL statement is a string of 
-- SQL text such as the following:
-- SELECT first_name, last_name FROM employees;
-- SQL statements enable you to perform the following tasks:
-- Query data
-- Insert, update, and delete rows in a table
-- Create, replace, alter, and drop objects
-- Control access to the database and its objects
-- Guarantee database consistency and integrity
-- SQL unifies the preceding tasks in one consistent language. Oracle SQL is an 
-- implementation of the ANSI standard. Oracle SQL supports numerous features that 
-- extend beyond standard SQL.
-- See Also:
-- "SQL" to learn more about SQL standards and the main types of SQL statements
-- PL/SQL and Java
-- PL/SQL is a procedural extension to Oracle SQL.
-- PL/SQL is integrated with Oracle Database, enabling you to use all of the 
-- Oracle Database SQL statements, functions, and data types. You can use PL/SQL 
-- to control the flow of a SQL program, use variables, and write error-handling 
-- procedures.
-- A primary benefit of PL/SQL is the ability to store application logic in the 
-- database itself. A PL/SQL procedure or function is a schema object that 
-- consists of a set of SQL statements and other PL/SQL constructs, grouped 
-- together, stored in the database, and run as a unit to solve a specific problem 
-- or to perform a set of related tasks. The principal benefit of server-side 
-- programming is that built-in functionality can be deployed anywhere.
-- Oracle Database can also store program units written in Java. A Java stored 
-- procedure is a Java method published to SQL and stored in the database for 
-- general use. You can call existing PL/SQL programs from Java and Java programs 
-- from PL/SQL.
-- See Also:
-- "Server-Side Programming: PL/SQL and Java"
-- "Client-Side Database Programming"
-- Transaction Management
-- Oracle Database is designed as a multiuser database. The database must ensure 
-- that multiple users can work concurrently without corrupting one another's data.
-- Transactions
-- A transaction is a logical, atomic unit of work that contains one or more SQL 
-- statements.
-- An RDBE must be able to group SQL statements so that they are either all 
-- committed, which means they are applied to the database, or all rolled back, 
-- which means they are undone.
-- An illustration of the need for transactions is a funds transfer from a savings 
-- account to a checking account. The transfer consists of the following separate 
-- operations:
-- Decrease the savings account.
-- Increase the checking account.
-- Record the transaction in the transaction journal.
-- Oracle Database guarantees that all three operations succeed or fail as a unit. 
-- For example, if a hardware failure prevents a statement in the transaction from 
-- executing, then the other statements must be rolled back.
-- 
-- Transactions are one feature that set Oracle Database apart from a file system. 
-- If you perform an atomic operation that updates several files, and if the 
-- system fails halfway through, then the files will not be consistent. In 
-- contrast, a transaction moves an Oracle database from one consistent state to 
-- another. The basic principle of a transaction is "all or nothing": an atomic 
-- operation succeeds or fails as a whole.
-- See Also:
-- "Transactions" to learn about the definition of a transaction, statement-level 
-- atomicity, and transaction control
-- Data Concurrency
-- A requirement of a multiuser RDBE is the control of data concurrency, which is 
-- the simultaneous access of the same data by multiple users.
-- Without concurrency controls, users could change data improperly, compromising 
-- data integrity. For example, one user could update a row while a different user 
-- simultaneously updates it.
-- If multiple users access the same data, then one way of managing concurrency is 
-- to make users wait. However, the goal of a DBE is to reduce wait time so it is 
-- either nonexistent or negligible. All SQL statements that modify data must 
-- proceed with as little interference as possible. Destructive interactions, 
-- which are interactions that incorrectly update data or alter underlying data 
-- structures, must be avoided.
-- Oracle Database uses locks to control concurrent access to data. A lock is a 
-- mechanism that prevents destructive interaction between transactions accessing 
-- a shared resource. Locks help ensure data integrity while allowing maximum 
-- concurrent access to data.
-- See Also:
-- "Overview of the Oracle Database Locking Mechanism"
-- Data Consistency
-- In Oracle Database, each user must see a consistent view of the data, including 
-- visible changes made by a user's own transactions and committed transactions of 
-- other users.
-- For example, the database must prevent the lost update problem, which occurs 
-- when one transaction sees uncommitted changes made by another concurrent 
-- transaction.
-- Oracle Database always enforces statement-level read consistency, which 
-- guarantees that the data that a single query returns is committed and 
-- consistent for a single point in time. Depending on the transaction isolation 
-- level, this point is the time at which the statement was opened or the time the 
-- transaction began. The Oracle Flashback Query feature enables you to specify 
-- this point in time explicitly.
-- The database can also provide read consistency to all queries in a transaction, 
-- known as transaction-level read consistency. In this case, each statement in a 
-- transaction sees data from the same point in time, which is the time at which 
-- the transaction began.
-- See Also:
-- "Data Concurrency and Consistency" to learn more about lost updates
-- Oracle Database Development Guide to learn about Oracle Flashback Query
-- 
-- /* Per thread arena example. */
-- #include <stdio.h>
-- #include <stdlib.h>
-- #include <pthread.h>
-- #include <unistd.h>
-- #include <sys/types.h>
-- 
-- void* threadFunc(void* arg) {
--         printf("Before malloc in thread 1\n");
--         getchar();
--         char* addr = (char*) malloc(1000);
--         printf("After malloc and before free in thread 1\n");
--         getchar();
--         free(addr);
--         printf("After free in thread 1\n");
--         getchar();
-- }
-- 
-- int main() {
--         pthread_t t1;
--         void* s;
--         int ret;
--         char* addr;
-- 
--         printf("Welcome to per thread arena example::%d\n",getpid());
--         printf("Before malloc in main thread\n");
--         getchar();
--         addr = (char*) malloc(1000);
--         printf("After malloc and before free in main thread\n");
--         getchar();
--         free(addr);
--         printf("After free in main thread\n");
--         getchar();
--         ret = pthread_create(&t1, NULL, threadFunc, NULL);
--         if(ret)
--         {
--                 printf("Thread creation error\n");
--                 return -1;
--         }
--         ret = pthread_join(t1, &s);
--         if(ret)
--         {
--                 printf("Thread join error\n");
--                 return -1;
--         }
--         return 0;
-- }
-- 
-- Database Storage Structures
-- A database can be considered from both a physical and logical perspective.
-- 
-- Physical data is data viewable at the operating system level. For example, 
-- operating system utilities such as the Linux ls and ps can list database files 
-- and processes. Logical data such as a table is meaningful only for the 
-- database. A SQL statement can list the tables in an Oracle database, but an 
-- operating system utility cannot.
-- 
-- The database has physical structures and logical structures. Because the 
-- physical and logical structures are separate, you can manage the physical 
-- storage of data without affecting access to logical storage structures. For 
-- example, renaming a physical database file does not rename the tables whose 
-- data is stored in this file.
-- 
-- Physical Storage Structures
-- The physical database structures are the files that store the data.
-- 
-- When you execute a CREATE DATABASE statement, the following files are created:
-- 
-- Data files
-- 
-- Every Oracle database has one or more physical data files, which contain all 
-- the database data. The data of logical database structures, such as tables and 
-- indexes, is physically stored in the data files.
-- 
-- Control files
-- 
-- Every Oracle database has a control file. A control file contains metadata 
-- specifying the physical structure of the database, including the database name 
-- and the names and locations of the database files.
-- 
-- Online redo log files
-- 
-- Every Oracle Database has an online redo log, which is a set of two or more 
-- online redo log files. An online redo log is made up of redo entries (also 
-- called redo log records), which record all changes made to data.
-- 
-- Many other files are important for the functioning of an Oracle database 
-- server. These include parameter files and networking files. Backup files and 
-- archived redo log files are offline files important for backup and recovery.
-- 
-- See Also:
-- 
-- "Physical Storage Structures"
-- 
-- Logical Storage Structures
-- Logical storage structures enable Oracle Database to have fine-grained control 
-- of disk space use.
-- 
-- This topic discusses logical storage structures:
-- 
-- Data blocks
-- 
-- At the finest level of granularity, Oracle Database data is stored in data 
-- blocks. One data block corresponds to a specific number of bytes on disk.
-- 
-- Extents
-- 
-- An extent is a specific number of logically contiguous data blocks, obtained in 
-- a single allocation, used to store a specific type of information.
-- 
-- Segments
-- 
-- A segment is a set of extents allocated for a user object (for example, a table 
-- or index), undo data, or temporary data.
-- 
-- Tablespaces
-- 
-- A database is divided into logical storage units called tablespaces. A 
-- tablespace is the logical container for segments. Each tablespace consists of 
-- at least one data file.
-- 
-- See Also:
-- 
-- "Logical Storage Structures"
-- 
-- Database Instance Structures
-- An Oracle database uses memory structures and processes to manage and access 
-- the database. All memory structures exist in the main memory of the computers 
-- that constitute the RDBE.
-- 
-- When applications connect to an Oracle database, they connect to a database 
-- instance. The instance services applications by allocating other memory areas 
-- in addition to the SGA, and starting other processes in addition to background 
-- processes.
-- 
-- Oracle Database Processes
-- A process is a mechanism in an operating system that can run a series of steps. 
-- Some operating systems use the terms job, task, or thread.
-- 
-- For the purposes of this topic, a thread is equivalent to a process. An Oracle 
-- database instance has the following types of processes:
-- 
-- Client processes
-- 
-- These processes are created and maintained to run the software code of an 
-- application program or an Oracle tool. Most environments have separate 
-- computers for client processes.
-- 
-- Background processes
-- 
-- These processes consolidate functions that would otherwise be handled by 
-- multiple Oracle Database programs running for each client process. Background 
-- processes asynchronously perform I/O and monitor other Oracle Database 
-- processes to provide increased parallelism for better performance and 
-- reliability.
-- 
-- Server processes
-- 
-- These processes communicate with client processes and interact with Oracle 
-- Database to fulfill requests.
-- 
-- Oracle processes include server processes and background processes. In most 
-- environments, Oracle processes and client processes run on separate computers.
-- 
-- See Also:
-- 
-- "Process Architecture"
-- 
-- Instance Memory Structures
-- Oracle Database creates and uses memory structures for program code, data 
-- shared among users, and private data areas for each connected user.
-- 
-- The following memory structures are associated with a database instance:
-- 
-- System Global Area (SGA)
-- 
-- The SGA is a group of shared memory structures that contain data and control 
-- information for one database instance. Examples of SGA components include the 
-- database buffer cache and shared SQL areas. Starting in Oracle Database 12c 
-- Release 1 (12.1.0.2), the SGA can contain an optional In-Memory Column Store 
-- (IM column store), which enables data to be populated in memory in a columnar 
-- format.
-- 
-- Program Global Areas (PGA)
-- 
-- A PGA is a memory region that contains data and control information for a 
-- server or background process. Access to the PGA is exclusive to the process. 
-- Each server process and background process has its own PGA.
-- 
-- See Also:
-- 
-- "Memory Architecture"
-- 
-- Application and Networking Architecture
-- To take full advantage of a given computer system or network, Oracle Database 
-- enables processing to be split between the database server and the client 
-- programs. The computer running the RDBE handles the database server 
-- responsibilities while the computers running the applications handle the 
-- interpretation and display of data.
-- 
-- Application Architecture
-- The application architecture is the computing environment in which a database 
-- application connects to an Oracle database. The two most common database 
-- architectures are client/server and multitier.
-- 
-- In a client/server architecture, the client application initiates a request for 
-- an operation to be performed on the database server.
-- 
-- The server runs Oracle Database software and handles the functions required for 
-- concurrent, shared data access. The server receives and processes requests that 
-- originate from clients.
-- 
-- In a traditional multitier architecture, one or more application servers 
-- perform parts of the operation.
-- 
-- An application server contains a large part of the application logic, provides 
-- access to the data for the client, and performs some query processing. In this 
-- way, the load on the database decreases. The application server can serve as an 
-- interface between clients and multiple databases and provide an additional 
-- level of security.
-- 
-- A service-oriented architecture (SOA) is a multitier architecture in which 
-- application functionality is encapsulated in services. SOA services are usually 
-- implemented as Web services. Web services are accessible through HTTP and are 
-- based on XML-based standards such as Web Services Description Language (WSDL) 
-- and SOAP.
-- 
-- Oracle Database can act as a Web service provider in a traditional multitier or 
-- SOA environment.
-- 
-- See Also:
-- 
-- "Overview of Multitier Architecture"
-- 
-- Oracle XML DB Developer’s Guide for more information about using Web services 
-- with the database
-- 
-- Oracle Net Services Architecture
-- Oracle Net Services is the interface between the database and the network 
-- communication protocols that facilitate distributed processing and distributed 
-- databases.
-- 
-- Communication protocols define the way that data is transmitted and received on 
-- a network. Oracle Net Services supports communications on all major network 
-- protocols, including TCP/IP, HTTP, FTP, and WebDAV.
-- 
-- Oracle Net, a component of Oracle Net Services, establishes and maintains a 
-- network session from a client application to a database server. After a network 
-- session is established, Oracle Net acts as the data courier for both the client 
-- application and the database server, exchanging messages between them. Oracle 
-- Net can perform these jobs because it is located on each computer in the network.
-- 
-- An important component of Net Services is the Oracle Net Listener (called the 
-- listener), which is a process that runs on the database or elsewhere in the 
-- network. Client applications send connection requests to the listener, which 
-- manages the traffic of these requests to the database. When a connection is 
-- established, the client and database communicate directly.
-- 
-- The most common ways to configure an Oracle database to service client requests 
-- are:
-- 
-- Dedicated server architecture
-- 
-- Each client process connects to a dedicated server process. The server process 
-- is not shared by any other client for the duration of the client's session. 
-- Each new session is assigned a dedicated server process.
-- 
-- Shared server architecture
-- 
-- The database uses a pool of shared server processes for multiple sessions. A 
-- client process communicates with a dispatcher, which is a process that enables 
-- many clients to connect to the same database instance without the need for a 
-- dedicated server process for each client.
-- 
-- See Also:
-- 
-- "Overview of Oracle Net Services Architecture"
-- 
-- Oracle Database Net Services Administrator's Guide to learn more about Oracle 
-- Net architecture
-- 
-- Oracle XML DB Developer’s Guide for information about using WebDAV with the 
-- database
-- 
-- Oracle Database Documentation Roadmap
-- The documentation set is designed with specific access paths to ensure that 
-- users are able to find the information they need as efficiently as possible.
-- 
-- The documentation set is divided into three layers or groups: basic, 
-- intermediate, and advanced. Users begin with the manuals in the basic group, 
-- proceed to the manuals in the intermediate group (the 2 Day + series), and 
-- finally to the advanced manuals, which include the remainder of the 
-- documentation.
-- 
-- Oracle Database Documentation: Basic Group
-- Technical users who are new to Oracle Database begin by reading one or more 
-- manuals in the basic group from cover to cover. Each manual in this group is 
-- designed to be read in two days.
-- 
-- In addition to this manual, the basic group includes the manuals shown in the 
-- following table.

   v_num := v_num + 1;
   v_num := v_num + 1;
   v_num := v_num + 1;
   v_num := v_num + 1;
   dbe_output.print_line('v_num := '||v_num);
END;
/

Logical export succeeded.

SQL> 
SQL> -- DTS2018122711387: bug: truncated binary & varbinary datatype
SQL> drop table if exists EXP_TAB_BIN;

Succeed.

SQL> create table EXP_TAB_BIN(fb binary(30), vb varbinary(200), img image, a blob);

Succeed.

SQL> insert into EXP_TAB_BIN select rownum, sin(rownum), sqrt(rownum/7.3) || floor(exp(rownum)), floor(exp(rownum)) || 'ABC' from dual connect by rownum <= 80;

80 rows affected.

SQL> 
SQL> select count(*), avg(fb::varchar(300)), avg(vb::varchar(300)), avg(img::varchar(300)), avg(vsize(a)) from EXP_TAB_BIN;

COUNT(*)             AVG(FB::VARCHAR(300))                    AVG(VB::VARCHAR(300))                    AVG(IMG::VARCHAR(300))                   AVG(VSIZE(A))                           
-------------------- ---------------------------------------- ---------------------------------------- ---------------------------------------- ----------------------------------------
80                   40.5                                     .006491634764188323679642709561301596272 2.22669895888468710486922989327928211849 27.8                                    

1 rows fetched.

SQL> 
SQL> exp tables=EXP_TAB_BIN file = "exp_tab_bin.sql";
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = EXP_TAB_BIN
-- FILE TYPE = TXT
-- DUMP FILE = exp_tab_bin.sql
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER_LONG_PROC.EXP_TAB_BIN ...
  exporting DDL of EXP_USER_LONG_PROC.EXP_TAB_BIN ...
  exporting data of EXP_USER_LONG_PROC.EXP_TAB_BIN ...
    data exporting success, 80 rows are dumped.

  exporting indexes on EXP_USER_LONG_PROC.EXP_TAB_BIN ...
  exporting constraints on EXP_USER_LONG_PROC.EXP_TAB_BIN ...

Logical export succeeded.

SQL> 
SQL> truncate table EXP_TAB_BIN;

Succeed.

SQL> 
SQL> \! ctsql exp_user_long_proc/exp_user123@127.0.0.1:1611 -c "@./exp_tab_bin.sql";


SQL> 
SQL> select count(*), avg(fb::varchar(300)), avg(vb::varchar(300)), avg(img::varchar(300)), avg(vsize(a)) from EXP_TAB_BIN;

COUNT(*)             AVG(FB::VARCHAR(300))                    AVG(VB::VARCHAR(300))                    AVG(IMG::VARCHAR(300))                   AVG(VSIZE(A))                           
-------------------- ---------------------------------------- ---------------------------------------- ---------------------------------------- ----------------------------------------
80                   40.5                                     .006491634764188323679642709561301596272 2.22669895888468710486922989327928211849 27.8                                    

1 rows fetched.

SQL> 
SQL> insert into EXP_TAB_BIN select * from EXP_TAB_BIN;

80 rows affected.

SQL> insert into EXP_TAB_BIN select * from EXP_TAB_BIN;

160 rows affected.

SQL> insert into EXP_TAB_BIN select * from EXP_TAB_BIN;

320 rows affected.

SQL> insert into EXP_TAB_BIN select * from EXP_TAB_BIN;

640 rows affected.

SQL> commit;

Succeed.

SQL> 
SQL> exp tables=EXP_TAB_BIN file = "exp_tab_bin2.sql" parallel=3 insert_batch=10;
Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = EXP_TAB_BIN
-- FILE TYPE = TXT
-- DUMP FILE = exp_tab_bin2.sql
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 10
-- FEEDBACK = 10000
-- PARALLEL = 3
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER_LONG_PROC.EXP_TAB_BIN ...
  exporting DDL of EXP_USER_LONG_PROC.EXP_TAB_BIN ...
  exporting indexes on EXP_USER_LONG_PROC.EXP_TAB_BIN ...
  exporting constraints on EXP_USER_LONG_PROC.EXP_TAB_BIN ...

Logical export succeeded.

SQL> 
SQL> truncate table EXP_TAB_BIN;

Succeed.

SQL> select count(*), avg(fb::varchar(300)), avg(vb::varchar(300)), avg(img::varchar(300)), avg(vsize(a)) from EXP_TAB_BIN;

COUNT(*)             AVG(FB::VARCHAR(300))                    AVG(VB::VARCHAR(300))                    AVG(IMG::VARCHAR(300))                   AVG(VSIZE(A))                           
-------------------- ---------------------------------------- ---------------------------------------- ---------------------------------------- ----------------------------------------
0                                                                                                                                                                                       

1 rows fetched.

SQL> 
SQL> \! ctsql exp_user_long_proc/exp_user123@127.0.0.1:1611 -c "@./exp_tab_bin2.sql";


SQL> select count(*), avg(fb::varchar(300)), avg(vb::varchar(300)), avg(img::varchar(300)), avg(vsize(a)) from EXP_TAB_BIN;

COUNT(*)             AVG(FB::VARCHAR(300))                    AVG(VB::VARCHAR(300))                    AVG(IMG::VARCHAR(300))                   AVG(VSIZE(A))                           
-------------------- ---------------------------------------- ---------------------------------------- ---------------------------------------- ----------------------------------------
1280                 40.5                                     .006491634764188323679642709561301596272 2.22669895888468710486922989327928211847 27.8                                    

1 rows fetched.

SQL> 
SQL> -- exp bigwidth table
SQL> drop user if exists EXP_USER_LONG_PROC cascade;

CT-00807, The user EXP_USER_LONG_PROC has logged in, can not be dropped now
SQL> create user EXP_USER_LONG_PROC IDENTIFIED by 'exp_user123';

CT-00753, The object user EXP_USER_LONG_PROC already exists.
SQL> grant dba to EXP_USER_LONG_PROC;

Succeed.

SQL> conn EXP_USER_LONG_PROC/exp_user123@127.0.0.1:1611

connected.

SQL> 
SQL> set termout off



OFF





Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TABLE_NAME_CHAOCHANGTAB_MMMMMMMMM_ARE_YOU_OK
-- FILE TYPE = TXT
-- DUMP FILE = ./exp_bigwide_table.sql
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = Y
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_USER_LONG_PROC.TABLE_NAME_CHAOCHANGTAB_MMMMMMMMM_ARE_YOU_OK ...
  exporting DDL of EXP_USER_LONG_PROC.TABLE_NAME_CHAOCHANGTAB_MMMMMMMMM_ARE_YOU_OK ...
  exporting data of EXP_USER_LONG_PROC.TABLE_NAME_CHAOCHANGTAB_MMMMMMMMM_ARE_YOU_OK ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_USER_LONG_PROC.TABLE_NAME_CHAOCHANGTAB_MMMMMMMMM_ARE_YOU_OK ...
  exporting constraints on EXP_USER_LONG_PROC.TABLE_NAME_CHAOCHANGTAB_MMMMMMMMM_ARE_YOU_OK ...

Logical export succeeded.














ON

ON

EXPORT_BIGWIDE_TABLE
--------------------
1                   

1 rows fetched.


connected.


Succeed.


Succeed.


Succeed.


connected.


Succeed.


1 rows affected.


1 rows affected.


1 rows affected.


Succeed.

Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TABLE_EXP_AUTO_INCREMENT
-- FILE TYPE = TXT
-- DUMP FILE = table_exp_auto_increment.sql
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_TEST_AUTO_INCRE.TABLE_EXP_AUTO_INCREMENT ...
  exporting DDL of EXP_TEST_AUTO_INCRE.TABLE_EXP_AUTO_INCREMENT ...
  exporting data of EXP_TEST_AUTO_INCRE.TABLE_EXP_AUTO_INCREMENT ...
    data exporting success, 3 rows are dumped.

  exporting auto_increment attr on EXP_TEST_AUTO_INCRE.TABLE_EXP_AUTO_INCREMENT ...  
  exporting indexes on EXP_TEST_AUTO_INCRE.TABLE_EXP_AUTO_INCREMENT ...
  exporting constraints on EXP_TEST_AUTO_INCRE.TABLE_EXP_AUTO_INCREMENT ...

Logical export succeeded.


Succeed.




1 rows affected.


1 rows affected.


Succeed.


F1           F2          
------------ ------------
1            1000        
2            1001        
5            1002        
4            1100        

4 rows fetched.


Succeed.


Succeed.


Succeed.


Succeed.

Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = EXP_GLOBAL_TMP_TABLE
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_TEST_AUTO_INCRE.EXP_GLOBAL_TMP_TABLE ...
  exporting DDL of EXP_TEST_AUTO_INCRE.EXP_GLOBAL_TMP_TABLE ...
  skipping to export the data of temporary table
  exporting indexes on EXP_TEST_AUTO_INCRE.EXP_GLOBAL_TMP_TABLE ...
  exporting constraints on EXP_TEST_AUTO_INCRE.EXP_GLOBAL_TMP_TABLE ...

--** The script is dumped by *CTSQL/EXP* tool, Zenith@Huawei Cantian Dept.

-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = EXP_GLOBAL_TMP_TABLE
-- FILE TYPE = TXT
-- DUMP FILE = stdout
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
ALTER SESSION SET NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';
ALTER SESSION SET NLS_TIMESTAMP_TZ_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF TZH:TZM';
ALTER SESSION SET NLS_TIME_FORMAT = 'HH:MI:SS.FF AM';
ALTER SESSION SET NLS_TIME_TZ_FORMAT = 'HH:MI:SS.FF AM TZR';

DROP TABLE IF EXISTS "EXP_GLOBAL_TMP_TABLE" CASCADE CONSTRAINTS;
CREATE GLOBAL TEMPORARY TABLE "EXP_GLOBAL_TMP_TABLE"
(
  "C" BINARY_INTEGER,
  "C1" BINARY_INTEGER
)ON COMMIT DELETE ROWS;
CREATE INDEX "EXP_GLOBAL_TMP_TABLE_INDX" ON "EXP_GLOBAL_TMP_TABLE"("C")
INITRANS 2
PCTFREE 8;


Logical export succeeded.


connected.


Succeed.


Succeed.


Succeed.


connected.


Succeed.


Succeed.


Succeed.


Succeed.

Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = eos_port_ldc_db
-- FILE TYPE = TXT
-- DUMP FILE = ./data/case_sensitive_unique_idx.dat
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table CASE_SENSITIVE_UNIQUE_IDX.eos_port_ldc_db ...
  exporting DDL of CASE_SENSITIVE_UNIQUE_IDX.eos_port_ldc_db ...
  exporting data of CASE_SENSITIVE_UNIQUE_IDX.eos_port_ldc_db ...
    data exporting success, 0 rows are dumped.

  exporting auto_increment attr on CASE_SENSITIVE_UNIQUE_IDX.eos_port_ldc_db ...  
  exporting indexes on CASE_SENSITIVE_UNIQUE_IDX.eos_port_ldc_db ...
  exporting constraints on CASE_SENSITIVE_UNIQUE_IDX.eos_port_ldc_db ...

Logical export succeeded.

Parsing import options ... 
Verify options ...
  verify tables ...
Starting import ...
Preparing to import ...
-- IMPORT TYPE = TABLE
-- IMPORT OBJECTS = eos_port_ldc_db
-- DUMP FILE = ./data/case_sensitive_unique_idx.dat
-- LOG FILE = 
-- FILE TYPE = TXT
-- SHOW = N
-- FEEDBACK = 10000
-- PARALLEL = 1
-- DDL_PARALLEL = 1
-- CONTENT_MODE = ALL
-- IGNORE = N
-- CREATE_USER = N
-- TIMING = OFF
-- BATCH_COUNT = 10000
-- DISABLE_TRIGGER = Y
-- NOLOGGING = N

data importing success, 0 rows are loaded.
Logical import succeeded.

Parsing export options ... 
Verify options ...
Starting export ...
Preparing to export ...

CT-00230, ?/ is not an existing folder
Logical export failed.

Parsing export options ... 
Verify options ...
Starting export ...
Preparing to export ...

CT-00230, ./data/?/ is not an existing folder
Logical export failed.

Parsing export options ... 
Verify options ...
Starting export ...
Preparing to export ...

CT-00230, ?/ is not an existing folder
Logical export failed.

Parsing export options ... 
Verify options ...
Starting export ...
Preparing to export ...

CT-00230, ./data/?/ is not an existing folder
Logical export failed.

Parsing export options ... 
Verify options ...
Starting export ...
Preparing to export ...

CT-00501, Invalid file name: ./exp_logpath/
Logical export failed.

Parsing export options ... 
Verify options ...
Starting export ...
Preparing to export ...

CT-00501, Invalid file name: ./exp_logpath1/
Logical export failed.

Parsing import options ... 
Verify options ...
Starting import ...
Preparing to import ...

CT-00230, ?/ is not an existing folder
Logical import failed.

Parsing import options ... 
Verify options ...
Starting import ...
Preparing to import ...

CT-00230, ./data/?/ is not an existing folder
Logical import failed.

Parsing import options ... 
Verify options ...
Starting import ...
Preparing to import ...

CT-00501, Invalid file name: ./imp_logpath/
Logical import failed.

Parsing import options ... 
Verify options ...
Starting import ...
Preparing to import ...

CT-00002, Failed to open the file ./imp_logpath1/, the error code was 2
Logical import failed.


connected.


Succeed.


Succeed.


Succeed.


Succeed.


Succeed.


connected.

Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...

CT-00501, Invalid file name: ./data/
Logical export failed.


connected.


Succeed.


Succeed.


Succeed.


connected.


Succeed.


Succeed.


1 rows affected.


LENGTHB(F_CLOB)      LENGTHB(F_BLOB)     
-------------------- --------------------
8000                 8000                

1 rows fetched.

Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = T_LOB_DATA
-- FILE TYPE = TXT
-- DUMP FILE = lob8000.txt
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_LOB.T_LOB_DATA ...
  exporting DDL of EXP_LOB.T_LOB_DATA ...
  exporting data of EXP_LOB.T_LOB_DATA ...
    data exporting success, 1 rows are dumped.

  exporting indexes on EXP_LOB.T_LOB_DATA ...
  exporting constraints on EXP_LOB.T_LOB_DATA ...

Logical export succeeded.


1 rows affected.


LENGTHB(F_CLOB)      LENGTHB(F_BLOB)     
-------------------- --------------------
8000                 8001                

1 rows fetched.

Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = T_LOB_DATA
-- FILE TYPE = TXT
-- DUMP FILE = blob8001.txt
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_LOB.T_LOB_DATA ...
  exporting DDL of EXP_LOB.T_LOB_DATA ...
  exporting data of EXP_LOB.T_LOB_DATA ...

CT-00108, Lob size limits to 8000, blob column(1) should use "filetype=bin" to export
Logical export failed.


1 rows affected.


LENGTHB(F_CLOB)      LENGTHB(F_BLOB)     
-------------------- --------------------
8001                 4000                

1 rows fetched.

Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = T_LOB_DATA
-- FILE TYPE = TXT
-- DUMP FILE = clob8001.txt
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table EXP_LOB.T_LOB_DATA ...
  exporting DDL of EXP_LOB.T_LOB_DATA ...
  exporting data of EXP_LOB.T_LOB_DATA ...

CT-00108, Lob size limits to 8000, clob column(0) should use "filetype=bin" to export
Logical export failed.


connected.


Succeed.


Succeed.


Succeed.


Succeed.


connected.


Succeed.


Succeed.


Succeed.


Succeed.


Succeed.


Succeed.


Succeed.


Succeed.


Succeed.


connected.

Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_VIEW_ORDER
-- FILE TYPE = TXT
-- DUMP FILE = ./data/exp_view_order.dmp
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema EXP_VIEW_ORDER ...
Exporting sequence of schema EXP_VIEW_ORDER ...
Exporting profile of schema EXP_VIEW_ORDER ...
Exporting type of schema EXP_VIEW_ORDER ...
Exporting tables of schema EXP_VIEW_ORDER ...
Reading table objects of EXP_VIEW_ORDER

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
TS                                                               1         

Exporting tables (scripts or data) of EXP_VIEW_ORDER
exporting table EXP_VIEW_ORDER.TS ...
  exporting DDL of EXP_VIEW_ORDER.TS ...
  exporting data of EXP_VIEW_ORDER.TS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_VIEW_ORDER.TS ...
  exporting constraints on EXP_VIEW_ORDER.TS ...

Exporting procedures/functions/triggers of schema EXP_VIEW_ORDER ...
Exporting views of schema EXP_VIEW_ORDER ...
  exporting view EXP_VIEW_ORDER.V_3
  exporting view EXP_VIEW_ORDER.V_1
  exporting view EXP_VIEW_ORDER.V_2
  exporting view EXP_VIEW_ORDER.V_4
Exporting synonyms of schema EXP_VIEW_ORDER ...
Exporting package of schema EXP_VIEW_ORDER ...
End of export schema EXP_VIEW_ORDER ...

Logical export succeeded.


Succeed.


Succeed.

Parsing import options ... 
Verify options ...
  verify schema ...
Starting import ...
Preparing to import ...
-- IMPORT TYPE = SCHEMA
-- IMPORT OBJECTS = EXP_VIEW_ORDER
-- DUMP FILE = ./data/exp_view_order.dmp
-- LOG FILE = 
-- FILE TYPE = TXT
-- SHOW = N
-- FEEDBACK = 10000
-- PARALLEL = 1
-- DDL_PARALLEL = 1
-- CONTENT_MODE = ALL
-- IGNORE = N
-- CREATE_USER = N
-- TIMING = OFF
-- BATCH_COUNT = 10000
-- DISABLE_TRIGGER = Y
-- NOLOGGING = N

data importing success, 0 rows are loaded.
Logical import succeeded.

Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_VIEW_ORDER
-- FILE TYPE = TXT
-- DUMP FILE = ./data/exp_view_order_consistent.dmp
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = Y
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema EXP_VIEW_ORDER ...
Exporting sequence of schema EXP_VIEW_ORDER ...
Exporting profile of schema EXP_VIEW_ORDER ...
Exporting type of schema EXP_VIEW_ORDER ...
Exporting tables of schema EXP_VIEW_ORDER ...
Reading table objects of EXP_VIEW_ORDER

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
TS                                                               1         

Exporting tables (scripts or data) of EXP_VIEW_ORDER
exporting table EXP_VIEW_ORDER.TS ...
  exporting DDL of EXP_VIEW_ORDER.TS ...
  exporting data of EXP_VIEW_ORDER.TS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_VIEW_ORDER.TS ...
  exporting constraints on EXP_VIEW_ORDER.TS ...

Exporting procedures/functions/triggers of schema EXP_VIEW_ORDER ...
Exporting views of schema EXP_VIEW_ORDER ...
  exporting view EXP_VIEW_ORDER.V_3
  exporting view EXP_VIEW_ORDER.V_1
  exporting view EXP_VIEW_ORDER.V_2
  exporting view EXP_VIEW_ORDER.V_4
Exporting synonyms of schema EXP_VIEW_ORDER ...
Exporting package of schema EXP_VIEW_ORDER ...
End of export schema EXP_VIEW_ORDER ...

Logical export succeeded.


Succeed.


Succeed.

Parsing import options ... 
Verify options ...
  verify schema ...
Starting import ...
Preparing to import ...
-- IMPORT TYPE = SCHEMA
-- IMPORT OBJECTS = EXP_VIEW_ORDER
-- DUMP FILE = ./data/exp_view_order_consistent.dmp
-- LOG FILE = 
-- FILE TYPE = TXT
-- SHOW = N
-- FEEDBACK = 10000
-- PARALLEL = 1
-- DDL_PARALLEL = 1
-- CONTENT_MODE = ALL
-- IGNORE = N
-- CREATE_USER = N
-- TIMING = OFF
-- BATCH_COUNT = 10000
-- DISABLE_TRIGGER = Y
-- NOLOGGING = N

data importing success, 0 rows are loaded.
Logical import succeeded.


Succeed.


connected.


Succeed.


Succeed.


Succeed.


connected.


Succeed.


Succeed.


2 rows affected.


Succeed.


connected.

Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_INTERVAL_PART_TABLE
-- FILE TYPE = BIN
-- DUMP FILE = ./data/exp_interval_part_table.dmp
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 16
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema EXP_INTERVAL_PART_TABLE ...
Exporting sequence of schema EXP_INTERVAL_PART_TABLE ...
Exporting profile of schema EXP_INTERVAL_PART_TABLE ...
Exporting type of schema EXP_INTERVAL_PART_TABLE ...
Exporting tables of schema EXP_INTERVAL_PART_TABLE ...
Reading table objects of EXP_INTERVAL_PART_TABLE

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
INTERVAL_T1                                                      1         

Exporting tables (scripts or data) of EXP_INTERVAL_PART_TABLE
exporting table EXP_INTERVAL_PART_TABLE.INTERVAL_T1 ...
  exporting DDL of EXP_INTERVAL_PART_TABLE.INTERVAL_T1 ...
    data exporting success! 2 rows are dumped.
  exporting indexes on EXP_INTERVAL_PART_TABLE.INTERVAL_T1 ...
  exporting constraints on EXP_INTERVAL_PART_TABLE.INTERVAL_T1 ...

Exporting procedures/functions/triggers of schema EXP_INTERVAL_PART_TABLE ...
Exporting views of schema EXP_INTERVAL_PART_TABLE ...
Exporting synonyms of schema EXP_INTERVAL_PART_TABLE ...
Exporting package of schema EXP_INTERVAL_PART_TABLE ...
End of export schema EXP_INTERVAL_PART_TABLE ...

Logical export succeeded.

Parsing import options ... 
Verify options ...
  verify schema ...
Starting import ...
Preparing to import ...
-- IMPORT TYPE = SCHEMA
-- IMPORT OBJECTS = EXP_INTERVAL_PART_TABLE
-- DUMP FILE = ./data/exp_interval_part_table.dmp
-- LOG FILE = 
-- FILE TYPE = BIN
-- SHOW = N
-- FEEDBACK = 10000
-- PARALLEL = 1
-- DDL_PARALLEL = 1
-- CONTENT_MODE = ALL
-- IGNORE = N
-- CREATE_USER = N
-- TIMING = OFF
-- BATCH_COUNT = 10000
-- DISABLE_TRIGGER = Y
-- NOLOGGING = N


Importing schema EXP_INTERVAL_PART_TABLE ... 
  Importing sequence of schema EXP_INTERVAL_PART_TABLE ...
    Sequence importing success, 0 rows are loaded.
  Importing profile of schema EXP_INTERVAL_PART_TABLE ...
    Profile importing success, 0 rows are loaded.
  Importing type of schema EXP_INTERVAL_PART_TABLE ...
    Type importing success, 0 rows are loaded.

  Importing tables of schema EXP_INTERVAL_PART_TABLE ,total number : 1 ...
    The order of importing table is:
    TABLE NAME                                                       RECORD NUMBER 
    ---------------------------------------------------------------- --------------
    INTERVAL_T1                                                          2         

  Importing foreign key of schema EXP_INTERVAL_PART_TABLE ...
    Foreign key importing success, 0 rows are loaded.

  Importing fuction/procedure/trigger of schema EXP_INTERVAL_PART_TABLE ...
    Fuction/procedure/trigger importing success, 0 rows are loaded.

  Importing view of schema EXP_INTERVAL_PART_TABLE ...
    View importing success, 0 rows are loaded.

  Importing synonym of schema EXP_INTERVAL_PART_TABLE ...
    Synonym importing success, 0 rows are loaded.

  Importing package of schema EXP_INTERVAL_PART_TABLE ...
    Package importing success, 0 rows are loaded.

data importing success, 2 rows are loaded.
Logical import succeeded.

F1           F2           F3                            
------------ ------------ ------------------------------
1            1            abc                           
1000000      1000000      abc                           

2 rows fetched.

Succeed.

Succeed.

Succeed.

Succeed.

connected.

Succeed.

Succeed.

Succeed.

connected.

Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_PRIMARY_KEY
-- FILE TYPE = BIN
-- DUMP FILE = ./data/exp_primary_key.dmp
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema EXP_PRIMARY_KEY ...
Exporting sequence of schema EXP_PRIMARY_KEY ...
Exporting profile of schema EXP_PRIMARY_KEY ...
Exporting type of schema EXP_PRIMARY_KEY ...
Exporting tables of schema EXP_PRIMARY_KEY ...
Reading table objects of EXP_PRIMARY_KEY

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
TS                                                               1         

Exporting tables (scripts or data) of EXP_PRIMARY_KEY
exporting table EXP_PRIMARY_KEY.TS ...
  exporting DDL of EXP_PRIMARY_KEY.TS ...
  exporting data of EXP_PRIMARY_KEY.TS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_PRIMARY_KEY.TS ...
  exporting constraints on EXP_PRIMARY_KEY.TS ...

Exporting procedures/functions/triggers of schema EXP_PRIMARY_KEY ...
Exporting views of schema EXP_PRIMARY_KEY ...
Exporting synonyms of schema EXP_PRIMARY_KEY ...
Exporting package of schema EXP_PRIMARY_KEY ...
End of export schema EXP_PRIMARY_KEY ...

Logical export succeeded.

Parsing import options ... 
Verify options ...
  verify schema ...
Starting import ...
Preparing to import ...
-- IMPORT TYPE = SCHEMA
-- IMPORT OBJECTS = EXP_PRIMARY_KEY
-- DUMP FILE = ./data/exp_primary_key.dmp
-- LOG FILE = ./data/exp_primary_key.log
-- FILE TYPE = BIN
-- SHOW = N
-- FEEDBACK = 10000
-- PARALLEL = 1
-- DDL_PARALLEL = 1
-- CONTENT_MODE = ALL
-- IGNORE = N
-- CREATE_USER = N
-- TIMING = OFF
-- BATCH_COUNT = 10000
-- DISABLE_TRIGGER = Y
-- NOLOGGING = N


Importing schema EXP_PRIMARY_KEY ... 
  Importing sequence of schema EXP_PRIMARY_KEY ...
    Sequence importing success, 0 rows are loaded.
  Importing profile of schema EXP_PRIMARY_KEY ...
    Profile importing success, 0 rows are loaded.
  Importing type of schema EXP_PRIMARY_KEY ...
    Type importing success, 0 rows are loaded.

  Importing tables of schema EXP_PRIMARY_KEY ,total number : 1 ...
    The order of importing table is:
    TABLE NAME                                                       RECORD NUMBER 
    ---------------------------------------------------------------- --------------

  Importing foreign key of schema EXP_PRIMARY_KEY ...
    Foreign key importing success, 0 rows are loaded.

  Importing fuction/procedure/trigger of schema EXP_PRIMARY_KEY ...
    Fuction/procedure/trigger importing success, 0 rows are loaded.

  Importing view of schema EXP_PRIMARY_KEY ...
    View importing success, 0 rows are loaded.

  Importing synonym of schema EXP_PRIMARY_KEY ...
    Synonym importing success, 0 rows are loaded.

  Importing package of schema EXP_PRIMARY_KEY ...
    Package importing success, 0 rows are loaded.

data importing success, 0 rows are loaded.
Logical import succeeded.

Succeed.

Succeed.

Succeed.

Succeed.

connected.

Succeed.

Succeed.

Succeed.

connected.

Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = EXP_FOREIGN_KEY
-- FILE TYPE = BIN
-- DUMP FILE = ./data/exp_foreign_key.dmp
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema EXP_FOREIGN_KEY ...
Exporting sequence of schema EXP_FOREIGN_KEY ...
Exporting profile of schema EXP_FOREIGN_KEY ...
Exporting type of schema EXP_FOREIGN_KEY ...
Exporting tables of schema EXP_FOREIGN_KEY ...
Reading table objects of EXP_FOREIGN_KEY

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
TS                                                               1         
TS1                                                              2         

Exporting tables (scripts or data) of EXP_FOREIGN_KEY
exporting table EXP_FOREIGN_KEY.TS ...
  exporting DDL of EXP_FOREIGN_KEY.TS ...
  exporting data of EXP_FOREIGN_KEY.TS ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_FOREIGN_KEY.TS ...
  exporting constraints on EXP_FOREIGN_KEY.TS ...

exporting table EXP_FOREIGN_KEY.TS1 ...
  exporting DDL of EXP_FOREIGN_KEY.TS1 ...
  exporting data of EXP_FOREIGN_KEY.TS1 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on EXP_FOREIGN_KEY.TS1 ...
  exporting constraints on EXP_FOREIGN_KEY.TS1 ...

Exporting procedures/functions/triggers of schema EXP_FOREIGN_KEY ...
Exporting views of schema EXP_FOREIGN_KEY ...
Exporting synonyms of schema EXP_FOREIGN_KEY ...
Exporting package of schema EXP_FOREIGN_KEY ...
End of export schema EXP_FOREIGN_KEY ...

Logical export succeeded.

Parsing import options ... 
Verify options ...
  verify schema ...
Starting import ...
Preparing to import ...
-- IMPORT TYPE = SCHEMA
-- IMPORT OBJECTS = EXP_FOREIGN_KEY
-- DUMP FILE = ./data/exp_foreign_key.dmp
-- LOG FILE = ./data/exp_foreign_key.log
-- FILE TYPE = BIN
-- SHOW = N
-- FEEDBACK = 10000
-- PARALLEL = 1
-- DDL_PARALLEL = 1
-- CONTENT_MODE = ALL
-- IGNORE = N
-- CREATE_USER = N
-- TIMING = OFF
-- BATCH_COUNT = 10000
-- DISABLE_TRIGGER = Y
-- NOLOGGING = N


Importing schema EXP_FOREIGN_KEY ... 
  Importing sequence of schema EXP_FOREIGN_KEY ...
    Sequence importing success, 0 rows are loaded.
  Importing profile of schema EXP_FOREIGN_KEY ...
    Profile importing success, 0 rows are loaded.
  Importing type of schema EXP_FOREIGN_KEY ...
    Type importing success, 0 rows are loaded.

  Importing tables of schema EXP_FOREIGN_KEY ,total number : 2 ...
    The order of importing table is:
    TABLE NAME                                                       RECORD NUMBER 
    ---------------------------------------------------------------- --------------

  Importing foreign key of schema EXP_FOREIGN_KEY ...
    Foreign key importing success, 1 rows are loaded.

  Importing fuction/procedure/trigger of schema EXP_FOREIGN_KEY ...
    Fuction/procedure/trigger importing success, 0 rows are loaded.

  Importing view of schema EXP_FOREIGN_KEY ...
    View importing success, 0 rows are loaded.

  Importing synonym of schema EXP_FOREIGN_KEY ...
    Synonym importing success, 0 rows are loaded.

  Importing package of schema EXP_FOREIGN_KEY ...
    Package importing success, 0 rows are loaded.

data importing success, 0 rows are loaded.
Logical import succeeded.

Succeed.


connected.


Succeed.


Succeed.


Succeed.


Succeed.


1 rows affected.


Succeed.


connected.

Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = MY_USER
-- FILE TYPE = BIN
-- DUMP FILE = my_test1.dmp
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema MY_USER ...
Exporting sequence of schema MY_USER ...
Exporting profile of schema MY_USER ...
Exporting type of schema MY_USER ...
Exporting tables of schema MY_USER ...
Reading table objects of MY_USER

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
TEST                                                             1         

Exporting tables (scripts or data) of MY_USER
exporting table MY_USER.TEST ...
  exporting DDL of MY_USER.TEST ...
  exporting data of MY_USER.TEST ...
    data exporting success, 1 rows are dumped.

  exporting indexes on MY_USER.TEST ...
  exporting constraints on MY_USER.TEST ...

Exporting procedures/functions/triggers of schema MY_USER ...
Exporting views of schema MY_USER ...
Exporting synonyms of schema MY_USER ...
Exporting package of schema MY_USER ...
End of export schema MY_USER ...

Logical export succeeded.

Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TEST
-- FILE TYPE = BIN
-- DUMP FILE = my_test3.dmp
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table MY_USER.TEST ...
  exporting DDL of MY_USER.TEST ...
  exporting data of MY_USER.TEST ...
    data exporting success, 1 rows are dumped.

  exporting indexes on MY_USER.TEST ...
  exporting constraints on MY_USER.TEST ...

Logical export succeeded.

Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = ALL_TABLES
-- EXPORT OBJECTS = 
-- FILE TYPE = BIN
-- DUMP FILE = my_test4.dmp
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Reading table objects of MY_USER

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------
TEST                                                             1         

Exporting tables (scripts or data) of MY_USER
exporting table MY_USER.TEST ...
  exporting DDL of MY_USER.TEST ...
  exporting data of MY_USER.TEST ...
    data exporting success, 1 rows are dumped.

  exporting indexes on MY_USER.TEST ...
  exporting constraints on MY_USER.TEST ...

Logical export succeeded.


connected.


Succeed.


connected.


Succeed.


Succeed.


Succeed.


connected.


Succeed.


Succeed.

Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = TBL_SHOW_CREATE_TABLE6
-- FILE TYPE = TXT
-- DUMP FILE = EXPDAT.DMP
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table MY_USER.TBL_SHOW_CREATE_TABLE6 ...
  exporting DDL of MY_USER.TBL_SHOW_CREATE_TABLE6 ...
  exporting data of MY_USER.TBL_SHOW_CREATE_TABLE6 ...
    data exporting success, 0 rows are dumped.

  exporting indexes on MY_USER.TBL_SHOW_CREATE_TABLE6 ...
  exporting constraints on MY_USER.TBL_SHOW_CREATE_TABLE6 ...

Logical export succeeded.

Parsing import options ... 
Verify options ...
Starting import ...
Preparing to import ...
-- IMPORT TYPE = FULL
-- IMPORT OBJECTS = 
-- DUMP FILE = EXPDAT.DMP
-- LOG FILE = 
-- FILE TYPE = TXT
-- SHOW = N
-- FEEDBACK = 10000
-- PARALLEL = 1
-- DDL_PARALLEL = 1
-- CONTENT_MODE = ALL
-- IGNORE = N
-- CREATE_USER = N
-- TIMING = OFF
-- BATCH_COUNT = 10000
-- DISABLE_TRIGGER = Y
-- NOLOGGING = N

data importing success, 0 rows are loaded.
Logical import succeeded.


connected.


Succeed.


Succeed.


connected.


Succeed.

Parsing export options ... 
Verify options ...
  default to export current schema: HBH
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = HBH
-- FILE TYPE = TXT
-- DUMP FILE = ./data/hbh.sql
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema HBH ...
Exporting sequence of schema HBH ...
Exporting profile of schema HBH ...
Exporting type of schema HBH ...
Exporting tables of schema HBH ...
Reading table objects of HBH

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of HBH
Exporting procedures/functions/triggers of schema HBH ...
Exporting views of schema HBH ...
Exporting synonyms of schema HBH ...
Exporting package of schema HBH ...
End of export schema HBH ...

Logical export succeeded.

Parsing import options ... 
Verify options ...
  default to import current schema: HBH
Starting import ...
Preparing to import ...
-- IMPORT TYPE = SCHEMA
-- IMPORT OBJECTS = HBH
-- DUMP FILE = ./data/hbh.sql
-- LOG FILE = 
-- FILE TYPE = TXT
-- SHOW = N
-- FEEDBACK = 10000
-- PARALLEL = 1
-- DDL_PARALLEL = 1
-- CONTENT_MODE = ALL
-- IGNORE = N
-- CREATE_USER = N
-- TIMING = OFF
-- BATCH_COUNT = 10000
-- DISABLE_TRIGGER = Y
-- NOLOGGING = N

data importing success, 0 rows are loaded.
Logical import succeeded.


connected.


Succeed.


Succeed.


Succeed.


connected.


Succeed.


3 rows affected.


Succeed.

Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = T1
-- FILE TYPE = TXT
-- DUMP FILE = ./data/test_exp_query.dmp
-- LOG FILE = 
-- QUERY = "where c = 1"
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table TEST_EXP_QUERY.T1 ...
  exporting DDL of TEST_EXP_QUERY.T1 ...
  exporting data of TEST_EXP_QUERY.T1 ...
    data exporting success, 1 rows are dumped.

  exporting indexes on TEST_EXP_QUERY.T1 ...
  exporting constraints on TEST_EXP_QUERY.T1 ...

Logical export succeeded.

Parsing export options ... 
Verify options ...

CT-00601, Sql syntax error: 'query' option is not supported when filetype is BIN or parallel > 1
Logical export failed.

Parsing export options ... 
Verify options ...

CT-00601, Sql syntax error: 'query' option is not supported when filetype is BIN or parallel > 1
Logical export failed.

Parsing export options ... 
Verify options ...

CT-00601, Sql syntax error: 'query' option is not supported when filetype is BIN or parallel > 1
Logical export failed.


connected.


Succeed.


Succeed.


connected.


Succeed.


Succeed.


Succeed.


Succeed.

Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = ORDER_ORDERLINE
-- FILE TYPE = TXT
-- DUMP FILE = ./data/test_comment_specail.dmp
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table COMMENT_SPECAIL_CHAR.ORDER_ORDERLINE ...
  exporting DDL of COMMENT_SPECAIL_CHAR.ORDER_ORDERLINE ...
  exporting data of COMMENT_SPECAIL_CHAR.ORDER_ORDERLINE ...
    data exporting success, 0 rows are dumped.

  exporting indexes on COMMENT_SPECAIL_CHAR.ORDER_ORDERLINE ...
  exporting constraints on COMMENT_SPECAIL_CHAR.ORDER_ORDERLINE ...

Logical export succeeded.

Parsing import options ... 
Verify options ...
  verify tables ...
Starting import ...
Preparing to import ...
-- IMPORT TYPE = TABLE
-- IMPORT OBJECTS = ORDER_ORDERLINE
-- DUMP FILE = ./data/test_comment_specail.dmp
-- LOG FILE = 
-- FILE TYPE = TXT
-- SHOW = N
-- FEEDBACK = 10000
-- PARALLEL = 1
-- DDL_PARALLEL = 1
-- CONTENT_MODE = ALL
-- IGNORE = N
-- CREATE_USER = N
-- TIMING = OFF
-- BATCH_COUNT = 10000
-- DISABLE_TRIGGER = Y
-- NOLOGGING = N

data importing success, 0 rows are loaded.
Logical import succeeded.


COMMENTS                                                        
----------------------------------------------------------------
X day(when the effect mode is 'x-day' later).                   

1 rows fetched.

Parsing export options ... 
Verify options ...
  verify tables ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = TABLE
-- EXPORT OBJECTS = ORDER_ORDERLINE
-- FILE TYPE = BIN
-- DUMP FILE = ./data/test_comment_specail_bin.dmp
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

exporting table COMMENT_SPECAIL_CHAR.ORDER_ORDERLINE ...
  exporting DDL of COMMENT_SPECAIL_CHAR.ORDER_ORDERLINE ...
  exporting data of COMMENT_SPECAIL_CHAR.ORDER_ORDERLINE ...
    data exporting success, 0 rows are dumped.

  exporting indexes on COMMENT_SPECAIL_CHAR.ORDER_ORDERLINE ...
  exporting constraints on COMMENT_SPECAIL_CHAR.ORDER_ORDERLINE ...

Logical export succeeded.

Parsing import options ... 
Verify options ...
  verify tables ...
Starting import ...
Preparing to import ...
-- IMPORT TYPE = TABLE
-- IMPORT OBJECTS = ORDER_ORDERLINE
-- DUMP FILE = ./data/test_comment_specail_bin.dmp
-- LOG FILE = 
-- FILE TYPE = BIN
-- SHOW = N
-- FEEDBACK = 10000
-- PARALLEL = 1
-- DDL_PARALLEL = 1
-- CONTENT_MODE = ALL
-- IGNORE = N
-- CREATE_USER = N
-- TIMING = OFF
-- BATCH_COUNT = 10000
-- DISABLE_TRIGGER = Y
-- NOLOGGING = N

  Importing sequence of schema COMMENT_SPECAIL_CHAR ...
    Sequence importing success, 0 rows are loaded.
  Importing profile of schema COMMENT_SPECAIL_CHAR ...
    Profile importing success, 0 rows are loaded.
  Importing type of schema COMMENT_SPECAIL_CHAR ...
    Type importing success, 0 rows are loaded.

  Importing tables of schema COMMENT_SPECAIL_CHAR ,total number : 1 ...
    The order of importing table is:
    TABLE NAME                                                       RECORD NUMBER 
    ---------------------------------------------------------------- --------------

  Importing foreign key of schema COMMENT_SPECAIL_CHAR ...
    Foreign key importing success, 0 rows are loaded.

  Importing fuction/procedure/trigger of schema COMMENT_SPECAIL_CHAR ...
    Fuction/procedure/trigger importing success, 0 rows are loaded.

  Importing view of schema COMMENT_SPECAIL_CHAR ...
    View importing success, 0 rows are loaded.

  Importing synonym of schema COMMENT_SPECAIL_CHAR ...
    Synonym importing success, 0 rows are loaded.

  Importing package of schema COMMENT_SPECAIL_CHAR ...
    Package importing success, 0 rows are loaded.

data importing success, 0 rows are loaded.
Logical import succeeded.


COMMENTS                                                        
----------------------------------------------------------------
X day(when the effect mode is 'x-day' later).                   

1 rows fetched.


Succeed.


PL/SQL procedure successfully completed.

Parsing export options ... 
Verify options ...
  verify schema ...
Starting export ...
Preparing to export ...
-- EXPORT TYPE = SCHEMA
-- EXPORT OBJECTS = TEST_EXP_STMT_LOSE1, TEST_EXP_STMT_LOSE2, TEST_EXP_STMT_LOSE3, TEST_EXP_STMT_LOSE4, TEST_EXP_STMT_LOSE5, TEST_EXP_STMT_LOSE6, TEST_EXP_STMT_LOSE7, TEST_EXP_STMT_LOSE8, TEST_EXP_STMT_LOSE9, TEST_EXP_STMT_LOSE10, TEST_EXP_STMT_LOSE11, TEST_EXP_STMT_LOSE12, TEST_EXP_STMT_LOSE13, TEST_EXP_STMT_LOSE14, TEST_EXP_STMT_LOSE15, TEST_EXP_STMT_LOSE16, TEST_EXP_STMT_LOSE17, TEST_EXP_STMT_LOSE18, TEST_EXP_STMT_LOSE19, TEST_EXP_STMT_LOSE20
-- FILE TYPE = TXT
-- DUMP FILE = ./data/test_exp_stmt_lose.dmp
-- LOG FILE = 
-- QUERY = ""
-- COMPRESS = N
-- CONSISTENT = N
-- CONTENT_MODE = ALL
-- SKIP_COMMENTS = N
-- FORCE = N
-- SKIP_ADD_DROP_TABLE = N
-- SKIP_TRIGGERS = N
-- QUOTE_NAMES = Y
-- TABLESPACE = N
-- COMMIT_BATCH = 1000
-- INSERT_BATCH = 1
-- FEEDBACK = 10000
-- PARALLEL = 0
-- TENANT = N
-- CREATE_USER = N
-- ROLE = N
-- GRANT = N
-- WITH_CR_MODE = N
-- WITH_FORMAT_CSF = Y
-- INDEX_PARTITIONS = N

Exporting schema TEST_EXP_STMT_LOSE1 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE1 ...
Exporting profile of schema TEST_EXP_STMT_LOSE1 ...
Exporting type of schema TEST_EXP_STMT_LOSE1 ...
Exporting tables of schema TEST_EXP_STMT_LOSE1 ...
Reading table objects of TEST_EXP_STMT_LOSE1

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE1
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE1 ...
Exporting views of schema TEST_EXP_STMT_LOSE1 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE1 ...
Exporting package of schema TEST_EXP_STMT_LOSE1 ...
End of export schema TEST_EXP_STMT_LOSE1 ...

Exporting schema TEST_EXP_STMT_LOSE2 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE2 ...
Exporting profile of schema TEST_EXP_STMT_LOSE2 ...
Exporting type of schema TEST_EXP_STMT_LOSE2 ...
Exporting tables of schema TEST_EXP_STMT_LOSE2 ...
Reading table objects of TEST_EXP_STMT_LOSE2

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE2
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE2 ...
Exporting views of schema TEST_EXP_STMT_LOSE2 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE2 ...
Exporting package of schema TEST_EXP_STMT_LOSE2 ...
End of export schema TEST_EXP_STMT_LOSE2 ...

Exporting schema TEST_EXP_STMT_LOSE3 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE3 ...
Exporting profile of schema TEST_EXP_STMT_LOSE3 ...
Exporting type of schema TEST_EXP_STMT_LOSE3 ...
Exporting tables of schema TEST_EXP_STMT_LOSE3 ...
Reading table objects of TEST_EXP_STMT_LOSE3

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE3
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE3 ...
Exporting views of schema TEST_EXP_STMT_LOSE3 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE3 ...
Exporting package of schema TEST_EXP_STMT_LOSE3 ...
End of export schema TEST_EXP_STMT_LOSE3 ...

Exporting schema TEST_EXP_STMT_LOSE4 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE4 ...
Exporting profile of schema TEST_EXP_STMT_LOSE4 ...
Exporting type of schema TEST_EXP_STMT_LOSE4 ...
Exporting tables of schema TEST_EXP_STMT_LOSE4 ...
Reading table objects of TEST_EXP_STMT_LOSE4

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE4
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE4 ...
Exporting views of schema TEST_EXP_STMT_LOSE4 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE4 ...
Exporting package of schema TEST_EXP_STMT_LOSE4 ...
End of export schema TEST_EXP_STMT_LOSE4 ...

Exporting schema TEST_EXP_STMT_LOSE5 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE5 ...
Exporting profile of schema TEST_EXP_STMT_LOSE5 ...
Exporting type of schema TEST_EXP_STMT_LOSE5 ...
Exporting tables of schema TEST_EXP_STMT_LOSE5 ...
Reading table objects of TEST_EXP_STMT_LOSE5

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE5
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE5 ...
Exporting views of schema TEST_EXP_STMT_LOSE5 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE5 ...
Exporting package of schema TEST_EXP_STMT_LOSE5 ...
End of export schema TEST_EXP_STMT_LOSE5 ...

Exporting schema TEST_EXP_STMT_LOSE6 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE6 ...
Exporting profile of schema TEST_EXP_STMT_LOSE6 ...
Exporting type of schema TEST_EXP_STMT_LOSE6 ...
Exporting tables of schema TEST_EXP_STMT_LOSE6 ...
Reading table objects of TEST_EXP_STMT_LOSE6

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE6
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE6 ...
Exporting views of schema TEST_EXP_STMT_LOSE6 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE6 ...
Exporting package of schema TEST_EXP_STMT_LOSE6 ...
End of export schema TEST_EXP_STMT_LOSE6 ...

Exporting schema TEST_EXP_STMT_LOSE7 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE7 ...
Exporting profile of schema TEST_EXP_STMT_LOSE7 ...
Exporting type of schema TEST_EXP_STMT_LOSE7 ...
Exporting tables of schema TEST_EXP_STMT_LOSE7 ...
Reading table objects of TEST_EXP_STMT_LOSE7

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE7
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE7 ...
Exporting views of schema TEST_EXP_STMT_LOSE7 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE7 ...
Exporting package of schema TEST_EXP_STMT_LOSE7 ...
End of export schema TEST_EXP_STMT_LOSE7 ...

Exporting schema TEST_EXP_STMT_LOSE8 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE8 ...
Exporting profile of schema TEST_EXP_STMT_LOSE8 ...
Exporting type of schema TEST_EXP_STMT_LOSE8 ...
Exporting tables of schema TEST_EXP_STMT_LOSE8 ...
Reading table objects of TEST_EXP_STMT_LOSE8

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE8
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE8 ...
Exporting views of schema TEST_EXP_STMT_LOSE8 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE8 ...
Exporting package of schema TEST_EXP_STMT_LOSE8 ...
End of export schema TEST_EXP_STMT_LOSE8 ...

Exporting schema TEST_EXP_STMT_LOSE9 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE9 ...
Exporting profile of schema TEST_EXP_STMT_LOSE9 ...
Exporting type of schema TEST_EXP_STMT_LOSE9 ...
Exporting tables of schema TEST_EXP_STMT_LOSE9 ...
Reading table objects of TEST_EXP_STMT_LOSE9

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE9
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE9 ...
Exporting views of schema TEST_EXP_STMT_LOSE9 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE9 ...
Exporting package of schema TEST_EXP_STMT_LOSE9 ...
End of export schema TEST_EXP_STMT_LOSE9 ...

Exporting schema TEST_EXP_STMT_LOSE10 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE10 ...
Exporting profile of schema TEST_EXP_STMT_LOSE10 ...
Exporting type of schema TEST_EXP_STMT_LOSE10 ...
Exporting tables of schema TEST_EXP_STMT_LOSE10 ...
Reading table objects of TEST_EXP_STMT_LOSE10

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE10
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE10 ...
Exporting views of schema TEST_EXP_STMT_LOSE10 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE10 ...
Exporting package of schema TEST_EXP_STMT_LOSE10 ...
End of export schema TEST_EXP_STMT_LOSE10 ...

Exporting schema TEST_EXP_STMT_LOSE11 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE11 ...
Exporting profile of schema TEST_EXP_STMT_LOSE11 ...
Exporting type of schema TEST_EXP_STMT_LOSE11 ...
Exporting tables of schema TEST_EXP_STMT_LOSE11 ...
Reading table objects of TEST_EXP_STMT_LOSE11

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE11
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE11 ...
Exporting views of schema TEST_EXP_STMT_LOSE11 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE11 ...
Exporting package of schema TEST_EXP_STMT_LOSE11 ...
End of export schema TEST_EXP_STMT_LOSE11 ...

Exporting schema TEST_EXP_STMT_LOSE12 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE12 ...
Exporting profile of schema TEST_EXP_STMT_LOSE12 ...
Exporting type of schema TEST_EXP_STMT_LOSE12 ...
Exporting tables of schema TEST_EXP_STMT_LOSE12 ...
Reading table objects of TEST_EXP_STMT_LOSE12

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE12
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE12 ...
Exporting views of schema TEST_EXP_STMT_LOSE12 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE12 ...
Exporting package of schema TEST_EXP_STMT_LOSE12 ...
End of export schema TEST_EXP_STMT_LOSE12 ...

Exporting schema TEST_EXP_STMT_LOSE13 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE13 ...
Exporting profile of schema TEST_EXP_STMT_LOSE13 ...
Exporting type of schema TEST_EXP_STMT_LOSE13 ...
Exporting tables of schema TEST_EXP_STMT_LOSE13 ...
Reading table objects of TEST_EXP_STMT_LOSE13

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE13
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE13 ...
Exporting views of schema TEST_EXP_STMT_LOSE13 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE13 ...
Exporting package of schema TEST_EXP_STMT_LOSE13 ...
End of export schema TEST_EXP_STMT_LOSE13 ...

Exporting schema TEST_EXP_STMT_LOSE14 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE14 ...
Exporting profile of schema TEST_EXP_STMT_LOSE14 ...
Exporting type of schema TEST_EXP_STMT_LOSE14 ...
Exporting tables of schema TEST_EXP_STMT_LOSE14 ...
Reading table objects of TEST_EXP_STMT_LOSE14

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE14
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE14 ...
Exporting views of schema TEST_EXP_STMT_LOSE14 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE14 ...
Exporting package of schema TEST_EXP_STMT_LOSE14 ...
End of export schema TEST_EXP_STMT_LOSE14 ...

Exporting schema TEST_EXP_STMT_LOSE15 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE15 ...
Exporting profile of schema TEST_EXP_STMT_LOSE15 ...
Exporting type of schema TEST_EXP_STMT_LOSE15 ...
Exporting tables of schema TEST_EXP_STMT_LOSE15 ...
Reading table objects of TEST_EXP_STMT_LOSE15

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE15
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE15 ...
Exporting views of schema TEST_EXP_STMT_LOSE15 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE15 ...
Exporting package of schema TEST_EXP_STMT_LOSE15 ...
End of export schema TEST_EXP_STMT_LOSE15 ...

Exporting schema TEST_EXP_STMT_LOSE16 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE16 ...
Exporting profile of schema TEST_EXP_STMT_LOSE16 ...
Exporting type of schema TEST_EXP_STMT_LOSE16 ...
Exporting tables of schema TEST_EXP_STMT_LOSE16 ...
Reading table objects of TEST_EXP_STMT_LOSE16

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE16
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE16 ...
Exporting views of schema TEST_EXP_STMT_LOSE16 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE16 ...
Exporting package of schema TEST_EXP_STMT_LOSE16 ...
End of export schema TEST_EXP_STMT_LOSE16 ...

Exporting schema TEST_EXP_STMT_LOSE17 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE17 ...
Exporting profile of schema TEST_EXP_STMT_LOSE17 ...
Exporting type of schema TEST_EXP_STMT_LOSE17 ...
Exporting tables of schema TEST_EXP_STMT_LOSE17 ...
Reading table objects of TEST_EXP_STMT_LOSE17

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE17
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE17 ...
Exporting views of schema TEST_EXP_STMT_LOSE17 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE17 ...
Exporting package of schema TEST_EXP_STMT_LOSE17 ...
End of export schema TEST_EXP_STMT_LOSE17 ...

Exporting schema TEST_EXP_STMT_LOSE18 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE18 ...
Exporting profile of schema TEST_EXP_STMT_LOSE18 ...
Exporting type of schema TEST_EXP_STMT_LOSE18 ...
Exporting tables of schema TEST_EXP_STMT_LOSE18 ...
Reading table objects of TEST_EXP_STMT_LOSE18

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE18
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE18 ...
Exporting views of schema TEST_EXP_STMT_LOSE18 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE18 ...
Exporting package of schema TEST_EXP_STMT_LOSE18 ...
End of export schema TEST_EXP_STMT_LOSE18 ...

Exporting schema TEST_EXP_STMT_LOSE19 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE19 ...
Exporting profile of schema TEST_EXP_STMT_LOSE19 ...
Exporting type of schema TEST_EXP_STMT_LOSE19 ...
Exporting tables of schema TEST_EXP_STMT_LOSE19 ...
Reading table objects of TEST_EXP_STMT_LOSE19

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE19
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE19 ...
Exporting views of schema TEST_EXP_STMT_LOSE19 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE19 ...
Exporting package of schema TEST_EXP_STMT_LOSE19 ...
End of export schema TEST_EXP_STMT_LOSE19 ...

Exporting schema TEST_EXP_STMT_LOSE20 ...
Exporting sequence of schema TEST_EXP_STMT_LOSE20 ...
Exporting profile of schema TEST_EXP_STMT_LOSE20 ...
Exporting type of schema TEST_EXP_STMT_LOSE20 ...
Exporting tables of schema TEST_EXP_STMT_LOSE20 ...
Reading table objects of TEST_EXP_STMT_LOSE20

The order of exporting table is:
TABLE NAME                                                       LEVEL     
---------------------------------------------------------------- ----------

Exporting tables (scripts or data) of TEST_EXP_STMT_LOSE20
Exporting procedures/functions/triggers of schema TEST_EXP_STMT_LOSE20 ...
Exporting views of schema TEST_EXP_STMT_LOSE20 ...
Exporting synonyms of schema TEST_EXP_STMT_LOSE20 ...
Exporting package of schema TEST_EXP_STMT_LOSE20 ...
End of export schema TEST_EXP_STMT_LOSE20 ...

Logical export succeeded.


PL/SQL procedure successfully completed.


Succeed.


connected.


Succeed.


Succeed.


connected.

Parsing import options ... 
Verify options ...
  verify tables ...
Starting import ...
Preparing to import ...
-- IMPORT TYPE = TABLE
-- IMPORT OBJECTS = STAFFS_P
-- DUMP FILE = ./data/large_table.dmp
-- LOG FILE = 
-- FILE TYPE = BIN
-- SHOW = N
-- FEEDBACK = 10000
-- PARALLEL = 1
-- DDL_PARALLEL = 1
-- CONTENT_MODE = ALL
-- IGNORE = N
-- CREATE_USER = N
-- TIMING = OFF
-- BATCH_COUNT = 10000
-- DISABLE_TRIGGER = Y
-- NOLOGGING = N

  Importing sequence of schema TEST_LARGE_TABLE ...
    Sequence importing success, 0 rows are loaded.
  Importing profile of schema TEST_LARGE_TABLE ...
    Profile importing success, 0 rows are loaded.
  Importing type of schema TEST_LARGE_TABLE ...
    Type importing success, 0 rows are loaded.

  Importing tables of schema TEST_LARGE_TABLE ,total number : 1 ...
    The order of importing table is:
    TABLE NAME                                                       RECORD NUMBER 
    ---------------------------------------------------------------- --------------

thread 1 : CT-00602, Sql text is too long, length = 1081116
CT-00520, Import error occur when check DDL thread, detail: DDL thread throws error
Logical import failed.

Parsing import options ... 
Verify options ...
  verify tables ...
Starting import ...
Preparing to import ...
-- IMPORT TYPE = TABLE
-- IMPORT OBJECTS = STAFFS_P
-- DUMP FILE = ./data/large_table.sql
-- LOG FILE = 
-- FILE TYPE = TXT
-- SHOW = N
-- FEEDBACK = 10000
-- PARALLEL = 1
-- DDL_PARALLEL = 1
-- CONTENT_MODE = ALL
-- IGNORE = N
-- CREATE_USER = N
-- TIMING = OFF
-- BATCH_COUNT = 10000
-- DISABLE_TRIGGER = Y
-- NOLOGGING = N

CT-00602, Sql text is too long, length = 1081116

CT-00050, Secure C lib has thrown an error -1
Logical import failed.




